[
  {
    "objectID": "contributor-docs/recommender.html",
    "href": "contributor-docs/recommender.html",
    "title": "Recommenders",
    "section": "",
    "text": "Too much content, not enough time. That about sums up the problem facing the data science community. So, our Recommenders are here to help. Contributors are invited to submit lists (or Feeds) of high-quality sources on all manner of topics – from foundational ideas in data science and cutting-edge techniques, to opinion and thought-leadership on the future of the profession. Reviews of new books and other material are also welcome.",
    "crumbs": [
      "Recommenders"
    ]
  },
  {
    "objectID": "contributor-docs/recommender.html#article-types-and-structures",
    "href": "contributor-docs/recommender.html#article-types-and-structures",
    "title": "Recommenders",
    "section": "Article types and structures",
    "text": "Article types and structures\n\nFeeds\nFeeds can be constructed around different topics and audiences. For example, you might want to recommend to all data scientists the “10 best blogs on machine learning” or “5 data visualisation experts to follow on Twitter/X”. Or you might have a list of sources specifically targeted at data science educators (e.g., “the best books on teaching coding”) or data science leaders (“5 insightful case studies on building data science teams”).\nWhatever you choose to focus on, the following outline provides a basic guide for structuring your feed:\n\n\nOverview\n\nA brief introduction to your list, its main focus, who you are writing it for, and why. You should also say something about yourself and your background, too. This will give additional context to the recommendations you are making.\n\n\n\nList of sources\n\nAs well as naming your sources and telling people how to find them, you should also explain why you are recommending them, how they helped you in your career or studies, or other reasons why you find them to be of value. Sample quotes or small excerpts from the sources themselves might also be worth including.\n\n\n\nStart a dialogue\n\nConclude with a call for readers to share recommendations of their own, either in the article comments or on social media. Contributors are welcome to update their lists any time with new sources, including those suggested by site users.\n\n\n\n\n\nReviews\nUnlike the feeds described above, each submitted review should focus only on a single publication. It must be an honest summary of the reviewer’s thoughts, feelings and impressions, covering what they liked and didn’t like, the perceived strengths and weaknesses of the publication, and whether it is likely to be of interest and value to its intended audience. Reviews should of course provide an overview of the publication in question but must avoid dry, itemised descriptions of the publication’s constituent parts (e.g., listing out the chapters in a book).\nAll reviews should list the following information (if relevant):\n\nTitle of publication\nAuthor(s)\nDate of publication\nEdition or format used for review\nPublisher\nLength\nPrice\nWebsite address or other source of further information",
    "crumbs": [
      "Recommenders"
    ]
  },
  {
    "objectID": "contributor-docs/recommender.html#advice-and-recommendations",
    "href": "contributor-docs/recommender.html#advice-and-recommendations",
    "title": "Recommenders",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nKeep lists to a sensible size. Feeds are meant to help data scientists to prioritise who and what to follow based on their interests and career stage – and it is much easier to keep tabs on 5 sources than it is 50, or even 15! So, the fewer the better.\nKeep your recommendations up to date. In this era of digital publishing, things can and do change overnight. So, if one of your recommended bloggers stops blogging, or the author of one of your favourite books makes a major update to the text, do be sure to let us – and your audience – know. We want to keep feeds and reviews current and useful.\nOf course you are brilliant, but… Please do not recommend or review your own publications or those in which you have a pecuniary or similar interest.",
    "crumbs": [
      "Recommenders"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html",
    "href": "contributor-docs/call-for-contributions.html",
    "title": "Call for contributions",
    "section": "",
    "text": "The Data Shrink aims to inform, inspire and strengthen the data science community by showcasing real-world examples of data science practice and bringing together data scientists to share knowledge.\nWe cannot succeed in these aims without the support and contributions of the data science community, so thank you for taking the time to review this open call for contributions.",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "href": "contributor-docs/call-for-contributions.html#what-are-we-looking-for",
    "title": "Call for contributions",
    "section": "What are we looking for?",
    "text": "What are we looking for?\nBelow is a list of our core content areas. We welcome submissions in any of these areas. Each content area is linked to its own set of notes for contributors.\n\nCase studies\nExplainers\nExercises\nDatasets\nTraining guides\nRecommenders\nDataScienceBites\n\nSubmissions can focus on any and all topics and application areas. We want our content to reflect the breadth and depth of real-world data science.",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#our-target-audience",
    "href": "contributor-docs/call-for-contributions.html#our-target-audience",
    "title": "Call for contributions",
    "section": "Our target audience",
    "text": "Our target audience\nThe Data Shrink is for all who work in data science – whether they are students, teachers, practitioners or leaders. Submissions do not have to appeal to all data scientists, however. Contributors should think carefully about who they are trying to reach, and craft their submissions accordingly.",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "href": "contributor-docs/call-for-contributions.html#what-can-submissions-include",
    "title": "Call for contributions",
    "section": "What can submissions include?",
    "text": "What can submissions include?\nWe encourage contributors to experiment with and include different media formats in their submissions – text, images, audio and video. And as our site is built on Quarto – the new open-source publishing system developed by Posit – submissions to The Data Shrink can also include code cells, equations, figures, interactive data displays, and other elements to enrich the user experience.\nIf you haven’t used Quarto before, check out this fantastic tutorial from the developers. You can also explore some of the range of Quarto features that we use in this GitHub template repository, created by Finn-Ole Höner. It’s an excellent resource to help The Data Shrink contributors get started!",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/call-for-contributions.html#how-to-submit",
    "href": "contributor-docs/call-for-contributions.html#how-to-submit",
    "title": "Call for contributions",
    "section": "How to submit",
    "text": "How to submit\nOnce you’ve reviewed our notes for contributors and settled on a content area, theme and audience, please review our contributor guidelines for details on the submission process.",
    "crumbs": [
      "Call for contributions"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html",
    "href": "contributor-docs/datasciencebites.html",
    "title": "Data Science Bites",
    "section": "",
    "text": "Our DataScienceBites blog publishes digestible, engaging summaries of interesting new pre-print and peer-reviewed publications in the data science space. Our goal is to make scientific papers more widely accessible.\nPosts are targeted at undergraduate level. Each presents a non-technical overview of a new research paper and its key findings, potential applications, and implications.\nWe welcome contributions from graduate students and early career researchers in data science (or related subjects) at universities throughout the world, as well as industry researchers. Contributors must have a passion for science communication and a drive to explain, clarify and demystify.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "href": "contributor-docs/datasciencebites.html#basic-structure-of-a-bites-post",
    "title": "Data Science Bites",
    "section": "Basic structure of a Bites post",
    "text": "Basic structure of a Bites post\n\n\nInformation box\n\nGive the full title of the paper you are discussing, the name(s) of its author(s) and year of publication. Say where the paper is published, whether it is a pre-print or peer-reviewed publication, whether it is open access or not, and include links to authorised HTML and/or PDF versions.\n\n\n\nIntroduction and background\n\nEase readers into the paper you are writing about, and help them to see why it is worth their attention. Sometimes a paper is sufficiently ground breaking to be attention grabbing in its own right. But more often than not you will have to find a way to “hook” people in. When writing about data science tools and methods, for example, it can be helpful to start by outlining a typical application scenario or use case. Readers may be more familiar with the use case than the tool, so this provides valuable framing within which you can talk about the shortcomings of existing methods and the advances promised by the new research. Ultimately, you want to get readers to the point where they understand the problem, question or challenge that this new research paper aims to solve.\n\n\n\nResearch overview\n\nHere is where you summarise the work done by the paper’s authors. Remember to keep the discussion non-technical and jargon-free, and explain important terms and concepts as necessary. Be careful not to oversimplify!\n\n\n\nTakeaways and implications\n\nPut this paper and its contributions into the appropriate context for readers. Does it make modest but important improvements to existing knowledge or research processes? Will it help others to address new questions, issues and challenges? What further work is needed to build on these advances?\n\n\n\nFurther reading\n\nReaders may wish to learn more about the research or the broader subject matter, so feel free to point them to additional resources: videos, podcasts, textbooks, conference presentations, etc.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#word-count-target",
    "href": "contributor-docs/datasciencebites.html#word-count-target",
    "title": "Data Science Bites",
    "section": "Word count target",
    "text": "Word count target\n500–1,000 words.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "href": "contributor-docs/datasciencebites.html#two-ways-of-contributing",
    "title": "Data Science Bites",
    "section": "Two ways of contributing",
    "text": "Two ways of contributing\n\nRegular contributors commit to publishing an agreed number of posts (6–12) each year. Regular contributors will collaborate closely with editors on the development of the blog, and experienced contributors will be invited to support and mentor new regular contributors to help grow the DataScienceBites team.\nGuest contributors make one-off or ad hoc contributions to the site. Proposals are to be submitted in the form of a content brief.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#next-steps",
    "href": "contributor-docs/datasciencebites.html#next-steps",
    "title": "Data Science Bites",
    "section": "Next steps",
    "text": "Next steps\nIf you are interested in contributing to DataScienceBites:\n\nIdentify a new data science publication that you are interested in writing about.\nReview the blog index to make sure we haven’t already covered this publication.\nDecide whether you would like to be a regular contributor or guest contributor.\nContact The Data Shrink to discuss.\n\n\n\n\n\n\n\nTwo things to keep in mind\n\n\n\nDataScienceBites contributors are not permitted to write about their own research papers.\nIf contributors are in any way affiliated with the authors of papers they write about, this must be disclosed as part of their submission.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "href": "contributor-docs/datasciencebites.html#about-the-sciencebites-family",
    "title": "Data Science Bites",
    "section": "About the ScienceBites family",
    "text": "About the ScienceBites family\nDataScienceBites is published by The Data Shrink and is part of the ScienceBites galaxy of sites. See sciencebites.org for an overview of the ScienceBites mission.",
    "crumbs": [
      "Data Science Bites"
    ]
  },
  {
    "objectID": "contributor-docs/exercises.html",
    "href": "contributor-docs/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises on The Data Shrink will provide users with the opportunity to put knowledge, skills and new learnings into practice, helping them to challenge and refine problem-solving approaches while strengthening the analytical mindset.\nWe will achieve this by supporting contributors to design tasks that replicate real-world data scientific processes.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "contributor-docs/exercises.html#structure",
    "href": "contributor-docs/exercises.html#structure",
    "title": "Exercises",
    "section": "Structure",
    "text": "Structure\nThe structure of each published exercise will vary based on the nature of the task(s) being set by contributors and the outcomes they have in mind. But, in general, exercises must do more than simply ask users to, e.g., download dataset – analyse – report.\n\n\nSet the scene\n\nProvide a believable, realistic scenario for the exercise. Establish the “client challenge” within that context, the resources available to the data scientist, and the outputs expected.\n\n\n\nGive users space to map the problem themselves\n\nHave users translate the “client challenge” into a data analytic question that can be answered using the resources available.\n\n\n\nMake data exploration, cleaning and tidying part of the process\n\nData exploration and preparation are an important part of most – if not all – data science projects, so let users loose on messy datasets so they can figure out for themselves (a) what they’re working with and (b) what analytical approach makes sense given the features of the data and the problem at hand.\n\n\n\nIntegrate ethics\n\nPrompt users to think about and work through ethical questions and issues that are relevant to the exercise: the challenge they’ve been set, the data they’ve been given, their proposed approach to analysis and modelling, etc.\n\n\n\nEncourage users to document their work and their thinking\n\nThrough computational notebooks (e.g., Jupyter notebooks), users can record not only what they’ve done and how they’ve done it, but why.\n\n\n\nMake data presentation part of the expected project outputs\n\nAsk users to think about presenting to specific audiences: not just fellow data scientists, but to decision-makers, policy experts, the public – whatever makes most sense given the exercise scenario.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "contributor-docs/exercises.html#advice-and-recommendations",
    "href": "contributor-docs/exercises.html#advice-and-recommendations",
    "title": "Exercises",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nInvite users to share their work. If users have followed the advice to document their work and thinking, encourage them to share their computational notebooks (including their results and outputs) on their own websites, in a GitHub repository, through social media, etc. This could be a great way for others to discover your exercise, and we could also link to a selection of these notebooks through the exercise page itself.\nThink about building in hints and tips. Some users – depending on their prior level of experience – might appreciate some additional guidance here and there.\n\n\n\n\n\n\nSpoiler warning\n\n\n\n\n\nCollapsible callouts like this make hints and tips visible only to those who want to see them.\n\n\n\nBring different resources together. Exercises provide an ideal opportunity to draw together different strands of the The Data Shrink platform: case studies could provide inspiration or pointers for how to tackle a particular challenge; explainers might contain useful information about the tools and techniques to apply to specific types of data; and if you’re looking for a suitable set of data, it may already be listed in our datasets section.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "contributor-docs/datasets.html",
    "href": "contributor-docs/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "It’s easy to find datasets online. What’s more difficult is finding quality datasets that are suitable for specific training and development needs. On The Data Shrink we aim to solve that problem.\nOur Datasets section will provide a curated list of recommended datasets along with detailed notes and guidance on what each dataset contains, how it is structured, and how best to make use of it. In particular, we want to highlight messy rather than pristine datasets – ones that capture the imperfections and oddities found in real-world data – so that users can practice not only data analysis and modelling, but data cleaning and preparation too!",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "contributor-docs/datasets.html#structure",
    "href": "contributor-docs/datasets.html#structure",
    "title": "Datasets",
    "section": "Structure",
    "text": "Structure\nIf you have a dataset to recommend, your submission must cover the following areas:\n\nDataset name\nLink to source\nWhat data science tasks/methods can this dataset be used to demonstrate?\nHave you used this dataset for your own teaching/learning? (see Advice and recommendations below)\nWhy was the dataset originally created?\nWhen was it created?\nWho created it?\nLicences/restrictions?\nSize of dataset\nData types/description\nReal/synthetic data?",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "contributor-docs/datasets.html#advice-and-recommendations",
    "href": "contributor-docs/datasets.html#advice-and-recommendations",
    "title": "Datasets",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nHelp others to make good use of your recommended dataset. If you’ve had experience using a recommended dataset for your own teaching and learning, please consider creating an exercise for platform users to complete. If you encountered the dataset as part of a training course, competition or exercise created by a third party, make sure to give them a namecheck.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2024/01/03/posit-conf-video.html",
    "href": "viewpoints/editors-blog/posts/2024/01/03/posit-conf-video.html",
    "title": "Creating a web publication with Quarto: the The Data Shrink origin story",
    "section": "",
    "text": "When I attended posit::conf(2023) in Chicago last year, I gave a talk about creating The Data Shrink using Quarto, the open source publishing system developed by Posit. That talk is now online, along with all the other conference talks and keynotes.\nMy talk, “From Journalist to Coder: Creating a Web Publication with Quarto,” is embedded below. You can also find a selection of talks on our posit::conf highlights blog. The full conference playlist is on YouTube.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “Creating a web publication with Quarto: the The Data Shrink origin story.” The Data Shrink, January 03, 2024. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2024/01/18/cherry-blossom.html",
    "href": "viewpoints/editors-blog/posts/2024/01/18/cherry-blossom.html",
    "title": "When will the cherry trees bloom? Get ready to make and share your predictions!",
    "section": "",
    "text": "The 2024 International Cherry Blossom Prediction Competition will open for entries on February 1, and The Data Shrink is once again proud to be a sponsor.\nContestants are invited to submit predictions for the date cherry trees will bloom in 2024 at five different locations – Kyoto, Japan; Liestal-Weideli, Switzerland; Vancouver, Canada; and Washington, DC and New York City, USA.\nThe competition organisers will provide all the publicly available data they can find for the bloom dates of cherry trees in these locations, and contestants will then be challenged to use this data “in combination with any other publicly available data (e.g., climate data) to provide reproducible predictions of the peak bloom date.”\n“For this competition, we seek accurate, interpretable predictions that offer strong narratives about the factors that determine when cherry trees bloom and the broader consequences for local and global ecosystems,” say the organisers. “Your task is to predict the peak bloom date for 2024 and to estimate a prediction interval, a lower and upper endpoint of dates during which peak bloom is most probable.”\nSo that organisers can reproduce the predictions, entrants must submit all data and code in a Quarto document.\nThere’s cash and prizes on offer for the best entries, including having your work featured on The Data Shrink. Head on over to the competition website for full details and rules.\nAnd, if you are looking for some inspiration, check out this tutorial on the law of the flowering plants, written by Jonathan Auerbach, a co-organiser of the prediction competition.\nGood luck to all entrants!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo by AJ on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “When will the cherry trees bloom? Get ready to make and share your predictions!” The Data Shrink, January 18, 2024. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2024/02/08/llms-whitepaper-response.html",
    "href": "viewpoints/editors-blog/posts/2024/02/08/llms-whitepaper-response.html",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "",
    "text": "The UK government this week announced a £10 million investment to “jumpstart regulators’ AI capabilities” as part of its commitment to a “pro-innovation approach to AI regulation.” But will this be sufficient to answer criticisms that it has so far been “too slow” to give regulators the tools they need to police the growing usage of AI?\nIt was March last year when a Department for Science, Innovation and Technology (DSIT) white paper first set out the government’s principles- and context-based approach to regulating artificial intelligence. This proposed to focus regulatory attention on “the context in which AI is deployed” rather than target specific technologies. Under this model, existing regulators, including the Information Commissioner’s Office, Ofcom, and the Competition and Markets Authority, would be responsible for ensuring that technologies deployed within their domains adhered to established rules – e.g., data protection regulation – and a common set of principles:\nThe approach was broadly well received, as was clear from a debate at techUK’s Digital Ethics Summit last December. However, concerns were expressed about whether regulators would be funded sufficiently to meet the expectations set out in the March white paper. Also, the Royal Statistical Society, in its response to the white paper, worried that “splitting responsibilities for regulating the use of AI between existing regulators does not meet the scale of the challenge,” and that “central leadership is required to give a clear, coherent and easily communicable framework that can be applied to all sectors.”\nWhile the DSIT white paper proposed that a range of “central functions” be created to support regulators, evidence presented to a House of Lords inquiry last November suggested that regulators “did not appear to know what was happening” with these mooted teams and were “keen to see progress” on this front.\nIn reporting the outcomes of its inquiry last week, the House of Lords Communications and Digital Committee concluded that government was being “too slow” to give regulators the tools required to meet the objectives set out in the white paper, and that “speedier resourcing of government‑led central support teams is needed.”\n“Relying on existing regulators to ensure good outcomes from AI will only work if they are properly resourced and empowered,” the committee said.\nThe £10 million funding for regulators announced this week is therefore likely to be welcomed. Money is earmarked to “help regulators develop cutting-edge research and practical tools to monitor and address risks and opportunities in their sectors, from telecoms and healthcare to finance and education,” according to a DSIT press release. Speaking on February 6 at a hearing of the Lords Communications and Digital Committee, Michelle Donelan, Secretary of State for Science, Innovation and Technology, said that the government would “stay on top” of what regulators need to be able to fulfil their responsibilities for regulating the use of AI in their sectors."
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#consultation-response",
    "href": "viewpoints/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#consultation-response",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Consultation response",
    "text": "Consultation response\nNews of the funding for regulators came as part of a long-awaited response by the government to the consultation on its AI regulation white paper. The response essentially confirmed that the government was proceeding with its principles- and context-based approach to regulating AI, having received “strong support from stakeholders across society.”\nThis approach is right for today, the government said, “as it allows us to keep pace with rapid and uncertain advances in AI.” However, it acknowledged that “the challenges posed by AI technologies will ultimately require legislative action in every country once understanding of risk has matured.”\n“Highly capable general-purpose AI systems” would, for example, present a particular challenge to the government’s current approach. It explained: “Even though some regulators can enforce existing laws against the developers of the most capable general-purpose systems within their current remits, the wide range of potential uses means that general-purpose systems do not currently fit neatly within the remit of any one regulator, potentially leaving risks without effective mitigations.”\nAs a next step in delivering on the white paper approach, the government is asking key regulators to publish an update on their strategic approach to AI by the end of April. This was welcomed by Royal Statistical Society (RSS) president Andrew Garrett, who said:\n\n“Urgency is certainly warranted, and the directive for key regulators to disclose their approach in the coming months is a positive development. Ensuring consistency and coherence not only among key regulators but also those who follow is crucial.”\n\nGarrett also reiterated the need for government to engage with statisticians and data scientists, particularly through its new AI Safety Institute (AISI). In the white paper consultation response, AISI is billed as being “fundamental to informing the UK’s regulatory framework”: it will “advance the world’s knowledge of AI safety by carefully examining, evaluating, and testing new frontier AI systems” and will also “research new techniques for understanding and mitigating AI risk.” Garrett said:\n\n“As always, fostering diversity of representation within government and regulatory bodies remains paramount; it cannot solely rely on input from major tech companies. It is especially important that the AI Safety Institute engages with a diverse array of voices, including statisticians and data scientists who play a pivotal role in both the development of AI systems and novel evaluation methodologies.”"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#risks-and-opportunities",
    "href": "viewpoints/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#risks-and-opportunities",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Risks and opportunities",
    "text": "Risks and opportunities\nCalls for a “diversity of representation within government and regulatory bodies” certainly chime with a warning bell sounded by the Lords Communications and Digital Committee last week, in the February 2 release of its inquiry report into large language models and generative AI. “Regulatory capture” by big commercial interests was highlighted as a danger to be avoided, amid concern that “the AI safety debate is being dominated by views narrowly focused on catastrophic risk, often coming from those who developed such models in the first place” and that “this distracts from more immediate issues like copyright infringement, bias and reliability.”1\nThe committee called for enhanced governance and transparency measures in DSIT and AISI to guard against regulatory capture, and for a rebalancing away from a “narrow focus on high-stakes AI safety” toward a “more positive vision for the opportunities [of AI] and a more deliberate focus on near-term risks” including cyber security and disinformation.\nIt also wants to see greater action by the government in support of copyright. “Some tech firms are using copyrighted material without permission, reaping vast financial rewards,” reads the report. “The legalities of this are complex but the principles remain clear. The point of copyright is to reward creators for their efforts, prevent others from using works without permission, and incentivise innovation. The current legal framework is failing to ensure these outcomes occur and the Government has a duty to act. It cannot sit on its hands for the next decade and hope the courts will provide an answer.”\nAgain, here’s RSS president Andrew Garrett’s take on the Lords committee report:\n\n\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Yaopey Yong on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach.” The Data Shrink, February 8, 2024. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#footnotes",
    "href": "viewpoints/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#footnotes",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee, for example, “No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye.”↩︎"
  },
  {
    "objectID": "viewpoints/interviews/posts/2024/03/11/democratizing-data.html",
    "href": "viewpoints/interviews/posts/2024/03/11/democratizing-data.html",
    "title": "Democratizing Data: Using natural language processing and machine learning to capture dataset usage",
    "section": "",
    "text": "Figuring out how much money is spent annually on collecting and publishing datasets is a challenge. According to the World Bank, it is “painfully hard to obtain” information just on government spending on data, never mind all the other bodies and organisations who invest in the creation of data assets. But there’s an even more challenging figure to pin down: How much value does all this data provide? By and large, there is very little data – or systematic collection of data – on dataset usage. There’s no easy way to find all the users of a particular dataset and to see how the data has been used, or what research topics it may have contributed to.\nWriting in the Harvard Data Science Review in April 2022, Julia Lane and others explained that “the current approach to finding what data sets are used to answer scientific questions … is largely manual and ad hoc.” They went on to argue: “Better information on the use of data is likely to have at least two results: (i) government agencies might use the information to describe the return on investment in data sets and (ii) scientists might use the information to reduce the time taken to search and discover other empirical research.”\nThis hope for a better understanding of how datasets are used and the value they provide underpins the creation of “Democratizing Data: A Search and Discovery Platform for Public Data Assets.”\nAs described on the platform’s homepage, Democratizing Data “describes how datasets identified by federal agencies have been used in scientific research. It uses machine learning algorithms to search over 90 million documents and find how datasets are cited, in what publications, and what topics they are used to study.”\nBut this is just the start of what Lane thinks the project could eventually achieve, as she explains in this interview.\nHow did the Democratizing Data platform come about?\nIt emerged from three things, really, and I can trace the start of it back to 2016, when I was asked to build a secure environment that could host confidential microdata in order to inform the work of the Commission on Evidence-Based Policymaking.1 They asked me to lead on this because I had built the NORC remote access Data Enclave at the University of Chicago 10 years before that.\nThis was the first step towards Democratizing Data.\nThe second step was figuring out how to create value from the data in the secure environment, because we knew that if we couldn’t create value, government agencies weren’t going to put their data into it.\nBut how do you figure out what agencies are going to want to do with the data in a secure environment? It’s difficult to get them to tell you what they want, so I thought, well, why don’t we build training classes, put people in these training classes and have them work on problems with their own data? That way, we’re going to know what problems they have so that we can address them.\nSo, step 1 was build secure environment. Step 2 was build capacity and identify the questions that are of interest to agencies so that they would put data into the secure environment. That led to the training classes that Frauke Kreuter, Rayid Ghani, and I put together, which are described here.\nBut then what happened was, people kept coming to me in the classes and asking, “Who else has worked with these data, and who can I go to and ask questions?”\nI could give them a list of people, but that list would be biased by my age and race and sex and the people I know. What about those people who are doing really interesting stuff with these data that I happen not to know about?\nSo, that’s how Democratizing Data got started. I thought, really the best way to give a full answer to those sorts of questions is to figure out what datasets are being used in research publications.\nNow, how was I going do that? I could read all the publications and manually write notes about who the authors were, and what the topics were, and what datasets they used. But that’s not realistic. So, I thought, well, maybe we could combine natural language processing techniques with machine learning so that you could “read” all these publications and find out how datasets are cited.\nWould this have been a problem that needed solving if there were common, established citation standards and practices for datasets?\nThere are great citation practices for datasets, and we’ve had them for 15 years. Back when I was a National Science Foundation (NSF) program officer, everyone was saying, well, if we just get the plumbing right, people will come and use it. Well, they don’t. Even when there are DOIs available and they’re relatively easy to cite, people don’t. The plumbing’s there, we built it, but they didn’t come.\nSo, I think there has to be a demand-side piece, and we talk about that in one of the papers in an upcoming special issue of the Harvard Data Science Review: How do you create an incentive structure so that people do provide information about how they’ve used data? My thinking was that, suppose we can find out who’s using what data. Then the incentive structure to an academic is “your name in lights”: you are the world’s living expert in orange carrots with green stripes, or whatever. So, we would read the publications, find the datasets, and then you could have a leaderboard of the people who have done the most work in a particular field, and then people would have an incentive both to cite datasets and to let you know when you miss things. And that was when I thought, well, let’s put all this information up in a dashboard.\nSo, is the grand vision for this to create a platform that, essentially, any data owner – anyone who publishes datasets – could plug into, connect to, and understand how other people are using their datasets?\nThat’s right. The grand vision is basically to set up a search and discovery portal. Originally, my thinking was that would be super helpful for people just starting out in a field; for a new graduate student or a postdoc to say, “I want to figure out what work has been done on recidivism of welfare recipients relative to access to jobs and neighborhood characteristics,” for example, and for them to see what datasets are available and how they have been used.\nBut from the data producer side it’d also be useful to know: Who’s using the datasets? Where are the gaps? Where are we maybe not reaching as many people as we thought we were, and how can we change that?\nSo, while the original idea was to build a platform for researchers, plans changed when the Evidence Act passed, and agencies were required to produce usage statistics for their datasets.2\nWe started a pilot with the US Department of Agriculture’s (USDA) Economic Research Service (ERS). They have been a huge supporter and helped us work through a lot of the issues. Then, when we began showing around the ERS wireframes and ideas, the NSF National Center for Science and Engineering Statistics joined in, and so did USDA’s National Agricultural Statistics Service and the National Center for Education Statistics and the National Oceanic and Atmospheric Administration.\nSo, we have these agencies involved and they’ve really been the drivers, the intellectual partners, pushing the design and the structure forward.\nI’ve had a chance to play around with some of the public dashboards you’ve released on Tableau, and I really like the way you can explore dataset usage from different start points and end up with a list of publications that use those datasets. My question is, though, how have you connected all this up – datasets and publications?\nOur start point was scientific publications because these are pretty well curated. We ended up working with Elsevier because Scopus [Elsevier’s abstract and citation database] is a well-curated corpus and they’ve got the associated publication metadata well curated.\nSo, we have the Scopus corpus, and we then ran a Kaggle competition to develop machine learning models to identify candidate snippets of text from scientific papers that seem like they might be referring to a dataset.\nHuman researchers would then validate those snippets as either referring to a dataset or not, and once they’ve validated the publication-to-dataset dyad, we then pull in all the metadata associated with the publication: authors, institutions, key topics, publication year, countries, etc. – all this information gets piped over to the dashboards.\nYou published a Harvard Data Science Review article about this competition a couple of years ago, and from that I understand that you can actually get quite far with a simple string-matching method for finding datasets, but you would still miss a lot of citations using this approach because of the variability in the way people refer to datasets.\nThat’s right. There were three different models that were developed, and each one picks up different aspects of how authors mention data in publications, and all three have been extremely useful. We learned a lot about the variety of ways in which researchers cite the data that they use.\nIt turns out that more people do cite datasets in references than we had originally thought, but usually they don’t cite a DOI, they cite the URL or they cite the exact name of the dataset, so string search of references and URLs pulls out quite a lot of information that the DOIs, per se, don’t.\nWhat are the next steps for scaling up the Democratizing Data work?\nI think it is more of a sociotechnical issue than a technical one. We have the plumbing, but really what we need to do is to figure out the incentives for researchers. We need to build a community around the data, which is what’s happened with code and the sharing of code on platforms like GitHub.\nObviously, our initial focus has been on federal statistical data, but there’s also a lot of interest in how administrative data or streaming data are being used.\nThe advantage of starting with statistical data is that they have names. As we learn more about citation patterns, though, it may be that we don’t need precise names. What may happen is that the community starts converging on common terminologies for datasets. That happens in a lot of fields.\nAt the moment, it feels a little bit like the Wild West. We’re searching for ways to measure how datasets are used, how they’re valued, and so on. Federal statistical data and Elsevier’s Scopus have been a great starting point for us, but the broader vision is to incorporate other datasets and other publication databases like arXiv and Semantic Scholar. But all those other datasets that are out there, they need to be curated and documented in some way and that’s a huge task, so the solution has got to be community curation and sharing, right?\nIf we don’t build a community around the data, we’re just going to have really bad information, really bad analysis, and really bad statistics on the value that our datasets – all these data assets – provide. My colleague Nancy Potok gave a talk a couple of days ago in which she said that our future depends on this – and it really does."
  },
  {
    "objectID": "viewpoints/interviews/posts/2024/03/11/democratizing-data.html#footnotes",
    "href": "viewpoints/interviews/posts/2024/03/11/democratizing-data.html#footnotes",
    "title": "Democratizing Data: Using natural language processing and machine learning to capture dataset usage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Commission on Evidence-Based Policymaking was “charged with examining all aspects of how to increase the availability and use of government data to build evidence and inform program design, while protecting privacy and confidentiality of those data.”↩︎\nSpecifically, the act requires federal agencies to identify and implement methods “for collecting and analyzing digital information on data asset usage by users within and outside of the agency.”↩︎"
  },
  {
    "objectID": "rwds-partners.html",
    "href": "rwds-partners.html",
    "title": "Our partners and funders",
    "section": "",
    "text": "The Data Shrink is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including The Data Shrink and other projects, become a member today.\nEmail: info@rss.org.uk",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#publisher",
    "href": "rwds-partners.html#publisher",
    "title": "Our partners and funders",
    "section": "",
    "text": "The Data Shrink is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including The Data Shrink and other projects, become a member today.\nEmail: info@rss.org.uk",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#partners",
    "href": "rwds-partners.html#partners",
    "title": "Our partners and funders",
    "section": "Partners",
    "text": "Partners\n\n\n\n\n\nThe American Statistical Association is the world’s largest community of statisticians, the “Big Tent for Statistics.” It is the second-oldest, continuously operating professional association in the US. Since it was founded in Boston in 1839, the ASA has supported excellence in the development, application, and dissemination of statistical science through meetings, member services, education, publications, advocacy, and accreditation.\nOur members serve in industry, government, and academia in more than 90 countries, advancing research and promoting sound statistical practice to inform public policy and improve human welfare.\nTo support the work of the ASA, become a member today.\nEmail: asainfo@amstat.org",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#funders",
    "href": "rwds-partners.html#funders",
    "title": "Our partners and funders",
    "section": "Funders",
    "text": "Funders\n\n\n\n\n\nThe Data Shrink was supported by startup funding from The Alan Turing Institute.\nThe Alan Turing Institute, headquartered in the British Library, London, was created as the UK’s national institute for data science in 2015. In 2017, as a result of a government recommendation, artificial intelligence was added to its remit. \nThe Institute is named in honour of Alan Turing (23 June 1912 – 7 June 1954), whose pioneering work in theoretical and applied mathematics, engineering and computing are considered to be the key disciplines comprising the fields of data science and artificial intelligence.\nTo find out more about The Alan Turing Institute, its strategy and programme of work, visit turing.ac.uk.",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "case-studies/posts/2024/05/08/dpm.html",
    "href": "case-studies/posts/2024/05/08/dpm.html",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "",
    "text": "Decisions around medium and long-term allocation of healthcare resources are fraught with challenges and uncertainties, which explains the use of blunt resource allocations based on across-the-board annual percentage uplifts.\nThe Bristol, North Somerset, South Gloucestershire Integrated Care Board (BNSSG ICB - we love elaborate acronyms in the National Health Service!), in the south west of England, is part of the local NHS apparatus responsible for planning the current and future health needs of the one million resident population.\n\n\n\n\n  \n\n\n\nFigure 1: A map of the area covered by BNSSG, a space covered by three local authorities, with about 1 million people living inside it."
  },
  {
    "objectID": "case-studies/posts/2024/05/08/dpm.html#background",
    "href": "case-studies/posts/2024/05/08/dpm.html#background",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "",
    "text": "Decisions around medium and long-term allocation of healthcare resources are fraught with challenges and uncertainties, which explains the use of blunt resource allocations based on across-the-board annual percentage uplifts.\nThe Bristol, North Somerset, South Gloucestershire Integrated Care Board (BNSSG ICB - we love elaborate acronyms in the National Health Service!), in the south west of England, is part of the local NHS apparatus responsible for planning the current and future health needs of the one million resident population.\n\n\n\n\n  \n\n\n\nFigure 1: A map of the area covered by BNSSG, a space covered by three local authorities, with about 1 million people living inside it."
  },
  {
    "objectID": "case-studies/posts/2024/05/08/dpm.html#population-segmentation",
    "href": "case-studies/posts/2024/05/08/dpm.html#population-segmentation",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Population Segmentation",
    "text": "Population Segmentation\nBefore tackling the complex problem of forecasting healthcare resources into the future, we first need to understand the current situation regarding the distribution of health needs.\nWhile every individual has a unique set of circumstances, population segmentation is an approach used to help understand overall need by combining individuals into different groups, based on certain criteria.\nWe use the Cambridge Multimorbidity Score which is a metric designed to summarise the presence of multiple health conditions, known as multimorbidity. Using that score, which applies different weights to different health conditions, we previously found a way of splitting the adult (17+) population into five Core Segments, with Core Segment 1 patients having the lowest score and being the least ill and Core Segment 5 being those with the most multimorbidity.\nApplied to the BNSSG adult population (of around 750K individuals), the following interesting properties were found:\n\nHalving: Going up one segment results in roughly half the number of people in that segment\nDoubling: Going up one segment results in roughly twice the NHS monetary spend per person per year\n\nWe can see this in Figure 2.\n\n\n\n\n\n\nFigure 2: Halving-Doubling Effect of the Core Segments"
  },
  {
    "objectID": "case-studies/posts/2024/05/08/dpm.html#sec-creating-the-model",
    "href": "case-studies/posts/2024/05/08/dpm.html#sec-creating-the-model",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Creating The Model",
    "text": "Creating The Model\nTo forecast health needs of the population, in terms of how many people will be in which Core Segment in what future year, the Dynamic Population Model (DPM) takes information from two different sources:\n\nThe Office for National Statistics projections for our area. From this, we get yearly projections for not just the total 17+ population, but also the predicted number of people turning 17 (and so entering our model), deaths, and in- and out-ward migration.\nNHS patient attribute and activity data, stored in the System Wide Dataset (SWD). This gives us: past and current information on the adult population’s NHS healthcare usage; the Core Segment breakdown of our current and past populations; the proportion of those turning 17, migrating, and dying that are in each Core Segment. From this, we estimate the historical rates of transition within Core Segments, which is essentially the yearly number of people getting sicker or healthier.\n\nBy synthesising these pieces of data, we create our DPM forecast. Starting from the most up to date Core Segment population breakdown, the model takes yearly time steps into the future, at each time step using the inputs to estimate how many people are to be in each Core Segment. This modelling approach of having discrete time steps and different movements between states can be set up as a Markov chain, although here we have formulated it as a set of difference equations - through which the outflow of each Core Segment population at each time step is deterministic. The design was led by Zehra and Christos, through a collaboration between the NHS and the Centre for Healthcare Innovation and Improvement (CHI2) at the University of Bath.\nThe model can be thought of as having the following inputs:\n\n\n\n\n\n\n\n\nModel Input\nDescription\nData Source\n\n\n\n\ninitial population\nThe starting number of people in each Core Segment\nSWD\n\n\ninner transition matrix\nThe yearly proportions of people moving from one Core Segment to another\nSWD\n\n\nbirths, net migration, deaths - numbers\nThe yearly number of people moving in and out of the area\nONS\n\n\nbirths, net migration, deaths - proportions\nThe proportion of births/migrations/deaths that come from each Core Segment group\nSWD\n\n\n\nFrom these inputs, it deterministically outputs the yearly forecasts for the number of people in each Core Segment. From these yearly Core Segment population figures, we can also forecast use by point of delivery by taking historic SWD information on the activity used by current Core Segment breakdown, under the assumption that stays the same into the future.\nWe combine these population health segment projections – i.e., how many people will be in which Core Segment in what future year – with recent NHS healthcare usage data to yield forecasted changes for various delivery points, like Emergency Department (ED) visits or maternity service appointments."
  },
  {
    "objectID": "case-studies/posts/2024/05/08/dpm.html#findings",
    "href": "case-studies/posts/2024/05/08/dpm.html#findings",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Findings",
    "text": "Findings\nThe first output of the model is the population forecast for each Core Segment, as plotted in Figure 3. The visualisation is a type of sankey diagram called an alluvial plot, which shows the proportion of people moving between the Core Segments each year. As it is to be expected, the majority of individuals stay in the same Core Segment year-on-year as the process of acquiring conditions and developing multimorbidity takes places over many years and decades.\nThe concerning insight shown in Figure 3 is that all Core Segments apart from (the most healthy) Core Segment 1 are due to increase in size, with Core Segment 5 having the largest percentage increase over the next 20 years. While, at first glance, this could be attributed to the effect an ageing population, in which people are staying alive for longer we will see in the next set of results that this itself does not wholly explain the forecasted Core Segment changes.\n\n\n\n\n\n\nFigure 3: All Core Segments, except the most healthy (CS1), are forecast to increase in size. BNSSG Population rescaled to have an initial population of 1,000.\n\n\n\nIn applying the typical NHS healthcare usage per Core Segment to the projections of Figure 3, we derive the expected future healthcare usage for various healthcare settings (Figure 4). In overlaying to these the equivalent projections due solely to demographic factors (both for total population size and capturing the effect of Age and Sex), we see that the DPM projections for increased resource use are not solely attributable to an ageing and growing population, but also to a population becoming gradually less healthy over time.\nSpecifically, from Figure 4 we can glean the following insights:\n\nIn all areas except Maternity, the DPM forecasts an increased use beyond just the growing, aging population. The reason that Maternity can be explained as the exception is due to it closely following the demographic changes forecast, specifically for numbers of women of child bearing age.\nFor Community contacts, with the highest proportion of use from Core Segment 5 patients, the DPM forecasts the highest increase into the future. This is because, relative to current size, the number of Core Segment 5 patients is set to increase the largest and so that has the largest impact on Community contacts, which include home visits to patients to support rehabilitation and services to manage long-term mobility issues such as physiotherapy.\nWhilst Secondary Elective and Non-Elective activity is forecast to grow at similar rates, the Carbon and Cost values are forecast to grow more for Secondary Non-Elective due to the average Carbon and Cost usage per person in Core Segment 5 being higher. In this context ‘Secondary’ is a hospital stay, with ‘Elective’ being planned and ‘Non-Elective’ being unplanned. For example, a hip replacement is elective whereas an admission following a road traffic accident is non-elective.\n\n\n\n\n\n\n\nFigure 4: Forecasts by activity, carbon, and cost for four different points of delivery."
  },
  {
    "objectID": "case-studies/posts/2024/05/08/dpm.html#limitations",
    "href": "case-studies/posts/2024/05/08/dpm.html#limitations",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Limitations",
    "text": "Limitations\n\nIt’s difficult to make predictions, especially about the future.\n– Danish Proverb\n\nAs with any modelling / forecasting method, there are limitations to be mindful of.\n\nThe cost and activity usage estimates are made under the assumption that we will continue to deliver services as they are currently being delivered. We know this isn’t going to be true, as healthcare-seeking behaviour evolves over time, with younger people accessing healthcare in different ways to previous generations. On top of that, healthcare advances can result in significant changes in healthcare provision, in ways unaccounted for within this model.\nThe model is tied to ONS forecasts for population change, and robust forecasting is hard. It is difficult to estimate what the population will look like in 20 years’ time, and the influence of uncertain and unknown future local development and housing plans. Having said this, population forecasts tend to be robust, one way to consider this is that everyone who will be an adult by the end of the forecast in 20 years’ time has already been born.\nThe DPM does not explicitly account for the interaction of demand and capacity: it simply predicts future healthcare resource requirement assuming that health needs of a given Core Segment patient are met in the same way they are met now. This is an essential assumption to help ensure legitimate use of the empirically derived Core Segment transition rates. However, it inevitably limits practical use, as flexing demand and capacity assumptions is of importance to planners and service managers.\nIt is not possible to validate the model on historic data, firstly because of point 3. above but also because we only have good quality SWD information for the past two years, so cannot reliably look further back into the past and create a forecast that we can check against what actually happened.\nWhilst it is possible to use the model in other healthcare systems and geographic areas, the underlying data required to generate the Core Segments is non-trivial, so significant data pipelining may be required to get to create local model inputs, as explained above in Section 3."
  },
  {
    "objectID": "case-studies/posts/2024/05/08/dpm.html#what-next",
    "href": "case-studies/posts/2024/05/08/dpm.html#what-next",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "What Next",
    "text": "What Next\nWe have already generated local use cases for the DPM in forecasting different geographical areas or specific hospital trusts. We envisage the DPM becoming a standard tool in most forward planning initiatives and will continue to refine the model as more information becomes available both for calibration and validation.\nOutside of BNSSG, we are keen to disseminate our modelling approach to others who may be interested, as well as expanding our collaboration. There are also other innovative approaches in this space, such as the Health in 2040 report by the Health Foundation which looks at England-level and uses the same ONS forecasts, but using a different ‘micro simulation’ modelling approach.\n\nIf long-term forecasting in the NHS is of interest to you and your work, we’d love to chat! Please get in touch at bnssg.analytics@nhs.net"
  },
  {
    "objectID": "case-studies/posts/2024/05/08/dpm.html#summary",
    "href": "case-studies/posts/2024/05/08/dpm.html#summary",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Summary",
    "text": "Summary\nReliably forecasting longer-term population health needs and healthcare resource requirements is essential if the NHS is to effectively plan for tomorrow’s problems today.\nWhile this is undoubtedly a difficult problem – both conceptually and statistically – our modelling, undertaken through an academic-NHS collaboration, demonstrates that there are alternatives beyond the commonly-used but simplistic approaches based only on demographic factors.\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nLuke Shaw is a Data Scientist working in the NHS.\n\n\nRich Wood is Head of Modelling Analytics at BNSSG ICB and Senior Visiting Research Follow at University of Bath School of Management.\n\n\nChristos Vasilakis is Director of the Centre for Healthcare Innovation and Improvement (CHI2), and Professor at the University of Bath School of Management.\n\n\nZehra Onen Dumlu is a Research Associate at CHI2 and Lecturer at the University of Bath.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Luke Shaw\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nShaw, Luke et al 2024. “Forecasting the Health Needs of a Changing Population” The Data Shrink, May 08, 2024. URL"
  },
  {
    "objectID": "LICENCE.html",
    "href": "LICENCE.html",
    "title": "Licence",
    "section": "",
    "text": "The website realworlddatascience.net (“the website”) and the “The Data Shrink” and “RWDS” brands and logos are copyright © Royal Statistical Society.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page, e.g.:\n\n\n\nExample of copyright and licence information from an RWDS article.\n\n\nWe make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish the website in its entirety.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "LICENCE.html#content",
    "href": "LICENCE.html#content",
    "title": "Licence",
    "section": "",
    "text": "The website realworlddatascience.net (“the website”) and the “The Data Shrink” and “RWDS” brands and logos are copyright © Royal Statistical Society.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page, e.g.:\n\n\n\nExample of copyright and licence information from an RWDS article.\n\n\nWe make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish the website in its entirety.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "LICENCE.html#software-and-services",
    "href": "LICENCE.html#software-and-services",
    "title": "Licence",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for the website are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct.\nThe website is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nThe Data Shrink is hosted by GitHub Pages.\nThe website uses Google Analytics 4 for web analytics reporting.\nFonts used on The Data Shrink are served by the Google Fonts API. This is to improve site loading speeds and font compatibility across devices. Review the Google Fonts Privacy and Data Collection statement.\nUser comments and reaction functionality is provided by giscus, a comments system powered by GitHub Discussions. Use of this comment functionality is governed by the Contributor Covenant Code of Conduct.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "ideas/posts/2024/06/04/ai-series-6.html",
    "href": "ideas/posts/2024/06/04/ai-series-6.html",
    "title": "AI series: What is “best practice” when working with AI in the real world?",
    "section": "",
    "text": "Over the course of the The Data Shrink AI series, we’ve had articles laying out the nitty gritty of what AI is, how it works, or at least how to get an explanation for its output as well as burning issues around the data involved, evaluating these models, ethical considerations, and gauging societal impacts such as changes in workforce demands. The ideas in these articles give a firm footing for establishing what best practice with AI models should look like but there is often a divide between theory and practice, and the same pitfalls can trip people up again and again. Here we discuss how to wrestle with real world limitations and flag these common hazards.\nOur interviewees, in order of appearance, are:\nAli Al-Sherbaz, academic director in digital skills at the University of Cambridge in the UK\nJanet Bastiman, Napier chief data scientist and chair of the Royal Statistical Society Data Science & AI Section\nJonathan Gillard, professor of statistics/data science at Cardiff University, and a member of the The Data Shrink Board\nFatemeh Torabi, senior research officer and data scientist, health data science at Swansea University, and also a member of the The Data Shrink board\nIt is often said that while almost everybody is now trying to leverage AI in their projects, most AI projects fail. What nuggets of wisdom do the panel have for swelling that minority that succeed with their AI projects, and what should you do before you start doing anything?\nAli Al-Sherbaz: It’s not easy to start, especially for people who are not aware how AI works. My advice is, first, they have to understand the basics of how AI works because the expectation could be overpromising, and that is a danger. Just 25 years ago, a master dissertation might be about developing a simple – we call it simple now but it was a master’s project 25 years ago – a simple model with a neural network of a combination of nodes to classify data. Whatever the data is – it could be drawing shapes, simple shapes, square, circle triangle – just classifying them was worth an MSc. Now, kids can do it. But that is not the same as understanding what the neural network or the AI is. It’s a matrix of numbers, and actually, for the learning process each does multiple iterations to find the best combination of these numbers – product of sum; sum of product – to classify, to do something, and train them for a certain situation, and that is a supervised learning. Over the last 25 years – especially in the last 10 years – the computational power is getting better, so AI is now working better.\nThere are other things people have to learn. There’s the statistics as well, and of course people who would like to work in AI and data science must understand the data, and they should also be experts in the data itself. For instance, I can talk about cybersecurity, I can talk about networking and other things, but if it comes to something regarding health data, or financial services, or stock markets, I’m not an expert in the data. So I’m not going to be actively working on those things even if I use the same AI tools. This is in a nutshell why I think some people fail sometimes using AI, or they succeed using AI. And we should emphasise the human value. The AI is there, and it exists to help us to make a better more accurate decision, but the human value is still there. We have to insist on that.\nJanet Bastiman: I would just like to build on all of that great stuff that Ali’s just said. When you look at basically the non-data scientist side of it, you often get businesses who think AI can solve a certain problem. They might go out and hire a team – whether that’s directly or indirectly – and get them to try and solve a problem that, as Ali said, they may not have the domain expertise for. The business might not even have the right data for it, and AI might not even be the right way of solving that problem. I think that’s one of the fundamental things to think about – really understanding what you’re trying to solve, and how you’re going to solve it before you start throwing complex tools, and potentially very expensive teams at the problem.\nWhen you look at a lot of the failures, it’s been because businesses have just gone, we can solve this problem, I’m just going to hire a team and let these intelligent people look at something. And then they’re restricted on the data that they’ve got, which won’t even answer the question; they’re restricted on the resources they have; and even restricted in terms of wider buy in from the company. So really understanding what is it that you want to solve? What are you trying to do? Is AI the right thing? And can you even do it with the resources you have available? And I think that’s, that’s a fundamental starting point. Because, you can have wonderful experts, who have that domain knowledge, who understand the statistics, and all that essential stuff that Ali just said. But then if from a business point of view, if you don’t give them the right data to work on, or you don’t let them do their job and tell you when they can’t do their job, then again, you’re going to be doomed to failure.\nJonathan Gillard: Explainability is a big issue when it comes to AI models, as well. They are at the moment, very largely “black box” – data goes in, then these models get trained on dumb data and answers get popped out. And when it works, well, it works fabulously well. And we’ve seen lots of examples of that happening. But often for business, industry or real life, we want to learn. We want to understand the laws of the universe, and to understand the reasons why this answer came about. Because this explainability piece is missing – because everything is hidden away almost – I think that’s a big issue in successful execution. And particularly when it comes to industries where there’s a degree of regulation there as well, if you can’t explain how a particular input arose to a particular output, then how can you justify to regulatory bodies that what you’ve got is satisfactory, ethical, and that you’re learning and you’re doing things in the right way?\nThere have been efforts at trying to get explanations from these models. How do you think things are progressing there?\nJG: Yeah, that’s a good question. I think where we are with explainability is in very simple scenarios, very simple models. This is where traditional statistical models do very well. There’s an explicit model which says if you put these things inside then you’ll get this output. So [for today’s AI] I think we’re actually very far away from having that complete explainability picture, particularly as we fetishise more and more grand models. The AI models are only getting bigger, more complex, and that makes the explainability per se even more challenging. And that’s why I think, as Ali says, at the moment, the human in the loop is absolutely crucial.\nWhat AI does share with classical statistics (or classical data science if you want to call it that) is it can still only be as good as the data that’s put into it, that’s still a fundamental truth. I think a lot of the assumptions currently with AI models – and this is where there could be a few trip ups is that it can create something from nothing. It’s “artificial intelligence” – almost the wording suggested it’s artificial. But fundamentally, we still need a robust and reliable comprehensive source of data there in order to train these models in the first place.\nIn terms of having outsourced expertise for these projects– does that make more problems if you’re then trying to understand what this AI has done?\nJB: Oh, hugely. Let’s say that domain expertise – that’s something Ali touched on –you’ve got to understand your data. Because even that fundamental initial preparation of data before you try and train anything is absolutely crucial – really looking at where are the gaps? Where are the assumptions? How is this data even being collected? Has it been manipulated before you got to it? If you don’t understand your industry, well enough you won’t know where those pitfalls might be – and a lot of teams do this, they just take the data, and then they just put it in, turn the handle and out comes something and it looks like it’s okay. What they’re really missing there – because they’re not putting that effort in to really understand those inputs, what the models are doing, they’re just turning the handle until they get something that feels about right – what they miss out is where it goes wrong. And there are some industries, where the false positives and false negatives from classification or the bad predictions from running things really have a severe human impact. And if you don’t understand what’s going in, and the potential impact of what comes out, then it’s very, very easy to just churn these things out and go, “it’s 80% accurate, but that’s fine” without really understanding the human impact of the 20% [that it gets wrong].\nGoing back to what Jon said about that explainability, it’s so crucial. It is challenging, and it is difficult, but going from these opaque systems to more transparent systems – we need that for trust. As humans, we divulge our trust very differently, depending on the impact. One of the examples I use all the time is, you know, sort of weather prediction stuff, you know, we don’t really care too much, because it’s not got a huge impact. But when you look at sort of financials or medicals, we really, really want to know that that output is good, and how we got to that output. The Turing Institute’s come out with some great research that says, as humans, if we want to understand why when another human has told us something, then we want the same thing from the models, and that can vary from person to person. So building that explainable level into everything we do, has to be one of the things we think about upfront. But you’ve got to really, truly deeply understand that data. And it’s not just a question of offloading a data set to a generalist who can turn that handle, otherwise you will end up with huge, huge problems.\nFatemeh Torabi: I very much agree with all the points that my colleagues raised. I also think it’s very important that we know why we are doing things. Having those incremental stages in our planning for any project, and then having a vision of where we see AI can contribute into this process and can give us further efficiency – and how – is very important. If we don’t have defined measures to see how this AI algorithm is contributing to this specific element of the project, we can get really lost bringing these capabilities on board. Yes, it might generate something, but how we are going to measure that something is very important. I think, as members of the scientific community, we must all view AI as a valuable tool. However, it has its own risks and benefits.\nFor example, in healthcare when we use AI for risk predictions, it can be a really great tool to aid clinicians to save time. However, in each stage, we need to assess the data quality, how these data are fed into the algorithm, what procedures, what models, and how we generate those models. And then which discriminative models do we use to balance the risk and eventually predict the risk of outcomes in patients? It’s very much a balance between risks and benefits for usefulness of these tools in practice. We have all these brilliant ideas of what best practice is. But in real terms, sometimes it’s a little bit tricky to follow through.\nCould you give us some thoughts on the sort of best practice with data, for example, that doesn’t quite turn out to be quite so easy to follow in practice, and what you might do about it?\nFT: We always call these AI algorithms, data hungry algorithms, because the models that we fit require us to see patterns in the data that we feed into them so that the learning happens. And then the discriminative functions come in place to balance and kind of give a score to wherever the learning is happening and give an evaluation of each step. However, the data that we put into these algorithms comes first – the quality of that data. Often in healthcare, because of its sensitivity, the data is held within a secure environment. So we cannot, at this point in time, expose an AI algorithm to a very diverse example, specifically for investigating rare diseases or rare conditions. And above that, there is also complexities in the data itself. We need to evaluate and clean the data before we feed it into these algorithms. We need to evaluate the diversity of the data itself – for example, the tabular data, the imaging data, the genomic data – and each one requires its own specific or tailored approach in data cleaning stages.\n\n\n\n\n\n\nFigure 1: The panel. Clockwise from top left: Ali Al-Sherbaz, Janet Bastiman, Fatemeh Torabi and Jonathan Gillard\n\n\n\nWe also have another level that is now being discovered in the health data science community, which is the generation of synthetic data. We can give AI models access to these synthetic versions of the data that we hold. However, that also has its own challenges because it requires reading the patterns from real data, and then creating those synthetic versions of data.\nFor example, Dementia Platforms UK is one of the pioneers in developing this. We hold a range of cohort data, patients’ data, genomics data and imaging data. In each one of these when we try to develop those processing algorithms, there are specific tailored approaches that we need to consider to ensure we are actually creating a low fidelity level of data that is holding some of the patterns in it for the AI algorithm to allow the learning to happen. However, we also need to consider whether it is safe enough so that we can ensure the data provided are secure to be released for use at a lower governance level compared to the actual data. So there are quite a lot of challenges, and we captured a lot of it in our article.\nA A-S: I can talk about the cybersecurity and other relevant data network security, the point being the amount of data we receive to analyse. It’s really huge. And when I say huge I mean about one gigabyte, probably in a couple of hours, or one terabyte in a week – that’s huge. One gigabyte of a text file – if I printed out this file with A4 – that would leave me with a stack of A4 paper, three times the Eiffel Tower.\nNow, if I have cyber traffic, and try to detect any cyber attack, AI helps with that. However, if we train this model properly, they have to detect cyber attacks in real time – when I say real time, we’re talking about within microseconds or a millisecond – and the decision has to be correct. AI alone doesn’t work, doesn’t help. Humans should also intervene, but rather than having 100,000 records to check for a suspected breach, AI can reduce that to 100. A human can interact with that. And then in terms of the authentication or verification, humans alongside AI can learn whether this is a false positive, or a real attack or a false negative. This is a challenge in the cybersecurity area.\nJB: I just wanted to dive in from the finance side – again the data is critical, and we have very large amounts of data. However in addition – and I think we probably suffer from the same sort of problem that Ali does in this – when I’m trying to detect things, there are people on the other side actively working against what I’m trying to detect, which I suppose is a problem that maybe Fatemeh doesn’t have in healthcare.\nWhen you’re trying to build models to look for patterns, and those patterns are changing underneath you, it can be incredibly difficult. I have an issue that all of my client’s data legally has to be kept separated – some of it has to be kept in certain parts of the world so we can’t put that into one place. We can try and create synthetic data that has the same nuances of the snapshots that we can see at any one point in time, and we can try and put that together in one place, but what we can detect now will very quickly not be what we need to detect in a month’s time. As soon as transactions start getting stopped, as soon as suspicious activity reports are raised, and banks are fined, everything switches and how all of that financial crime occurs, changes. And it’s changing, on a big scale worldwide, but also subtly because, there are a team of data scientists on the other side trying desperately to circumvent the models that me and my team are building. It’s absolutely crazy. So while I would love to be able to pull all of the data that I have access to in one place and get that huge central visual view, legally I can’t do that because of all the worldwide jurisdictional laws around data and keeping it in certain places.\nThen I’ve also got the ethical side of it, which is something that Fatemeh touched on. If I get it wrong, that can have a material impact on usually some of the most marginalised in society. The profile of some of the transactions that are highly correlated with financial crime are also highly correlated with people in borderline poverty, even in Western countries. So false positives in my world have a huge, huge ethical impact. But at the same time, we’re trying really hard to minimise those false negatives – that balance is critical, and the data side of it is such a problem.\nFatemeh mentioned the synthetic side of it. There’s a huge push, particularly in the UK to get good synthetic data to really showcase some of these things that we’re trying to detect. But by the time you get that pooling, and the synthesising of data that you can ethically use and share around without fear of all the legal repercussions, what we’re trying to detect has already moved on. So we’re constantly several steps behind.\nI imagine Ali has similar problems in the cybercrime space in that as soon as things are detected, the ways in which they work move on. So there’s an awful lot I think that, as an industry, although we have different verticals, we can share best practices on.\nIs there a demand for new types of expertise?\nA A-S: There is a huge gap in the in the UK, at least and worldwide about finding people working as a data scientist or working with the data. So we created a course in Cambridge, which we call the data science career accelerator for people who work in data, and would like to move on and learn more. We did market research, and we interviewed around 50 people between CEO and head of security and head of data scientists, in science departments and in industry, to tell us – what kind of skills are you after? What problems do you currently have? And then we designed this course.\nWe found that first of all there are people who don’t know from where to start – what kind of data they need, what tools they have to learn with… Even if they learn the tools, they still need to learn what kind of machine learning process to use. And then suddenly, we have ChatGPT turned out, and the LLM [large language model] development – all of that in one course, it is a real challenge.\nThe course has started now, the first cohort. The big advice from industry we have is that during the course they have to work on real world case studies, on scenarios with data that nobody has touched before – that is, it’s new, not public. We teach them on a public data, but companies also have their own data, and we get consent from them to use that data for the students so we can test the skills they learned on virgin data that nobody has touched before.\nWe just started this month, and the students are going to start with the first project now. They are enjoying the course but that is the challenge we have now. How did we handle that? It’s to work together with the industry side by side, even during the delivery. We have an academic from Cambridge, and we have experts from the industry to support the learners to learn to get the best of both worlds.\nThe industry has changed so much in the last couple of years. Does that mean that the expertise and demands are also changing very quickly or is there a common thread that you can work with?\nA A-S: Well, there is a common thread, but having new tools – I mean, Google just released Gemini, and that’s a new skill they have learnt and been tested on, and looked into how others feel about it and compared it to ChatGPT, or Claude 3 or Copilot. That’s all happened in the last 12 months. And then, of course, reacting on that, reflecting on the material, teaching the material – it’s a challenge. It’s not easy and you need to find the right person. Of course, people who have this kind of experience are in demand, and it’s hard to secure these kinds of human resources as well as to deliver the course. So there are challenges and we have to act dynamically and be adaptive.\nWhat are your thoughts on the evaluation of these models, and how to manage the risk of something that you haven’t thought of before, and the role of regulation.\nJG: I think a lot of our discussions at the moment are assuming that we’ve got well meaning, well intentioned people and well meaning, well intentioned companies and industries, who are trying to seek to do their best ethically and regulatorily and with appropriate data, and so on. But there is a space here for bad actors in the system.\nUnfortunately, digital transformation of human life will happen in a good and bad way – unfortunately, I think there are going to be those two streams to this. Individuals are very capable now of making their own large language models by following a video guide if they wanted to, and having that data is, of course going to enable them maybe to do bad things with it.\nData is already a commodity in quite a strong way, but I do think we have to visit data security, and even the risks of open data as well. We live in a country, which I think does very well in producing lots of publicly available data. But that could be twisted in a way that we might not expect. And when I speak of those things, we’re usually thinking of groundwork – writing and implementing your own large language models – but there were recent examples of where just by using very clever prompting of existing large language models, you could get quite dangerous material, shall we say, which circumnavigated inbuilt existing safeguards. Again, that’s an emerging thing that we have to have to try and address as it comes on.\nI think my final point with ethics and regulation is it will rapidly evolve, and it will rapidly change. And a story which I think can illustrate that is, when the first motorcar was introduced into the UK, it was law for a human to walk in front of the motorcar with a large red flag to warn passers-by of the incoming car because people weren’t really familiar with it. Now, of course, that’s in distant memory, right? We don’t have people with red flags, walking in front of cars. I do wonder, in 20 years or 50 years, what will the ethical norms regarding AI and its use be? Likewise, will we have deregulation? That seems to be the common theme in history that when we get more familiar with things, we deregulate because we’re more comfortable with their existence. That makes me quite curious about what the future holds.\nFT: Jon raised a very interesting point and Janet touched upon keeping financial data in silos but we are facing this in healthcare as well. Data has to be checked within a trusted research environment or secure data environment that’s making the data silos. However, efforts at this point in time are on enhancing these digital platforms to bring data and federal data together. Alongside what is happening in terms of our progression towards development of a new ethical or legal requirement, is documenting what is being practised at the moment, because at the moment there are quite a lot of bubbles. Each institution has their own data and applies their own rules to it. So understanding what it is that we are currently working on – the data flows that are flowing into the secure environments – is building the basis of developments that are going on in terms of developing standardisation and common frameworks. A lot of projects have been focused on understanding the current to develop on it for the future.\nWe know for example, the Data Protection Act, put forward some specific requirements, but that was developed in 2018, before we had this massive AI consideration. In my academic capacity as well, we are facing what Jon mentioned, in terms of the diversity of assessments for students. For example, when we ask these questions, even if the data is provided within the course and within this defined governance, we know that the answers can possibly be aided by AI – a model. So we are defining more diverse assessment methods in academic practice to ensure that we have a way to evaluate the outcome that we are receiving by the human eye, rather than being blinded by what we receive from AI, and then calling it high quality output, whether in research practice or in academic practice. So there’s quite a lot of consideration of these issues, I think that is bringing our past knowledge to the current point where we now have to balance between human and machine interactions in every single process that we are facing.\nHow does this change the skill set required of data scientists, as AI is getting more and more developed?\nA A-S: Regarding the terminology of data scientists, when we talk about data we immediately link that with statistics, and statistics is an old topic. There has been an accumulation of expertise for 100 years, to the best of my knowledge or more in statistics, and people who are new to data analysis or data, have to learn about this legacy. And when we develop the course, we should mention these skills in statistics and build this knowledge on top, that is, when we reach the right point, then we talk about learning or machine learning, supervised and unsupervised, and about LLM – these are the new skills they have to learn. As I mentioned, it’s tricky when we teach learners about it, we have to provide them with simple datasets to teach them something complex in statistics because it’s a danger to teach both [data and statistics at the same time] – we will lose them, they will lose concentration and it’s hard to follow up. So, a little bit of statistics – they have to learn the basics like normal distribution, the distribution, the type, and what does it mean when we have these distributions, the meaning of the data – and that is the point I made earlier about how people should have a sense for the numbers. What does it mean, when I say 0.56 in healthcare? Is that a danger? 60% – is that OK? In cybersecurity, if the probability of attack today is 60% should I inform the police? Should I inform someone; is that important? Or for example, for the stock market? Say we have dropped off 10% – Is that something we have to worry about? So making sense of the numbers is part of it.\nThat is part of personalised learning because it depends on their background or what they have learned – it’s not straightforward, and it has to be personalised not just for people taking the course now, for instance for someone who is 18 years old coming from their A levels. No, it’s for a wide range. People from diverse courses like to approach this data science course. And now we are in the era of people who are in social science, and engineering, doctors, journalism, art, they are all interested in learning a little bit of data science, and utilising AI for their benefit. So there is no one answer.\nYou emphasise that people still need to be able to make sense of numbers. We’re often told that AI will devalue knowledge and devalue experience – it sounds like you don’t feel that’s the case.\nA A-S: I have to stick with the following: human value is just that – value. AI without humans is worth nothing. I have one example: In 1997, some software was developed for chess, to play against a human, and for the first time, that computer programme (called AI now) beat Kasparov. Guess what happened? Did chess disappear? No, we still value human to human competition. The value of the human is the same for art and for music. So we still have human value, and we have to maintain that for the next generation. They shouldn’t lose this human value, and handover to AI value, which I feel is zero without the human.\nJ B: I think one of the things we are seeing is that diversity in people’s backgrounds coming into data science, which is fantastic, because I think that really helps with the understanding of when things can go wrong, and how things can be misused. If you have this cookie cutter set of people that have all got a degree from the same place and all had the same experience, which is very similar – this happens a lot in the financial industry where there’s like five universities that all feed into the banks – they all think and solve problems in the same way because that’s how they’ve been trained. But as soon as you start bringing in people with different backgrounds, they’re the ones that say, hang on, this is a problem. So having those different backgrounds is really useful.\nBut then as Ali said there’s so many people who call themselves a data scientist that don’t understand data, or science. And I think he was absolutely right. If you’ve got a probability of 60%, or you’ve got a small standard deviation, when is that an issue? What do you really understand about that based on your industry, and based on your statistical knowledge? That’s so so key. And it’s something that a lot of people who are self-trained and call themselves data scientists have missed out on. So coming back to your original question about is it harder or is it easier, in some respects, it’s a lot harder, because someone who calls himself a data scientist now needs to do everything from basically fundamental research, trying to make models better, you’ve got to understand statistics, you’ve got to understand machine learning, engineering, production, isolation, efficiencies, effectiveness, ethics – it’s this huge, huge sphere. And it’s too much for one person. So you’ve really got to have well balanced teams and support. Because you can’t keep on top of your game across all of those. It’s just not possible. So I think that becomes really difficult. When I look at how things have changed, there’s so many basic principles from, you know, the 80s and 90s, in standard, good quality computer programming and testing. And I think the one thing that we’re really missing as an industry is a specialist AI testing role. Someone who understands enough about how models work and how they can go wrong and can do the same thing for AI solutions, as good QA analysts can do for standard software engineering models. Someone who can really test them to extremes with what happens when I put the wrong data in.\nWe saw this – there were a couple of days under COVID, where all the numbers went wrong, because the data hadn’t been delivered correctly, or not enough of it had been delivered. There were no checks in place to say, actually, we’ve only got 10% of what we were expecting, so don’t automatically publish these results. It’s things like that, that we really need to make sure are built into the systems because those are the things that, again, could cause problems. As soon as you get a model that’s not doing the right thing – going back to our original question – when they do go wrong, you can then find a company pulls that model even though it could be easily fixed. And then they’re disillusioned with AI, and won’t use it. That’s that whole project, and all of the expense and investment on that just thrown away when a bit more testing and understanding could have saved it.\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nAnna Demming is a freelance science writer and editor based in Bristol, UK. She has a PhD from King’s College London in physics, specifically nanophotonics and how light interacts with the very small, and has been an editor for Nature Publishing Group (now Springer Nature), IOP Publishing and New Scientist. Other publications she contributes to include The Observer, New Scientist, Scientific American, Physics World and Chemistry World..\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. \n\n\n\nHow to cite\n\nDemming, Anna. 2024. “What is “best practice” when working with AI in the real world?.” The Data Shrink, June 4, 2024. URL"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html",
    "title": "Creating Christmas cards with R",
    "section": "",
    "text": "When you think about data visualisation in R (R Core Team 2022), you’d be forgiven for not jumping straight to thinking about creating Christmas cards. However, the package and functions we often use to create bar charts and line graphs can be repurposed to create festive images. This tutorial provides a step-by-step guide to creating a Christmas card featuring a snowman – entirely in R. Though this seems like just a fun exercise, the functions and techniques you learn in this tutorial can also transfer into more traditional data visualisations created using {ggplot2} (Wickham 2016) in R.\nThe code in this tutorial relies on the following packages:"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#lets-build-a-snowman",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#lets-build-a-snowman",
    "title": "Creating Christmas cards with R",
    "section": "Let’s build a snowman!",
    "text": "Let’s build a snowman!\nBefore we jump in to writing R code, let’s take a step back and think about what you actually need to build a snowman. If you were given some crayons and a piece of paper, what would you draw?\nYou might draw two or three circles to make up the head and body. Perhaps some smaller dots for buttons and eyes, and a (rudimentary) hat constructed from some rectangles. Some brown lines create sticks for arms and, of course, a triangle to represent a carrot for a nose. For the background elements of our Christmas card, we also need the night sky (or day if you prefer), a light dusting of snow covering the ground, and a few snowflakes falling from the sky.\nNow lines, rectangles, circles, and triangles are all just simple geometric objects. Crucially, they’re all things that we can create with {ggplot2} in R."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#build-a-snowman-with-r",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#build-a-snowman-with-r",
    "title": "Creating Christmas cards with R",
    "section": "Build a snowman with R",
    "text": "Build a snowman with R\nLet’s start with the background. The easiest way to start with a blank canvas in {ggplot2} is to create an empty plot using ggplot() with no arguments. We can also remove all theme elements (such as the grey background and grid lines) with theme_void(). To change the background colour to a dark blue for the night sky, we can edit the plot.background element of the theme using element_rect() (since the background is essentially just a big rectangle).\nIn {ggplot2} fill is the inner colour of shapes whilst colour is the outline colour. You can specify colours in different ways in R: either via the rgb() function, using a character string for a hex colour such as \"#000000\", or using a named colour. If you run colors(), you’ll see all the valid named colours you can use. Here, we’ve picked \"midnightblue\".\nLet’s save this initial plot as an object s1 that we’ll keep adding layers to. Saving plots in different stages of styling as objects can help to keep your code more modular.\ns1 &lt;- ggplot() +\n  theme_void() +\n  theme(\n    plot.background = element_rect(\n      fill = \"midnightblue\"\n      )\n  )\ns1\nNext we’ll add some snow on the ground. We’ll do this by drawing a white rectangle along the bottom of the plot. There are two different functions that we could use to add a rectangle: geom_rect() or annotate(). The difference between the two is that geom_rect() maps columns of a data.frame to different elements of a plot whereas annotate() can take values passed in as vectors. Most of the {ggplot2} graphs you’ll see will use geom_*() functions. However, if you’re only adding one or two elements to a plot then annotate() might be quicker.\nSince we’re only adding one rectangle for the snow, it’s easier to use annotate() with the \"rect\" geometry. This requires four arguments: the minimum and maximum x and y coordinates of the rectangle – essentially specifying where the corners are. We can also change the colour of the rectangle and its outline using the fill and colour arguments. Here, I’ve used a very light grey instead of white.\nIf we don’t set the axis limits using xlim() and ylim(), the plot area will resize to fit the area of the snow rectangle. The night sky background will disappear. You can choose any axis limits you wish here – but the unit square will make it easier to find the right coordinates when deciding where to position other elements. Finally, we add coord_fixed() to fix the 1:1 aspect ratio and make sure our grid is actually square with expand = FALSE to remove the additional padding at the sides of the plot.\ns2 &lt;- s1 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0, xmax = 1,\n    ymin = 0, ymax = 0.2,\n    fill = \"grey98\",\n    colour = \"grey98\"\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  coord_fixed(expand = FALSE)\ns2\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo finish off the background, we’ll add some falling snowflakes. We first need to decide where on the plot the snowflakes will appear. We’ll be plotting lots of snowflakes, so manually typing out the coordinates of where they’ll be would be very inefficient. Instead, we can use functions to generate the locations randomly. For this we’ll use the uniform distribution. The uniform distribution has two parameters – the lower and upper bounds where any values between the bounds are equally likely. You can generate samples from a uniform distribution in R using the runif() function.\nWhen generating random numbers in R (or any other programming language), it’s important to set a seed. This means that if you give your code to someone else, they’ll get the same random numbers as you. Some people choose to use the date as the random seed and since we’re making Christmas cards, we’ll use Christmas day as the random seed – in yyyymmdd format, of course!\nWe create a variable n specifying how many snowflakes we’ll create. Creating a variable rather than hard coding the variables makes it easier to vary how many snowflakes we want. Since our plot grid goes between 0 and 1 in both the x and y directions, we generate random numbers between 0 and 1 for both the x and y coordinates and store the values in a data.frame called snowflakes.\nset.seed(20231225)\nn &lt;- 100\nsnowflakes &lt;- data.frame(\n  x = runif(n, 0, 1),\n  y = runif(n, 0, 1)\n)\nNow we can plot the snowflakes data using geom_point() – the same function you’d use for a scatter plot. Since we’re using a geom_*() function, we need to tell {ggplot2} which columns go on the x and y axes inside the aes() function. To plot the snowflakes, we’re going to make using of R’s different point characters. The default when plotting with geom_point() is a small black dot, but we can choose to use a small star (close enough to a snowflake!) by setting pch = 8 and changing the colour to \"white\".\ns3 &lt;- s2 +\n  geom_point(\n    data = snowflakes,\n    mapping = aes(\n      x = x,\n      y = y\n    ),\n    colour = \"white\",\n    pch = 8\n  )\ns3\nNow comes the part where we start rolling up some snowballs! Or, in the case of an R snowman, we draw some circles. Unfortunately, there isn’t a built-in geom_*() function in {ggplot2} for plotting circles. We could use geom_point() here and increase the size of the points but this approach can look a little bit fuzzy when the points are very large. Instead, we’ll turn to a {ggplot2} extension package for some additional geom_* functions - {ggforce} (Pedersen 2022).\nThe geom_circle() function requires at least three elements mapped to the aesthetics inside aes(): the coordinates of the centre of the circle given by x0 and y0, and the radii of each of the circles, r. Instead of creating a separate data frame and passing it into geom_circle(), we can alternatively create the data frame inside the function. The fill and colour arguments work as they do in {ggplot2} and we can set both to \"white\".\ns4 &lt;- s3 +\n  geom_circle(\n    data = data.frame(\n      x0 = c(0.6, 0.6),\n      y0 = c(0.3, 0.5),\n      r = c(0.15, 0.1)\n    ),\n    mapping = aes(x0 = x0, y0 = y0, r = r),\n    fill = \"white\",\n    colour = \"white\"\n  )\ns4\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use geom_point() again to add some more points to represent the buttons and the eyes. Here, we’ll manually specify the coordinates of the points. For the buttons we add them in a vertical line in the middle of the snowman’s body circle, and for the eyes we add them in a horizontal line in the middle of the head circle.\nSince no two rocks are exactly the same size, we can add some random variation to the size of the points using runif() again. We generate five different sizes between 2 and 4.5. For reference, the default point size is 1.5. Adding scale_size_identity() means that the sizes of the points are actually equally to the sizes we generated from runif() and removes the legend that is automatically added when we add size inside aes().\ns5 &lt;- s4 +\n  geom_point(\n    data = data.frame(\n      x = c(0.6, 0.6, 0.6, 0.57, 0.62),\n      y = c(0.25, 0.3, 0.35, 0.52, 0.52),\n      size = runif(5, 2, 4.5)\n    ),\n    mapping = aes(x = x, y = y, size = size)\n  ) +\n  scale_size_identity()\ns5\nTo add sticks for arms, we can make use of geom_segment() to draw some lines. We could also use geom_path() but that is designed to connect points across multiple cases, whereas geom_segment() draws a single line per row of data – and we don’t want to join the snowman’s arms together!\nTo use geom_segment() we need to create a data frame containing the x and y coordinates for the start and end of each line, and then pass this into the aesthetic mapping with aes(). We can control the colour and width of the lines using the colour and linewidth arguments. Setting the lineend argument to \"round\" means that the ends of the lines will be rounded rather than the default straight edge.\ns6 &lt;- s5 + \n  geom_segment(\n    data = data.frame(\n      x = c(0.46, 0.7),\n      xend = c(0.33, 0.85),\n      y = c(0.3, 0.3),\n      yend = c(0.4, 0.4)\n    ),\n    mapping = aes(x = x, y = y, xend = xend, yend = yend),\n    colour = \"chocolate4\",\n    lineend = \"round\",\n    linewidth = 2\n  )\ns6\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ll now add a (very simple) hat to our snowman, fashioned out of two rectangles. We can add the rectangles as we did before using the annotate() function and specifying the locations of the corners of the rectangles. We start with a shorter wider rectangle for the brim of the hat, and then a taller, narrower rectangle for the crown of the hat. Since we’ll colour them both \"brown\", it doesn’t matter if they overlap a little bit.\nThis might be one of the situations we should have used geom_rect() instead of annotate() but it might take a lot of trial and error to position the hat exactly where we want it, and this seemed a little easier with annotate().\ns7 &lt;- s6 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0.46, xmax = 0.74,\n    ymin = 0.55, ymax = 0.60,\n    fill = \"brown\"\n  ) +\n  annotate(\n    geom = \"rect\",\n    xmin = 0.50, xmax = 0.70,\n    ymin = 0.56, ymax = 0.73,\n    fill = \"brown\"\n  )\ns7\nNow we can move on to the final component of building a snowman – the carrot for his nose! We’re going to use a triangle for the nose. Unfortunately, there are no built-in triangle geoms in {ggplot2} so we’ll have to make our own. There are different ways to do this, but here we’re going to make use of the {sf} package (Pebesma 2018). The {sf} package (short for simple features) is designed for working with spatial data. Although we’re not working with maps, we can still use {sf} to make shapes – including polygons.\nWe start by constructing a matrix with two columns – one for x coordinates and one for y. The x coordinates start in the middle of the head and go slightly to the right for the triangle point. The y coordinates take a little bit more trial and error to get right. Note that although triangles only have three corners, we have four rows of points. The last row must be the same as the first to make the polygon closed. The matrix is then converted into a spatial object using the st_polygon() function, and we can check how it looks using plot().\nnose_pts &lt;- matrix(\n  c(\n    0.6, 0.5,\n    0.65, 0.48,\n    0.6, 0.46,\n    0.6, 0.5\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\nnose &lt;- st_polygon(list(nose_pts))\nplot(nose)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can plot sf objects with {ggplot2} using geom_sf(). geom_sf() is a slightly special geom since we don’t need to specify an aesthetic mapping for the x and y axes – they are determined automatically from the sf object along with which type of geometry to draw. If your sf object has points, points will be drawn. If it has country shapes, polygons will be drawn. Like other geom_*() functions, we can change the colour and fill arguments to a different colour – in this case \"orange\" to represent a carrot!\nYou should see a Coordinate system already present. Adding new coordinate system, which will replace the existing one. message when you run the following code. The is because geom_sf forces it’s own coordinate system on the plot overriding our previous code specifying coord_fixed(). If you run it without the coord_sf(expand = FALSE), the extra space around the plot will reappear. We can remove it again with expand = FALSE.\ns8 &lt;- s7 +\n  geom_sf(\n    data = nose,\n    fill = \"orange\",\n    colour = \"orange\"\n  ) +\n  coord_sf(expand = FALSE)\ns8\n\nYou could skip the sf part of this completely and pass the coordinates directly into geom_polygon() instead. However, I’ve often found it quicker and easier to tinker with polygon shapes using sf.\n\nA key part of any Christmas card is the message wishing recipients a Merry Christmas! We can add text to our plot using the annotate() function and the \"text\" geometry (you could instead use geom_text() if you prefer). When adding text, we require at least three arguments: the x and y coordinates of where the text should be added, and the label denoting what text should appear. We can supply additional arguments to annotate() to style the text, such as: colour (which changes the colour of the text); family (to define which font to use); fontface (which determines if the font is bold or italic, for example); and size (which changes the size of the text). The \"mono\" option for family tells {ggplot2} to use the default system monospace font.\ns9 &lt;- s8 +\n  annotate(\n    geom = \"text\",\n    x = 0.5, y = 0.07,\n    label = \"Merry Christmas\",\n    colour = \"red3\",\n    family = \"mono\",\n    fontface = \"bold\", size = 7\n  )\ns9"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#sending-christmas-cards-in-r",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#sending-christmas-cards-in-r",
    "title": "Creating Christmas cards with R",
    "section": "Sending Christmas cards in R",
    "text": "Sending Christmas cards in R\nNow that we’ve finished creating our Christmas card, we need to think about how to send it. You could save it as an image file using ggsave(), print it out, and send it in the post. Or you could also use R to send it!\nThere are many different R packages for sending emails from R. If you create a database of email addresses and names, you could personalise the message on the Christmas card and then send it automatically as an email from R. If you want to automate the process of sending physical cards from R, you might be interested in the {ggirl} package from Jacqueline Nolis (Nolis 2023). {ggirl} allows you to send postcards with a ggplot object printed on the front. {ggirl} is also an incredible example of an eCommerce platform built with R! Note that {ggirl} can currently only send physical items to addresses in the United States."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#other-christmas-r-packages",
    "href": "ideas/tutorials/posts/2023/12/12/xmas-cards.html#other-christmas-r-packages",
    "title": "Creating Christmas cards with R",
    "section": "Other Christmas R packages",
    "text": "Other Christmas R packages\nIf you’re curious about making Christmas cards with R but you don’t have the time to make them from scratch, you’ll likely find the christmas R package (Barrera-Gomez 2022) helpful. This package from Jose Barrera-Gomez can generate lots of different Christmas cards, many of them animated and available in different languages (English, Catalan and Spanish).\nEmil Hvitfeldt has also created a Quarto extension that gives the effect of falling snowflakes on HTML outputs – including revealjs slides which is perfect for festive presentations!\nHave you made your own Christmas cards with R? We’d love to see your designs!\n\n\n\n\n\n\nInspired by Nicola’s tutorial, The Data Shrink has indeed made its own Christmas card design. Check out our attempt over at the Editors’ Blog!\n\n\n\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nNicola Rennie is a lecturer in health data science in the Centre for Health Informatics, Computing, and Statistics (CHICAS) within Lancaster Medical School at Lancaster University. She’s an R enthusiast, data visualisation aficionado, and generative artist, among other things. Her personal website is hosted at nrennie.rbind.io, and she is a co-author of the Royal Statistical Society’s Best Practices for Data Visualisation.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Nicola Rennie\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nRennie, Nicola. 2023. “Creating Christmas cards with R.” The Data Shrink, December 12, 2023."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2024/6/25/nowcasting-3step.html",
    "href": "ideas/datasciencebites/posts/2024/6/25/nowcasting-3step.html",
    "title": "Nowcasting upgrade for better real time estimation of GDP and inflation",
    "section": "",
    "text": "Governments, policymakers and central banks across the world are wrestling to keep rising prices under control using monetary policies such as interest rate increases. The effectiveness of such policy changes should be assessed by monitoring inflation data as well as studying the impact on real GDP, making timely and accurate access to key economic indicators crucial for policy planning. The delay in publishing economic indicators such as Real GDP, inflation and other labour related series, makes this real time assessment of the economy particularly challenging. Now Menzie Chinn at the University of Wisconsin, Baptiste Meunier at the European Central Bank and Sebastian Stumpner at the Banque de France report an approach for “nowcasting” built on previous research that develops a framework using different machine learning techniques and is flexible and adaptable compared with traditional methods1. They report on the accuracy of their 3-step framework for nowcasting global trade volume estimates, showing how it can outperform traditional methods. They also highlight that the 3-step framework can be extended beyond World Trade data.\nNowcasting, an amalgamation of the term now and forecasting, provides a methodology to assess the current state of the economy by predicting the current value of inflation or Real GDP. The Federal Reserve Bank of New York and Federal Reserve Bank of Atlanta have used nowcasting to publish real time GDP estimates, for the USA. Similarly, the Federal Reserve Bank of Cleveland estimates real time inflation using nowcasting methods.\nThe basic principle of nowcasting is utilising information that is published early such as using data published at higher frequency, survey data, financial indicators or economic indicators. For example, the running estimate of Real GDP (aka GDPNow) that the Federal Reserve Bank of Atlanta provides is updated 6 or 7 times a month on weekdays when one of the 7 input data sources are released. Similarly, the real GDP growth estimate that the Federal Reserve Bank of New York provides is based on data releases in categories such as housing and construction, manufacturing, surveys, retail and consumption, income, labour, international trade, prices and others.\nThe traditional methods of nowcasting do not provide an integrated framework, and forecasters need to know which variables to use, and select a method for factor extraction and machine learning regression. Chinn, Meunier and Stumpner propose a sequential framework that selects the most important predictors. The selected variables are then summarized using Principal Component Analysis (PCA) and these factors are used as explanatory variables to perform the regression. Although traditional methods of nowcasting also utilized many of these techniques, the authors test various combinations of pre-selection, factor extraction and regression techniques and propose a combination that improves model accuracy."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2024/6/25/nowcasting-3step.html#model-framework-improved-flexibility-and-accuracy",
    "href": "ideas/datasciencebites/posts/2024/6/25/nowcasting-3step.html#model-framework-improved-flexibility-and-accuracy",
    "title": "Nowcasting upgrade for better real time estimation of GDP and inflation",
    "section": "Model framework improved flexibility and accuracy:",
    "text": "Model framework improved flexibility and accuracy:\nThe 3 steps in the framework are chronological steps to be performed in which the first step is pre-selection of the independent variables with the highest predictive power. The independent variables from the first step are then summarised into a few factors using factor extraction methodology in the second step. The final step consists of using the factors from step 2 to perform regression.\n\n\n\n\n\n\nFigure 2: The various methods that can be employed in the 3 step framework in Chinn et al (2024). Credit: National Bureau of Economic Research.\n\n\n\nFigure 2 summarises the various methods employed at each step in the 3 step framework. In their report Chinn, Meunier and Stumpner aim to propose the best techniques for pre-selection, factor extraction and regression. As such their 3-step framework comprises performing pre-selection using Least Angle Regression (LARS), factor extraction using Principal Component Analysis (PCA) and employing a Macroeconomic Random Forest (MRF) machine learning technique for nowcasting.\nThe model performance or accuracy of MRF is compared with traditional methods using Root Mean Square Error (RMSE), a measure of the deviation between the actual data and the predicted data. The 3-step framework model accuracy is tested by holding the preselection and factor extraction fixed to isolate the impact of regression techniques.\n\n\n\n\n\n\nFigure 3: Bar chart comparing the accuracy of different methods in terms of RMSE. Credit: National Bureau of Economic Research.\n\n\n\nFigure 3 compares the RMSE of traditional methods, machine learning tree and machine learning regression model for backcasting (t-2 and t-1), nowcasting (t) and forecasting (t+1). It highlights the greater model accuracy of MRF and Gradient Boosting compared with traditional models and tree models for backcasting, nowcasting and forecasting."
  },
  {
    "objectID": "ideas/datasciencebites/posts/2024/6/25/nowcasting-3step.html#whats-next",
    "href": "ideas/datasciencebites/posts/2024/6/25/nowcasting-3step.html#whats-next",
    "title": "Nowcasting upgrade for better real time estimation of GDP and inflation",
    "section": "What’s Next?",
    "text": "What’s Next?\nOrganisations such as The Nowcasting Lab provide GDP estimates for European countries. Such nowcasting techniques have been employed by humanitarian agencies including the United Nations Refugee Agency (UNHCR) which uses nowcasting to estimate the actual forced displaced population. The nowcasting techniques, dashboards and tools have been implemented and accepted as a reliable source of information at government organisations for policy making, central banks, and financial organisations. The 3-step framework, proposed by Chinn, Meunier and Stumpner, is easily adaptable, flexible and provides higher accuracy, which will be valuable to a range of fields employing nowcasting.\n\nExplore more data science ideas\n\n\n\n\n\nAbout the author\n\nAtmajitsinh Gohil is an independent researcher in the field of AI and ML, specifically managing AI and ML risk. He has worked with consulting firm assisting clients in model risk management. He has graduated from SUNY, Buffalo with a M.S. in Economics.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Atmajitsinh Gohil\n\n\n  Text, code, and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) International licence, except where otherwise noted. Thumbnail image by Shutterstock Van Fink.\n\n\n\nHow to cite\n\nGohil, Atmajitsinh. 2024. “Nowcasting upgrade for better real time estimation of GDP and inflation.” The Data Shrink, June 25, 2024. URL"
  },
  {
    "objectID": "ideas/datasciencebites/posts/2024/6/25/nowcasting-3step.html#footnotes",
    "href": "ideas/datasciencebites/posts/2024/6/25/nowcasting-3step.html#footnotes",
    "title": "Nowcasting upgrade for better real time estimation of GDP and inflation",
    "section": "References",
    "text": "References\n\n\nNowcasting World Trade with Machine Learning: a Three-Step Approach Chinn, M. D., Meunier, B. & Stumpner, S. NBER DOI 10.3386/w31419) ↩︎"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: rwds@rss.org.uk\nGitHub: @realworlddatascience\nZenodo: The Data Shrink community\nLinkedIn: RSS The Data Shrink\nX: @rwdatasci\nMastodon: @rwdatasci",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "contact.html#editorial",
    "href": "contact.html#editorial",
    "title": "Contact us",
    "section": "",
    "text": "Email: rwds@rss.org.uk\nGitHub: @realworlddatascience\nZenodo: The Data Shrink community\nLinkedIn: RSS The Data Shrink\nX: @rwdatasci\nMastodon: @rwdatasci",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "contact.html#advertising-and-commercial",
    "href": "contact.html#advertising-and-commercial",
    "title": "Contact us",
    "section": "Advertising and commercial",
    "text": "Advertising and commercial\n\nEmail: advertising@rss.org.uk",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "careers/posts/2024/03/27/pholborn-interview.html",
    "href": "careers/posts/2024/03/27/pholborn-interview.html",
    "title": "Data science and AI in the public sector: An interview with ONS’s Penny Holborn",
    "section": "",
    "text": "Back to Careers\n\n\n\n\n\n\nAbout the author\n\nJonathan Gillard is a professor of statistics and data science at Cardiff University and a member of the editorial board of The Data Shrink.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail background by Marcin Skalij on Unsplash.\n\n\n\nHow to cite\n\nGillard, Jonathan. 2024. “Data science and AI in the public sector: An interview with ONS’s Penny Holborn.” The Data Shrink, March 27, 2024. URL"
  },
  {
    "objectID": "about-tds.html",
    "href": "about-tds.html",
    "title": "Welcome to The Data Shrink",
    "section": "",
    "text": "Welcome to the home of The Data Shrink, a new project from the Royal Statistical Society, in partnership with the American Statistical Association. This site and its content are being developed by data science practitioners and leaders with a single goal in mind: to help you deliver high quality, ethical, impactful data science in your workplace."
  },
  {
    "objectID": "about-tds.html#what-are-our-aims",
    "href": "about-tds.html#what-are-our-aims",
    "title": "Welcome to The Data Shrink",
    "section": "What are our aims?",
    "text": "What are our aims?\nThe Data Shrink aims to be a trusted, go-to source for high-quality, engaging and inspiring content which helps data science students, practitioners and leaders to:\n\ndiscover and learn more efficiently;\n\nacquire practical problem-solving skills;\n\nshare their knowledge and accomplishments publicly;\n\nwork smarter, ethically, and more effectively."
  },
  {
    "objectID": "about-tds.html#what-we-provide",
    "href": "about-tds.html#what-we-provide",
    "title": "Welcome to The Data Shrink",
    "section": "What we provide",
    "text": "What we provide\nResources are created to meet the needs of our target audiences. These include:\n\nCase studies – showing how data science is used to solve real-world problems in business, public policy and beyond.\nExplainers – interrogating the underlying assumptions and limitations of data science tools and methods, to help data scientists make smarter, more informed analytical choices.\nExercises – to challenge and develop the analytical mindset that all data scientists need to succeed.\n\nAdvice – interviews, Q&As, and FAQs on such topics as data science ethics, career paths, and communication, to support professional development.\n\nWe are also curating resources to help data scientists identify trustworthy, high-quality content. These include:\n\nTraining guides – step-by-step approaches and recommended sources for learning new skills and methods.\nDatasets – tagged and sorted to help educators and practitioners find data to meet their teaching and training needs.\nFeeds – who and what to follow to keep up with new ideas and developments."
  },
  {
    "objectID": "about-tds.html#how-you-can-get-involved",
    "href": "about-tds.html#how-you-can-get-involved",
    "title": "Welcome to The Data Shrink",
    "section": "How you can get involved",
    "text": "How you can get involved\nSee our open call for contributions."
  },
  {
    "objectID": "careers/career-profiles/posts/2024/01/31/tamanna-haque.html",
    "href": "careers/career-profiles/posts/2024/01/31/tamanna-haque.html",
    "title": "‘Data science challenges you to keep learning – there’ll always be new advances in the field’",
    "section": "",
    "text": "Hi, Tamanna. Thank you for sharing your career story with The Data Shrink. Please tell us a little about yourself and your role in data science.\nI’m Tamanna Haque. I’ve been working at Jaguar Land Rover for nearly four years, recently promoted to lead data scientist working within product engineering. It’s coming up to eight years that I’ve been working in the field, and my areas of interest are the use of machine learning to provide the best products and experiences for my customers and stakeholders.\nWhat does your job involve?\nMy role involves using the connected car and AI to make our products and customer experiences better, whilst leading within our wide data science team too. The data science team in Manchester, UK, originated with myself and one of my teammates – it’s since grown to nearly 40 (cross-sites and countries) and developed into a high-performing, advanced data science team.\nWhat makes us stand out is the nature of our work – we mostly use vehicle data (of participating customers), which is different to a lot of other commercial businesses or teams who’ll focus more on transactional or web data. The data we use lends itself to some pretty interesting projects, and a general futuristic feel here.\nI’m particularly interested and active in enabling a more electric and modern luxury future from the use of vehicle data.\nWhat does “data science” mean to you?\nThe realisation of value! Whether that is added revenue, saved costs or improved growth, I’m led by what data science can do for the business and its customers. The use of data science can open up many exciting, value-adding opportunities.\n\n\n\n\n\n\nPhoto supplied by Tamanna Haque, courtesy of Jaguar Land Rover. Used with permission.\n\n\n\n\n\nThere are more routes to getting into data science nowadays, but it’s important to not lose sight of fundamentals such as statistics and mathematics. A lot of people can code-up models but it’s fair to say that only a portion of them appreciate how to do this responsibly.\n\n\n\nWhat do you think is your most important skill as a data scientist?\nI’ve always presented myself as a technically astute data scientist, even when entering leadership. But my niche is my ever-growing commercial awareness and passion about our products, customers and business. These aren’t new qualities, but they now align with my professional interests, as well as personal (I’ve been a fan of the Jaguar brand since childhood)!\nHow did you get into data science?\nI did a maths degree at the University of Manchester, where I specialised in statistics. I didn’t do any post-graduate education and this was fine for me.\nAfter graduating, I joined a digital fashion retailer (with a financial services proposition) as an analyst initially. I learned a lot about real-life data and analytics itself, whilst developing a rounded understanding about the business and how to deal with stakeholders cross-functionally. I must have served a few hundred at least(!) and left most of the ‘fancy’ stuff I learned at university aside, whilst getting to grips with so many aspects of commercial analytics. A great way for me to set solid foundations for what followed, and I personally feel this gives me a lens that others who dive straight into data science don’t have.\nI was soon attracted to data science because it tapped into what I learned at university and challenges you to keep learning; there’ll always be things to learn, and new advances in the field.\nWhat, or who, first inspired you to become a data scientist?\nI have a twin sister, we’ve always been together throughout education. Even before we graduated together, she secured her first role as an analyst. This opened my eyes to data, and data science followed for us both!\nWhat were the hurdles or challenges that you needed to overcome on your route into the profession?\nI had a few people tell me I couldn’t do data science, possibly because I didn’t fit the typical data scientist stereotype in several ways. I think attitudes in the field have changed over time though and on a personal level, it’s motivated me to give it everything, and I can’t regret that.\nAnd what are the challenges that you face now, as a working data scientist?\nI need to manage my diary well to ensure effectiveness and work-life balance. I’m overseeing people, other projects, doing public speaking and trying to remain hands on. I sometimes block out chunks of time in my diary – I need some meeting-free time to produce quality technical work. I try to finish on time and enjoy a very busy social life with my family and friends. A flexible attitude to how we work helps to keep me happy and energised whilst I’m delivering from various angles.\nThinking back to your earlier roles in data science, how do they compare to your current role?\nMy current role is very different to my previous roles. I’m continually learning and adapting how I can be a good leader, providing support to a breadth of colleagues (in and outside the team) whilst delivering myself. I’m actively involved in setting and refining our team’s strategy and I’m enjoying leading projects which either deliver high financial impact or help set the path in terms of new tech and/or machine learning capability. There is much more responsibility but it’s easy to stay energised when working on cars and for a business I’ve long admired.\nWhat was the most important thing you learned in your first year on the job?\nI should have had more confidence in myself, but this grew – as I adjusted to the new environment I became much more assertive. My domain knowledge and data science expertise combined help to build my self-confidence, credibility and reputation.\nWhat have been your career highlights so far?\nI’m most proud of my recent promotion from senior to lead data scientist. Also it was exciting for my family and I when I gained an offer to join Jaguar Land Rover.\nHave there been any mistakes or regrets along the way?\nNo, what’s meant to be will be!\nHow do you think your role will evolve over the rest of your career?\nMy progression has been relatively rapid, and I hope I’ve got many, many years ahead of me in my career. It’s hard to say how my role will evolve, I have a blend of responsibilities in my role which combined provide great fulfilment for me at the moment.\nIf you were starting out in data science now, what would you put at the top of your reading/study list?\nA good understanding of analytics and the domain you’re in are my recommended prerequisites to doing data science.\nAnalytics is an important part of the data science lifecycle, being able to get the data yourself and communicate results with influence, for example, are just a few aspects of analytics which underpin successful data science projects.\nAlso, without awareness of the business and industry you’re working in, you can become very dependent on others. Data science itself can be quite challenging, so it’s great to have a solid foundation before starting out.\nWhat personal or professional advice would you give for anyone wanting to be a data scientist now?\nWith the level of continuous learning required to just simply keep up, it can be more of a lifestyle and not a job, so this is something to consider!\nWhat do you think will be the main challenges facing data science as a field in the next few years?\nI still expect to see a skills gap in the field. There are more routes to getting into data science nowadays, but it’s important to not lose sight of fundamentals such as statistics and mathematics. A lot of people can code-up models but it’s fair to say that only a portion of them appreciate how to do this responsibly, understanding samples versus populations, statistical testing, which type of regularisation to use in a neural network, et cetera.\nI also think there’s a challenge of questionable data science products reaching high levels of popularity and usage amongst the public… Some recent developments in this space have been extremely intelligent but raise ethical concerns. Just because something can be done with AI doesn’t mean it should, and my preferences are towards AI being ethical and (ideally) explainable.\n\nDiscover more Career profiles\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Data science challenges you to keep learning – there’ll always be new advances in the field.’” The Data Shrink, March 28, 2023. URL"
  },
  {
    "objectID": "careers/posts/2024/03/21/mjones-interview.html",
    "href": "careers/posts/2024/03/21/mjones-interview.html",
    "title": "Data science and AI in financial services: An interview with Nationwide’s Matthew Jones",
    "section": "",
    "text": "Back to Careers\n\n\n\n\n\n\nAbout the author\n\nJonathan Gillard is a professor of statistics and data science at Cardiff University and a member of the editorial board of The Data Shrink.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This interview is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail background by Devin Pickell on Unsplash.\n\n\n\nHow to cite\n\nGillard, Jonathan. 2024. “Data science and AI in financial services: An interview with Nationwide’s Matthew Jones.” The Data Shrink, March 21, 2024. URL"
  },
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of The Data Shrink (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of The Data Shrink (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nRWDS editors and contributors recommend external web links on the basis of their suitability and usefulness for our users. Selection and addition of links to our website is entirely a matter for RWDS and for RWDS alone.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders of any product, service, policy or opinion of the organisation or individual. RWDS, its editors, the RSS, or other partners and funders are not responsible for the content of external websites.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of RWDS is final and no correspondence will be entered into.\nIf you wish to report a concern, please email rwds@rss.org.uk.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown policy",
    "text": "Notice and Takedown policy\nIf you are a rights holder and are concerned that you have found material on our site for which you have not given permission, or is not covered by a limitation or exception in national law, please contact us in writing stating the following:\n\nYour contact details.\nThe full bibliographic details of the material.\nThe exact and full url where you found the material.\nProof that you are the rights holder and a statement that, under penalty of perjury, you are the rights holder or are an authorised representative.\n\nContact details:\nNotice and Takedown,\nLicensing,\n12 Errol Street,\nLondon EC1Y 8LX\nweb@rss.org.uk\nUpon receipt of notification, the ‘Notice and Takedown’ procedure is then invoked as follows:\n\nWe will acknowledge receipt of your complaint by email or letter and will make an initial assessment of the validity and plausibility of the complaint.\nUpon receipt of a valid complaint the material will be temporarily removed from our website pending an agreed solution.\nWe will contact the contributor who deposited the material, if relevant. The contributor will be notified that the material is subject to a complaint, under what allegations, and will be encouraged to assuage the complaints concerned.\nThe complainant and the contributor will be encouraged to resolve the issue swiftly and amicably and to the satisfaction of both parties, with the following possible outcomes:\n\nThe material is replaced on our website unchanged.\nThe material is replaced on our website with changes.\nThe material is permanently removed from our website.\n\n\nIf the contributor and the complainant are unable to agree a solution, the material will remain unavailable through the website until a time when a resolution has been reached.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html",
    "title": "A demonstration of the law of the flowering plants",
    "section": "",
    "text": "This tutorial will demonstrate a popular method for predicting the day a flower will bloom. There are many reasons why you might want to predict a bloom date. You might be a scientist studying ecosystems stressed by climate change. Or you might be planning a trip to Amsterdam and would like to time your stay to when the tulips are in bloom. Or maybe you are participating in the annual Cherry Blossom Prediction Competition and want some ideas to help you get started.\nIn any case, you might be surprised to learn that the day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The mathematical rule developed in the eighteenth century to make these predictions – now called the “law of the flowering plants” – shaped the direction of statistics as a field and is still used by scientists with relatively few changes.\nWe present the law of the flowering plants as it was stated by Adolphe Quetelet, an influential nineteenth century statistician. Upon completing this tutorial, you will be able to:\nAt the end of the tutorial, we challenge you to design an algorithm that beats our predictions. The tutorial uses the R programming language. In particular, the code relies on the following packages:"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "The law of the flowering plants",
    "text": "The law of the flowering plants\nWe begin by reviewing the law of the flowering plants as it was stated by Adolphe Quetelet. You may already know Quetelet as the inventor of the body mass index. Less known is that Quetelet recorded the bloom dates of hundreds of different plants between 1833 and 1852 at the Brussels Observatory, which he founded and directed. Quetelet reported that a plant flowers when exposed to a specific quantity of heat, measured in degrees of Celsius squared (°C²). For example, he calculated that a lilac blooms when the sum of the daily temperatures squared exceeds 4264°C² following the last frost.\nHe communicated this law in his Letters addressed to HRH the grand duke of Saxe-Coburg and Gotha (Number 33, 1846; translated 1849) and in his reporting On the climate of Belgium (Chapter 4, Part 4, 1848; data updated in Part 7, 1857). A picture of Quetelet and the title page of On the climate of Belgium are displayed in Figure 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Quetelet reported on the law of the flowering plants in On the climate of Belgium (1857). Sources: Wikimedia Commons, Gallica.\n\nQuetelet was not the first to study bloom dates. Anthophiles have recorded the dates that flowers bloom for centuries. Written records of cherry trees go back as far as 812 AD in Japan and peach and plum trees as far as 1308 AD in China. Systematic record keeping began a century before Quetelet with Robert Marsham’s Indications of Spring (1789).\nQuetelet was also not the first to study the relationship between temperature and bloom dates. René Réaumur (1735), an early adopter of the thermometer, noted the relationship before Marsham published his Indications. But Quetelet was the first to systematically study the relationship across a wide variety of plants and derive the amount of heat needed to bloom. An example of Quetelet’s careful record keeping can be seen in Figure 2, one of many tables he reported in his publications.\n\n\n\n\n\n\nFigure 2: Bloom dates at Brussels Observatory observed by Quetelet between 1839 and 1852. Source: Gallica."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Reproducing Quetelet’s law of the flowering plants",
    "text": "Reproducing Quetelet’s law of the flowering plants\nTo reproduce Quetelet’s law, we combine the data in Figure 2 with additional observations from his Letters. We focus on Quetelet’s primary example, the bloom date of the common lilac, Syringa vulgaris, row 18 of Figure 2. We do this because Quetelet carefully describes his methodology for measuring the bloom date of lilacs. For example, Quetelet considers a lilac to have bloomed when “the first corolla opens and shows the stamina.” That event is closest to what the USA Phenology Network describes as “open flowers”, depicted in the center image of Figure 3 below. This detail will become relevant when we attempt to replicate Quetelet’s law in a later section. Note that although we focus on lilacs in this tutorial, the R code is easily edited to predict the day that other plants will bloom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The bloom date occurs when the first corolla opens and shows the stamina (center image). Source: USA National Phenology Network.\n\nIn the R code below, the five-column tibble lilac contains the date each year that Quetelet observed the lilacs bloom at Brussels Observatory. The first three columns are the month, day, and year the lilacs bloomed between 1839 and 1852. These columns are combined to form the fourth column, the full date the lilacs bloomed. The last column converts the date to the day of the year the lilacs bloomed, abbreviated “doy.” That is, “doy” is the number of days it took for the lilacs bloom following January 1. Both “date” and “doy” representations of Quetelet’s observations will be useful throughout this tutorial.\n```{r}\nlilac &lt;-                   \n  tibble(month = c(\"May\", \"April\", \"April\", \"April\", \"April\", \"April\", \"May\", \n                   \"April\", \"May\", \"April\", \"May\", \"April\", \"May\", \"May\"),\n         day   =  c(10, 28, 24, 28, 20, 25, 13, 12, 9, 21, 2, 30, 1, 12),\n         year  = 1839:1852,\n         date  = as.Date(paste(month, day, year), format = \"%B %d %Y\"),\n         doy   = parse_number(format(date, \"%j\"))) \n\nlilac %&gt;% \n  kable(align = \"c\",\n        caption = \"Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\n\n\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\nMay\n\n\n10\n\n\n1839\n\n\n1839-05-10\n\n\n130\n\n\n\n\nApril\n\n\n28\n\n\n1840\n\n\n1840-04-28\n\n\n119\n\n\n\n\nApril\n\n\n24\n\n\n1841\n\n\n1841-04-24\n\n\n114\n\n\n\n\nApril\n\n\n28\n\n\n1842\n\n\n1842-04-28\n\n\n118\n\n\n\n\nApril\n\n\n20\n\n\n1843\n\n\n1843-04-20\n\n\n110\n\n\n\n\nApril\n\n\n25\n\n\n1844\n\n\n1844-04-25\n\n\n116\n\n\n\n\nMay\n\n\n13\n\n\n1845\n\n\n1845-05-13\n\n\n133\n\n\n\n\nApril\n\n\n12\n\n\n1846\n\n\n1846-04-12\n\n\n102\n\n\n\n\nMay\n\n\n9\n\n\n1847\n\n\n1847-05-09\n\n\n129\n\n\n\n\nApril\n\n\n21\n\n\n1848\n\n\n1848-04-21\n\n\n112\n\n\n\n\nMay\n\n\n2\n\n\n1849\n\n\n1849-05-02\n\n\n122\n\n\n\n\nApril\n\n\n30\n\n\n1850\n\n\n1850-04-30\n\n\n120\n\n\n\n\nMay\n\n\n1\n\n\n1851\n\n\n1851-05-01\n\n\n121\n\n\n\n\nMay\n\n\n12\n\n\n1852\n\n\n1852-05-12\n\n\n133\n\n\n\n\n\n\nTo reproduce Quetelet’s law of the flowering plants, we will combine these bloom dates with daily temperature. The daily maximum and minimum temperatures at Brussels Observatory between 1839 and 1852 are available from the Global Historical Climatology Network. The data can be downloaded using the ghcnd_search function contained within the R package rnoaa (2021). The station id for Brussels Observatory is “BE000006447”.\nThe ghcnd_search function returns the maximum and minimum temperature as separate tibbles in a list. In the R code below, we join the tibbles using the reduce function. Note that the temperature is reported in tenths of a degree (i.e. 0.1°C) so we divide by 10 before calculating the temperature midrange, our estimate of the daily temperature.\nThe result is a five-column tibble temp, which contains the year of the temperature record (“year”), the date of the temperature record (“date”), the maximum temperature (“tmax”), the minimum temperature (“tmin”), and the midrange temperature (“temp”). The first 10 rows of the table are below. When you produce the full table yourself, you may notice that a small portion of temperature records are missing. We found that imputing these missing values does not significantly change the results. Therefore, we ignore these days when conducting our analysis.\n```{r}\ntemp &lt;- \n  ghcnd_search(stationid = \"BE000006447\",\n               var = c(\"tmax\", \"tmin\"),\n               date_min = \"1839-01-01\",\n               date_max = \"1852-12-31\") %&gt;%\n  reduce(left_join) %&gt;%\n  transmute(year = parse_number(format(date, \"%Y\")), \n            date, \n            tmax = tmax / 10, \n            tmin = tmin / 10, \n            temp = (tmax + tmin) / 2)\n  \ntemp %&gt;% \n  kable(align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 2: Temperature observed at Brussels Observatory between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 2: Temperature observed at Brussels Observatory between 1839 and 1852.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n1839\n\n\n1839-01-01\n\n\n5.7\n\n\n-0.2\n\n\n2.75\n\n\n\n\n1839\n\n\n1839-01-02\n\n\n6.3\n\n\n0.8\n\n\n3.55\n\n\n\n\n1839\n\n\n1839-01-03\n\n\n7.2\n\n\n1.8\n\n\n4.50\n\n\n\n\n1839\n\n\n1839-01-04\n\n\n8.0\n\n\n1.8\n\n\n4.90\n\n\n\n\n1839\n\n\n1839-01-05\n\n\n5.3\n\n\n0.8\n\n\n3.05\n\n\n\n\n1839\n\n\n1839-01-06\n\n\n10.0\n\n\n1.3\n\n\n5.65\n\n\n\n\n1839\n\n\n1839-01-07\n\n\n8.9\n\n\n1.4\n\n\n5.15\n\n\n\n\n1839\n\n\n1839-01-08\n\n\n3.0\n\n\n0.1\n\n\n1.55\n\n\n\n\n1839\n\n\n1839-01-09\n\n\n0.8\n\n\n-0.1\n\n\n0.35\n\n\n\n\n1839\n\n\n1839-01-10\n\n\n2.8\n\n\n-2.8\n\n\n0.00\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nReproducing Quetelet’s law is now a simple matter of calculating the sum of the squared daily temperature from the day of last frost until the bloom day. We could use the day of last frost reported in Quetelet’s Letters. However, since we will replicate Quetelet’s analysis with recent data in a later section, we use our own definition of the day of last frost. We define the day of last frost to be the day following the last day the maximum temperature is below 0. The R code below creates the function doy_last_frost to extract the day of last frost from the maximum temperature. To demonstrate this function, we then compare the bloom date with the last frost date in 1839, the first year Quetelet observed.\n```{r}\ndoy_last_frost &lt;- function(tmax, doy_max = 100) {\n  dof &lt;- which(tmax[1:doy_max] &lt;= 0)\n  if(length(dof) == 0) 1 else max(dof) + 1\n  }\n\nbloom_day &lt;- \n  lilac %&gt;% \n  filter(year == 1839) %&gt;%\n  pull(doy) + \n  as.Date(\"1839-01-01\")\n  \nfrost_day &lt;- \n  temp %&gt;% \n  filter(year == 1839) %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"1839-01-01\") \n\ntibble(`last frost date` = frost_day, \n       `bloom date` = bloom_day) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n1839-03-08\n\n\n1839-05-11\n\n\n\n\n\n\nIf Quetelet’s law of the flowering plants is correct, Table 3 has the following interpretation. On March 8, 1839 the lilacs at Brussels Observatory began “collecting” temperature. The lilacs continued to “collect” temperature until May 11, at which point they exceeded their 4264°C² quota and bloomed. We visualize this theory in Figure 4 with the R packages ggplot2, a member of the set of packages that constitute the “tidyverse” (2019), and plotly.\n```{r}\n(temp %&gt;% \n  filter(date &lt; as.Date(\"1839-06-01\")) %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title = \n      \"Figure 4: According to Quetelet's law, the lilacs bloom when exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(bloom_day, frost_day)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day, bloom_day)),\n                  y = c(-4, -4),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-10, -12)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 4: According to Quetelet’s law, the lilacs bloom when exposed to 4264°C² following the last frost. Author provided, CC BY 4.0.\n\nWe now have all the ingredients necessary to reproduce Quetelet’s findings. Our reproduction is greatly simplified by using the nest function from the tidyr package, another member of the “tidyverse”. For an overview of nest, see the “Nested data” section of Grolemund and Wickham (2017). We will group the data by year, nest, calculate the cumulative squared temperature from the frost date to the bloom date within each year, and then unnest. We ignore temperatures below 0°C. That is, temperatures below 0°C are set to 0°C. We do this because it is clear from Quetelet’s derivation of the law that only positive temperatures should be squared. See the next section for details.\n```{r}\nquetelet &lt;- \n  temp %&gt;% \n  group_by(year) %&gt;% \n  nest() %&gt;% \n  left_join(lilac) %&gt;% \n  mutate(law = map(data, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax) + 1):doy]^2))) %&gt;% \n  unnest(law) %&gt;% \n  ungroup()\n\nquetelet %&gt;% \n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law)/sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\", \n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\", \n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 4: Reproduction of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 4: Reproduction of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4261\n\n\n197\n\n\n[3867, 4656]\n\n\n\n\n\n\nThe results show that Quetelet’s findings are indeed reproducible. Quetelet estimated that lilacs bloom once exposed to 4264°C² following the last frost. Our reanalysis suggests a similar amount. However, 4264°C² is the overall average across all years – the estimated amount needed to bloom varies year to year. As a result, the average has a 95% confidence interval of approximately 3870°C² to 4660°C². Quetelet was well aware of this variation. He argued it was due to unobserved factors that influence growing conditions and change each year, and he dedicated significant space in his Letters to discuss them.\nThese unobserved factors limit the accuracy of predictions made using the law. To assess the predictive accuracy of the law, we temporarily ignore the bloom dates Quetelet observed. Instead, we apply the 4264°C² quota to the temperature records at Brussels Observatory to predict the bloom date. We then compare our predictions with the bloom date Quetelet observed. The R code below creates the function doy_prediction to estimate the day the lilac will bloom from temperature records. Table 5 summarizes the accuracy of Quetelet’s law by the mean absolute error and root mean squared error.\n```{r}\ndoy_prediction &lt;- function(temp, tmax)\n  doy_last_frost(tmax) + which.max(cumsum(pmax(temp[(doy_last_frost(tmax) + 1):365], 0, na.rm = TRUE)^2) &gt; 4264)\n\nquetelet %&gt;% \n  mutate(pred = map(data, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup() %&gt;%\n  summarize(mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\", \"root mean squared error (days)\"),\n        caption = \"Table 5: Predictions using Quetelet's law are accurate within a week on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 5: Predictions using Quetelet’s law are accurate within a week on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n5\n\n\n6\n\n\n\n\n\n\nTable 5 indicates that predictions made using the law are accurate to within a week on average. For comparison purposes, we also predict the day the lilacs will bloom using the average bloom date between 1839 and 1852. That is, on average the lilac bloomed on April 30 (April 29 on leap years), and we check the accuracy of simply predicting this average date each year. Table 6 indicates the average bloom date yields predictions that are less accurate by an average of two days.\n```{r}\nquetelet %&gt;%\n  summarize(pred = mean(doy),\n            mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 6: Predictions using the average bloom date are off by a week or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 6: Predictions using the average bloom date are off by a week or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n7\n\n\n9"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s derivation of the law of the flowering plants",
    "text": "Quetelet’s derivation of the law of the flowering plants\nQuetelet believed that, as in physics, universal laws govern social and biological phenomenon. Quetelet was not only inspired by physics to describe social and biological patterns using mathematical formulas. He often took his formulas directly from physics. In fact, you may have already recognized similarities between his law and Newton’s second law of motion.\nQuetelet reasoned that temperature exerts a “force” on plants in the same way that gravity exerts a force on a falling object. Newton’s second law states that acceleration is proportional to force. It follows that an object initially at rest and subject to a constant force will travel a distance proportional to time squared. Quetelet simply substituted temperature for time.\nWe briefly elaborate. Let \\(d(t)\\) denote the distance an object travels after time \\(t\\). Let \\(v(t) = d'(t)\\) denote its speed and \\(a(t) = v'(t)\\) its acceleration. If acceleration is constant, i.e. \\(a(t) = c\\),\n\n\\(v(t) = \\int_0^t a(s) \\, ds = \\int_0^t c \\, ds = c t\\)\n\nand\n\n\\(d(t) = \\int_0^t v(s) \\, ds = \\int_0^t c s \\, ds = \\tfrac{c}{2} t^2\\)\n\nQuetelet imagined plants experience time in temperature and bloom after “traveling” distance \\(d_*\\). If a plant is exposed to temperature \\(t_i\\) on day \\(i = 1, 2, \\ldots\\), then the bloom date, \\(n_*\\), is the first day \\(\\sum_{i=1}^{n_*} \\tfrac{c}{2} t_i^2 \\geq d_*\\). Multiplying both sides of the inequality by \\(\\tfrac{2}{c}\\), yields Quetelet’s law: the bloom is the first day, \\(n_*\\), that \\(\\sum_{i=1}^{n_*} t_i^2 \\geq \\tfrac{2}{c} d_*\\).\nThe derivation of laws like the law of the flowering plants was popular in the nineteenth century. But any similarities between the “force” of temperature and the force of gravity are likely coincidental. We are not aware of any biological mechanisms that justify Quetelet’s application of Newton’s law.\nToday, the law of the flowering plants is considered a heuristic, or rule of thumb, that approximates complicated biological mechanisms. Like Quetelet, scientists model plants as experiencing time in temperature instead of calendar time. These temperature units are typically called “growing degree days”. Scientists often find that plants may only be sensitive to temperatures in specific ranges or “modified growing degree days”. Although modern statistical methods can greatly improve the accuracy of predictions, laws like Quetelet’s remain popular because they are simple to communicate and easy to replicate, as we demonstrate in the next section."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Replicating Quetelet’s law of the flowering plants",
    "text": "Replicating Quetelet’s law of the flowering plants\nIn the previous section, we explained how Quetelet derived the law of the flowering plants. Quetelet believed the law of the flowering plants was universal, describing the bloom date of all flowers around the world and in any year. Whether the law can in fact be considered universal requires replicating Quetelet’s results with new data collected at a different location in a different year.\nIn this section, we replicate the law of the flowering plants using lilac bloom dates observed by scientists between 1956 and 2009 at 53 locations throughout the Pacific Northwest (2015). The data can be downloaded from the USA National Phenology Network using the rnpn package (2022). For space considerations, the R code that downloads and cleans the data is provided in the Appendix. Running this code yields the tibble usa_npn. Each row of the tibble corresponds with a bloom date observed at a given site in a given year. There are 31 columns, only seven of which we use in our replication. The remaining columns are documented in the rnpn package, and we will not review them here.\nTable 7 displays six of the seven columns (and only the first 10 rows of the full table). These columns are defined in the same way as the columns of Table 1, except for “site_id”, which denotes the site at which the observation was made. Table 1 does not have a “site_id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nload(url(\"https://github.com/jauerbach/miscellaneous/blob/main/usa_npn.RData?raw=true\"))\n\nusa_npn %&gt;%\n  transmute(site_id, \n            month = first_yes_month, \n            day   = first_yes_day, \n            year  = first_yes_year, \n            date  = as.Date(paste(month, day, year), format = \"%m %d %Y\"),\n            doy) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\nTable 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\n\n\n\n\nsite_id\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\n150\n\n\n5\n\n\n25\n\n\n1956\n\n\n1956-05-25\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n22\n\n\n1957\n\n\n1957-05-22\n\n\n142\n\n\n\n\n150\n\n\n5\n\n\n12\n\n\n1958\n\n\n1958-05-12\n\n\n132\n\n\n\n\n150\n\n\n6\n\n\n3\n\n\n1959\n\n\n1959-06-03\n\n\n154\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1960\n\n\n1960-05-27\n\n\n148\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1961\n\n\n1961-05-27\n\n\n147\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1962\n\n\n1962-05-26\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n24\n\n\n1963\n\n\n1963-05-24\n\n\n144\n\n\n\n\n150\n\n\n5\n\n\n28\n\n\n1964\n\n\n1964-05-28\n\n\n149\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1966\n\n\n1966-05-26\n\n\n146\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nThe seventh column we review is “temp”. Each row of “temp” is a tibble of temperature records taken at the nearest station in the Global Historical Climatology Network. The first tibble (again, only the first 10 rows) is displayed in Table 8 below. The columns are defined in the same way as the columns of Table 2, except for “id”, which denotes the location at which the temperature record was made. Table 2 does not have an “id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nusa_npn %&gt;%\n  pull(temp) %&gt;%\n  .[[1]] %&gt;%\n  mutate(year = parse_number(format(date, \"%Y\"))) %&gt;%\n  select(id, year, date, tmax, tmin, temp) %&gt;%\n  kable(align = \"c\",\n        col.names = c(\"id\", \"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 8: Temperature observed at an example pacific northwest site in 1956.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\nTable 8: Temperature observed at an example pacific northwest site in 1956.\n\n\n\n\nid\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-01\n\n\n5.6\n\n\n-5.6\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-02\n\n\n1.7\n\n\n-7.2\n\n\n-2.75\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-03\n\n\n3.3\n\n\n-11.7\n\n\n-4.20\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-04\n\n\n4.4\n\n\n-10.0\n\n\n-2.80\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-05\n\n\n7.8\n\n\n0.0\n\n\n3.90\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-06\n\n\n4.4\n\n\n-11.1\n\n\n-3.35\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-07\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-08\n\n\n4.4\n\n\n-4.4\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-09\n\n\n1.7\n\n\n-9.4\n\n\n-3.85\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-10\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nWe are now prepared to replicate Quetelet’s findings. We will use R code nearly identical to the code we used to reproduce Quetelet’s findings earlier. The main difference is due to the fact that temperature records are dependent across sites within a year. To account for this dependence, we compute the cumulative temperature squared from the last frost to the bloom date for each site and year. We then take the average across all sites within a year. Finally, we calculate the standard error and confidence interval using only the variation of the averages across years. Table 9 displays the results.\n```{r}\nusa_npn %&gt;%             \n  group_by(rownames(usa_npn)) %&gt;%\n  mutate(law = \n           map(temp, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax, doy) + 1):(doy - 1)]^2))) %&gt;%\n  unnest(law) %&gt;% \n  group_by(year) %&gt;%    \n  summarize(law = mean(law)) %&gt;%\n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law) / sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\",\n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\",\n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 9: Replication of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 9: Replication of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4329\n\n\n116\n\n\n[4098, 4560]\n\n\n\n\n\n\nTable 9 indicates that Quetelet’s findings are replicable in the sense that the confidence interval calculated using Quetelet’s data (Table 4) overlaps with the confidence interval calculated using the USA lilac data (Table 9). The standard error in Table 9 is smaller than Table 4 because the replication uses 54 years of data compared to Quetelet’s 14. Note that in the R code above, we subtract 1 from “doy” to correct for differences in how the bloom date is reported. This correction is not particularly important; the confidence intervals still overlap when this correction is removed.\nWe now investigate the accuracy of Quetelet’s law when applied to the USA lilac data. As before, we make use of the doy_prediction function.\n```{r}\nusa_npn &lt;- \n  usa_npn %&gt;% \n  mutate(pred = map(temp, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup()\n\nusa_npn %&gt;% \n  summarize(mae  = mean(abs(doy - 1 - pred)),\n            rmse = sqrt(mean((doy - 1 - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\", \n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 10: Predictions using Quetelet's law are accurate within about two weeks on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 10: Predictions using Quetelet’s law are accurate within about two weeks on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n10\n\n\n15\n\n\n\n\n\n\nTable 10 indicates that the predictions are accurate to within two weeks on average. Recall that the predictions using Quetelet’s own data were accurate to within one week on average (Table 5). We speculate that the decrease in accuracy is due in part to the fact that both Quetelet’s lilacs and the temperature were observed at the same site, Brussels Observatory. In some cases, the USA lilacs were a few miles from where the temperature was recorded.\nAlthough the accuracy of the predictions made using Quetelet’s law is lower when applied to the USA lilac data, Figure 5 indicates that the law produces the correct bloom date on average. The figure plots the predictions made by the law against the actual bloom dates scientists observed. Note that instead of representing prediction-observation pairs as points in a scatter plot, the data are represented using blue contours. We use contours because there are more than 1,500 observations – too many to study using a scatter plot.\n```{r}\n(usa_npn %&gt;% \n   mutate(doy = first_yes_doy) %&gt;%\n   unnest(pred) %&gt;% \n   ungroup() %&gt;%\n   mutate(predicted = as.Date(\"2020-01-01\") + pred,\n          observed = as.Date(\"2020-01-01\") + doy) %&gt;%\n   ggplot() + \n    aes(x = observed, y = predicted) +\n    geom_density2d(contour_var = \"ndensity\") +\n    geom_abline(intercept = 0, slope = 1, linetype = 2) +\n    labs(x = \"date observed\", \n         y = \"date predicted\",\n         title = \"Figure 5: Predictions using Quetelet's law are accurate within about two weeks on average.\") +\n    theme(legend.position = \"none\")) %&gt;%\n  ggplotly(tooltip = \"\") %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 5: Predictions using Quetelet’s law are accurate within about two weeks on average. Author provided, CC BY 4.0.\n\nThe contours are easy to interpret. The blue lines are much like a mountain range observed from above. The inner circles are peaks of high elevation in which many prediction-observation pairs co-occur. The outer circles are areas of low elevation in which few prediction-observation pairs co-occur.\nThe dotted line is the “y = x” line, having zero intercept and unit slope. Prediction-observation pairs that lie on the line indicate perfect predictions. The fact that the dotted line intersects the blue contours at their peak suggests the law derived from Quetelet’s data accurately predicts the typical bloom date of the USA data. This accuracy is impressive given the fact that the USA lilacs were observed more than a century later and on a different continent. The blue curves deviate from the line by about two weeks in the vertical direction, which is consistent with Table 10.\nAn average accuracy of two weeks might not sound impressive. But it is far more accurate than using the average bloom date Quetelet observed, April 30 (April 29 on leap years). The average bloom date yields predictions that are off by an additional eleven days on average.\n```{r}\nusa_npn %&gt;%\n  mutate(doy = first_yes_doy) %&gt;%\n  ungroup() %&gt;%\n  summarize(\n    pred = mean(quetelet$doy), \n    mae  = mean(abs(doy - pred)),\n    rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(\n    dig = 0,\n    align = \"c\",\n    col.names = c(\"mean absolute error (days)\",\n                  \"root mean squared error (days)\"),\n    caption = \"Table 11: Predictions using the average bloom date are off by three weeks or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 11: Predictions using the average bloom date are off by three weeks or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n21\n\n\n24"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Predicting the day the lilac will bloom in Brussels in 2023",
    "text": "Predicting the day the lilac will bloom in Brussels in 2023\nAny weather forecast can become a flower forecast by applying the law of the flowering plants. In this section, we use the AccuWeather forecast to predict the day a hypothetical lilac will bloom in Brussels in 2023. AccuWeather forecasts daily maximum and minimum temperatures three months into the future. We do not evaluate the quality of these forecasts. The purpose of this section is to simply convert them into flower forecasts.\nWe use the AccuWeather forecast as it appeared on the webpage AccuWeather.com on February 19, 2023. AccuWeather reports the forecast for each month on a separate webpage. For reproducibility, we saved each page on the Internet Archive. The following R code creates the function get_weather_table to retrieve each page we saved, extract the forecast contained within that page, and arrange the data as a tibble. The get_weather_table function combines several functions from the rvest package, which is yet another member of the “tidyverse”. In particular, the forecast on each page is contained within the div “monthly-calendar” and can be extracted with the html_nodes and html_text2 functions.\nApplying the get_weather_table function to the url for each page yields a five column tibble temp_br, with columns defined in the same way as the tibble temp, discussed in previous sections. The first 10 rows are below; the data are also available on the author’s GitHub.\n```{r}\n get_weather_table &lt;- function(url)\n  read_html(url) %&gt;% \n  html_nodes(\"div.monthly-calendar\") %&gt;% \n  html_text2() %&gt;%\n  str_remove_all(\"°|Hist. Avg. \") %&gt;%\n  str_split(\" \", simplify = TRUE) %&gt;%\n  parse_number() %&gt;%\n  matrix(ncol = 3, \n         byrow = TRUE,\n         dimnames = list(NULL, c(\"day\", \"tmax\", \"tmin\"))) %&gt;%\n  as_tibble() %&gt;%\n  filter(\n    row_number() %in%\n      (which(diff(day) &lt; 0) %&gt;% (function(x) if(length(x) == 1) seq(1, x[1], 1) else seq(x[1] + 1, x[2], 1))))\n\ntemp_br &lt;-\n  tibble(\n    base_url = \"https://web.archive.org/web/20230219151906/https://www.accuweather.com/en/be/brussels/27581/\",\n    month = month.name[1:5],\n    year = 2023,\n    url = str_c(base_url, tolower(month), \"-weather/27581?year=\", year, \"&unit=c\")) %&gt;%\n  mutate(temp = map(url, get_weather_table)) %&gt;%\n  pull(temp) %&gt;%\n  reduce(bind_rows) %&gt;%\n  transmute(date = seq(as.Date(\"2023-01-01\"), as.Date(\"2023-05-31\"), 1),\n            year = parse_number(format(date, \"%Y\")),\n            tmax,\n            tmin,\n            temp = (tmax + tmin) / 2)\n\ntemp_br %&gt;%\n  relocate(year) %&gt;%\n  kable(dig = 2,\n        align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\",\n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n2023\n\n\n2023-01-01\n\n\n15\n\n\n11\n\n\n13.0\n\n\n\n\n2023\n\n\n2023-01-02\n\n\n14\n\n\n5\n\n\n9.5\n\n\n\n\n2023\n\n\n2023-01-03\n\n\n9\n\n\n3\n\n\n6.0\n\n\n\n\n2023\n\n\n2023-01-04\n\n\n13\n\n\n8\n\n\n10.5\n\n\n\n\n2023\n\n\n2023-01-05\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-06\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-07\n\n\n11\n\n\n9\n\n\n10.0\n\n\n\n\n2023\n\n\n2023-01-08\n\n\n10\n\n\n6\n\n\n8.0\n\n\n\n\n2023\n\n\n2023-01-09\n\n\n8\n\n\n5\n\n\n6.5\n\n\n\n\n2023\n\n\n2023-01-10\n\n\n12\n\n\n4\n\n\n8.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nWe now predict the day the lilacs will bloom. The R code below uses the doy_prediction and doy_last_frost functions created in earlier sections and displays the prediction in Table 13. At the time of our writing, the predicted date is April 19. The forecast is easily updated by providing the url to the updated AccuWeather webpage. (You might use the url https://web.archive.org/save to save a webpage to the Internet Archive to ensure your work is reproducible.)\n```{r}\nbloom_day_br &lt;-\n  temp_br %&gt;%\n  summarize(date = doy_prediction(temp, tmax) + as.Date(\"2023-01-01\")) %&gt;%\n  pull(date)\n\nfrost_day_br &lt;- \n  temp_br %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"2023-01-01\") \n\ntibble(`last frost date` = frost_day_br, \n       `bloom date` = bloom_day_br) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 13: Last frost date and lilac bloom date in Brussels in 2023.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 13: Last frost date and lilac bloom date in Brussels in 2023.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n2023-01-27\n\n\n2023-04-19\n\n\n\n\n\n\nWe visualize the predictions in Figure 6, which has the same interpretation as Figure 4. If the temperature forecast and Quetelet’s law are correct, on January 27, 2023 the lilacs in Brussels began “collecting” temperature. The lilacs will continue to “collect” temperature until April 19, at which point they will exceed their 4264°C² quota and bloom.\n```{r}\n(temp_br %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title =\n      \"Figure 6: According to Quetelet's law, the lilacs will bloom once exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(frost_day_br, bloom_day_br)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day_br, bloom_day_br)),\n                  y = c(14, 14),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-14, -16)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 6: According to Quetelet’s law, the lilacs will bloom once exposed to 4264°C² following the last frost. Author provided, CC BY 4.0."
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist",
    "text": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist\nIn this tutorial, we stated the law of the flowering plants and explained how Quetelet derived it. We also reproduced and replicated Quetelet’s findings before using his law to predict the day the lilac will bloom in Brussels. We now conclude with a reflection on Quetelet’s legacy.\nThe law of the flowering plants surely stands the test of time. It continues to be used by scientists – with relatively few changes – to plan harvests, manage pests, and study ecosystems stressed by climate change. We speculate the law’s longevity is due to the fact that it balances simplicity with relatively accurate predictions.\nAlthough Quetelet did not discover the law, he did much to advance it. Quetelet founded an international network for “observations of the periodical phenomena” (in addition to numerous statistical societies and publications, including the precursor to the Royal Statistical Society). Quetelet’s network of 80 stations collected observations throughout Europe from 1841 until 1872. In particular, Quetelet collaborated with Charles Morren – who later coined the term phenology, the name of the field that now studies biological life-cycle events like the timing of flower blooms (Demarée and Rutishauser 2011).\nIn recent years, the observations collected through phenology networks have become an important resource for understanding the impacts of climate change. For example, the USA National Phenology Network calculates the Spring Bloom Index, which measures the “first day of spring” using the days lilacs are observed to bloom at locations across the United States. The index is then compared to previous years. Figure 7 shows one comparison, called the Return Interval. The Return Interval is much like a p-value, calculating how frequently more extreme spring indices were observed in previous decades. Bloom dates that are uncommonly early (green) or late (purple) may indicate environments stressed by changing climate. Scientists exploit the relationship between temperature and bloom date to extrapolate the index to areas with few observations.\n\n\nFigure 7: The Spring Bloom Index Return Interval measures whether spring is typical when compared to recent decades. Source: USA National Phenology Network.\n\nQuetelet’s emphasis on discovering the universal laws he believed govern social and biological phenomenon has not endured. But data scientists continue to appropriate laws from one area of science to study another. For example, data scientists use neural networks and genetic algorithms to study a wide variety of phenomenon unrelated to neuroscience or genetics. Perhaps Quetelet’s appropriation of Newton’s law, in addition to his careful use of data, make him among the first data scientists?"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Your turn: Do you have what it takes to beat Quetelet’s law?",
    "text": "Your turn: Do you have what it takes to beat Quetelet’s law?\nQuetelet reported that a plant flowers when the sum of the daily temperatures squared exceeds a specific quantity. His prediction rule was state of the art in 1833. But surely you, a twenty-first century data scientist, can do better. Here are some ideas to get you started.\n\nQuetelet squared the temperature before calculating the sum. Would another function of temperature produce a more accurate prediction?\n\nRemove the square so that a plant flowers once the sum of the daily temperatures exceeds a (different) specific quantity. Does this version of the law produce more accurate predictions? What if you use the daily temperatures cubed? (Beginner)\nSuppose a lilac only registers temperatures between 0°C and 10°C. That is, a lilac experiences temperature below the lower limit, 0°C, as 0°C, and above the upper limit, 10°C, as 10°C. Does the accuracy of the predictions improve if you use the temperature the lilac experienced instead of the ambient temperature measured by a weather station? Write a program that finds the lower and upper limits that produce the most accurate predictions. (Intermediate)\nQuetelet used mean absolute error to evaluate the accuracy of his predictions. But his estimate of the specific quantity of heat needed to bloom, 4264°C², does not actually minimize mean absolute error. Write a program that finds the specific quantity that minimizes mean absolute error. Redo part i. and ii. using this function. (Advanced)\n\nQuetelet calculated the sum of the daily temperature squared between the day of last frost and the bloom date. Would another time interval produce more accurate predictions?\n\nWe estimated the day of last frost using the last day the maximum temperature was below 0°C. Try estimating the day of last frost by the last day the midrange temperature was below 0°C? Which estimate yields the most accurate predictions? What if you ignore the day of last frost and simply calculate the sum of the daily temperatures squared between February 1 and the bloom date? When you change the time interval, be sure to calculate the new specific quantity of heat needed to bloom. (Beginner)\nWrite a program that finds the time interval which yields the best predictions. (Intermediate)\nWrite a program that calculates the prediction rule for many different time intervals. Use cross-validation to combine these prediction rules into a single prediction rule. (Advanced)\n\nQuetelet’s law only considers the temperature. Would additional information provide more accurate predictions?\n\nIs the specific quantity of heat needed to bloom different in years with abnormally cold winters? Would the predictions be more accurate if you use one quantity of heat for years with cold winters and a different quantity of heat for years with warm winters? (Beginner)\nIs the estimated quantity of heat needed to bloom similar for locations close in space and time? Write a program that leverages spatial and temporal correlation to improve the accuracy of the predictions. (Intermediate)\nSome biologists report that a plant must be exposed to a fixed amount of cold temperature in the winter – in addition to a fixed amount of warm temperature in the spring – before it can bloom. Augment the law of the flowering plants to require the accumulation of a specific quantity of cold temperature before the accumulation of a specific quantity of warm temperature. Write a program that uses this new law to predict the day the lilac blooms. (Advanced)\n\n\nFeeling good about your prediction algorithm? Show it off at the annual Cherry Blossom Prediction Competition!"
  },
  {
    "objectID": "ideas/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "href": "ideas/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Appendix: Preparing USA NPN Data",
    "text": "Appendix: Preparing USA NPN Data\n```{r}\n# 1. download lilac data using `rnpn`\nusa_npn &lt;- \n  npn_download_individual_phenometrics(request_source = \"Jonathan Auerbach\",\n                                       year = 1900:2050,\n                                       species_ids = 36,                       \n                                       phenophase_ids = c(77, 412))            \n\n# 2. limit analysis to sites that report more than 25 times\nsite_ids &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(n = n()) %&gt;% filter(n &gt; 25) %&gt;% pull(site_id)\n\nusa_npn &lt;- \n  usa_npn %&gt;% \n  filter(site_id %in% site_ids)\n\n# 3. find nearest weather stations for each site\nlocations &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(latitude = first(latitude), \n            longitude = first(longitude))\n\nstations &lt;- \n  ghcnd_stations() %&gt;%\n  filter(first_year &lt;= min(usa_npn$first_yes_year),\n         last_year  &gt;= max(usa_npn$first_yes_year),\n         state != \"\") %&gt;%\n  group_by(id, latitude, longitude, state) %&gt;%\n  summarize(temp_flag = sum(element %in% c(\"TMIN\", \"TMAX\"))) %&gt;%            \n  filter(temp_flag == 2) %&gt;% \n  ungroup()\n\ndist &lt;- function(x, y = stations %&gt;% select(latitude, longitude)) \n  stations$id[which.min(sqrt((x[1] - y[,1])^2 + (x[2] - y[,2])^2)[,1])]\n\nlocations$station_id &lt;- apply(locations, 1, function(x) dist(c(x[\"latitude\"], x[\"longitude\"])))\n\n# 4. get weather data from nearest station using `rnoaa`\nget_station_data &lt;- function(station_id) \n  ghcnd_search(stationid = station_id,\n               var = c(\"tmin\", \"tmax\"),\n               date_min = \"1956-01-01\",\n               date_max = \"2011-12-31\") %&gt;%\n  reduce(left_join, by = c(\"id\", \"date\")) %&gt;%\n  transmute(id, \n            date, \n            tmax = tmax / 10,\n            tmin = tmin / 10)\n\nusa_npn &lt;- \n  locations %&gt;%\n  mutate(temp = map(station_id, get_station_data)) %&gt;%\n  right_join(usa_npn, by = c(\"site_id\", \"latitude\", \"longitude\")) %&gt;% \n  group_by(rownames(usa_npn)) %&gt;% \n  mutate(temp = map(temp, ~ .x %&gt;% \n                      filter(format(date, \"%Y\") == first_yes_year) %&gt;%\n                      mutate(temp = (tmin + tmax) / 2)),\n         num_obs = map(temp,~ sum(format(.x$date,\"%j\") &lt;= 150)),\n         doy = first_yes_doy, year = first_yes_year) %&gt;% \n  unnest(num_obs) %&gt;%  \n  filter(num_obs == 150) %&gt;%\n  ungroup()\n```\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nJonathan Auerbach is an assistant professor in the Department of Statistics at George Mason University. His research covers a wide range of topics at the intersection of statistics and public policy. His interests include the analysis of longitudinal data, particularly for data science and causal inference, as well as urban analytics, open data, and the collection, evaluation, and communication of official statistics. He co-organizes the annual Cherry Blossom Prediction Competition with David Kepplinger and Elizabeth Wolkovich.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Jonathan Auerbach\n\n\n  Text and code are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence, except where otherwise noted.\n\n\n\nHow to cite\n\nAuerbach, Jonathan. 2023. “A demonstration of the law of the flowering plants.” The Data Shrink, April 13, 2023. \n\n\n\n\n\n\n\n\n\nSource: Wikimedia Commons\n\n\nSource: Gallica\n\n\nSource: Gallica\n\n\nSource: USA National Phenology Network\n\n\nSource: USA National Phenology Network\n\n\nSource: USA National Phenology Network\n\n\nAuthor provided, CC BY 4.0\n\n\nAuthor provided, CC BY 4.0\n\n\nAuthor provided, CC BY 4.0\n\n\nSource: USA National Phenology Network"
  },
  {
    "objectID": "ideas/posts/2024/06/11/ai-series-7.html",
    "href": "ideas/posts/2024/06/11/ai-series-7.html",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "",
    "text": "“There’s some beautiful stories in clinical notes,” said Mark Sales, global strategy leader of the cloud technology company Oracle Life Sciences. He was speaking to delegates at the 2024 London Biotechnology Show about “unlocking health data and artificial intelligence within life sciences”, where opportunities abound, such as exploiting large language models (LLMs) to process some of the detailed information currently hidden in clinical notes into more structured data to inform fields like oncology. Oracle are also looking into using AI to take some of the luck out of connecting the right patients with clinical trials that might help them. The AI in Medicine and Surgery group at the University of Leeds headed by Sharib Ali has demonstrated the potential to reduce the number of times patients need to go through uncomfortable procedures like oesophageal scansfor Barrett’s syndrome , and is working on the potential to provide haptic feedback for robot mediated surgery. The London Biotechnology Showcase delegates had already heard about all these opportunities. Nonetheless Sales’s talk had opened with a note of caution: “There’s a lot more we could do, and there’s a lot more we probably shouldn’t do.”\nIt is an increasingly familiar caveat. “In the best scenario, AI could widely enrich humanity, equitably equipping people with the time, resources, and tools to pursue the goals that matter most to them,” suggest the Partnership on AI, a non-profit partnership of academic, civil society, industry, and media organizations. The goal of the partnership is to ensure AI brings a net positive contribution to society as a whole not just a lucky minority, which they suggest will not necessarily be the case if we rely on chance and market forces to direct progress. While people working in developing and deploying AI tackle the burgeoning size and complexity of their models, as well as the myriad requirements of testing and training data, establishing whether a model is fit for purpose, and dodging the numerous pitfalls that cause most AI projects to fail, perhaps the greatest challenge remains the range of ethical considerations including inclusiveness and fairness, robustness and reliability, transparency and accountability, privacy and security and general forethought and design. The scope of societal impact can reach far further than the immediate sphere of interaction with the model, or the interests of the companies deploying them, suggesting the need for some sort of governing forces.\nHowever, technology is moving fast in a lot of different directions. Even with agreed sound values that all technological developments should respect, there is still space for companies to deploy AI models without supplying the necessary resources and expertise so that the roll out meets ethical and societal expectations. This expertise can range from the statistical skills required to ensure the appropriate level of representation in training datasets to the social science understanding to extrapolate potential implications for human behaviour when interacting with the technology.\nAlthough the right checks and balances to avoid potential negative societal impacts have been slower to develop than the technologies they should be regulating, some guiding principles are emerging from organisations labouring to assess with greater clarity what the real immediate and longer term hazards are, what has worked well in other sectors, and the impact of government actions so far. There is an element of urgency in the challenge. As the Partnership on AI put it, “Our current moment serves as a profound opportunity — one that we will miss if we don’t act now.”"
  },
  {
    "objectID": "ideas/posts/2024/06/11/ai-series-7.html#high-stakes",
    "href": "ideas/posts/2024/06/11/ai-series-7.html#high-stakes",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "High stakes",
    "text": "High stakes\nWhen Open AI publicised their Voice Engine’s ability to clone human voices from just 15s of audio, they too flagged the potential benefit for people with poor health conditions, since those with deteriorating speech could find a means to have their speech restored. However, voice clones had already been used to make robot calls to voters imitating the voice of President Joe Biden and telling voters to stay at home.\n“The question you have to ask there is what’s the societal benefit of that tool? And what are the risks,” associate director at the Ada Lovelace Institute Andrew Strait told The Data Shrink. “They thankfully decided to not fully release it,” he adds, highlighting how the timing “right before an election year with 40 democracies across the world” could have made the release particularly problematic.\n\n\n\n\n\n\nFigure 1: Themis, goddess of justice. External governance is required to ensure the outcomes of AI deployment are safe and just. Credit Shutterstock, Michal Bednarek.\n\n\n\nWhile OpenAI’s voice engine might have made voice cloning more accessible had they proceeded with a full release, voice cloning is clearly still well within reach for some already. Strait cites the experiences of hundreds of performing artists in the UK over the past few months that have been brought to the attention of the Ada Lovelace Institute. “They’re brought into a room; they’re asked to record their voice and have their face and likeness scanned; and that’s the end of their career,” says Strait. The sums paid to artists on these transactions are not large either. “They are never going to be asked to come back for audition again, because they [the companies] can generate their likeness, that voice doing anything that a producer wants without any sense of attribution, further payments, or consent to be used in that way.”\nCustomer service is another sector where jobs have been threatened with replacement by a generative AI chatbot, however the technology can run into problems since gen-AI is known to “hallucinate”, generating false information. Air Canada has just lost a case defending its use of a chatbot that misinformed a customer that they could apply for a bereavement fare retroactively, which is not the case according to Air Canada’s bereavement fare policy. In their defence Air Canada flagged that the chatbot had supplied a link to a webpage with the correct information but the court ruled that there was no reason to believe the webpage information over the chatbot, or for the customer to double check the information they had been supplied. While there are ways to mitigate problems with gen-AI with the right teams in place , other industries have also hit problems with the accuracy and reliability of gen-AI, which may dampen the impact AI has on the labour market. All in all the wider picture of how AI deployment may affect jobs is largely a matter of speculation. Here a US piloted scheme may soon provide framework for a more data informed approach to tackling AI’s impact on the workforce.\nStrait highlights that conversations that centre around efficiency when weighing up the possible advantages of introducing AI can be ill informed. “If we’re talking about an allocation of resources in which we’re spending an increasing amount of money on automating certain parts of the NHS, or healthcare or the education system, or public sector services, how are we making the decisions that are determining if that is worth the value for money? Instead of investing in more doctors, more teachers, more social workers?” He tells The Data Shrink that these are the questions he and his colleagues at the Ada Lovelace Institute are often pushing governments to try to answer and evidence rather than to just assume the benefits will accrue. When it comes to measures of success of an AI model, Strait says “It’s often defined in terms of how many staff can be cut and still deliver some kind of service…This is not a good metric of success,” he adds. “We don’t want to just get rid of as many jobs as we can, right, we want to actually see improvements in care, improvements in service.”\nMichael Katell, ethics fellow in the Turing’s Public Policy Programme and a visiting Senior lecturer at the Digital Environment Research Institute (DERI) at Queen Mary University of London suggests the problems may go deeper still when looking at the use of generative AI in creative industries. “There are definitely parallels with prior waves of disruption,” he says citing as an example the move to drum-based and eventually laser printing as opposed to manual typesetting. “A key difference, though, is that, in the creative arts, we’re talking about contributions to culture, and culture is something that, I think we often take for granted.” He highlights the often overlooked role cultural practices that enable and empower shared experiences have in holding society together. These may come in various forms from works of art to theatre, and the working and living practices among the wider community may play an important role too. While acknowledging there may be interesting and fascinating uses of AI in art to explore, Katell adds, “If we’re not attending to maintaining some aspects, or trying to manage the changes that are happening in our culture, I think we’ll see societal level effects that are much greater than the elimination of some jobs.”"
  },
  {
    "objectID": "ideas/posts/2024/06/11/ai-series-7.html#the-need-for-legislation",
    "href": "ideas/posts/2024/06/11/ai-series-7.html#the-need-for-legislation",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "The need for legislation",
    "text": "The need for legislation\nThese stakes all highlight the need for regulatory interventions. However, most governments, bar China and the EU, have so far favoured “voluntary commitments” towards AI safety, which would seem to fall short of providing the kind of governance over the sector that can be robustly enforced. In a recent blog Strait alongside the Ada Lovelace Institute’s UK public policy lead Matt Davies and associate director (Law & Policy) Michael Birtwhistle, “evaluate the evaluations” of the UK’s AI Safety Institute for companies that have opted in for these voluntary commitments. They highlight that on the whole the companies planning to release the product hold too much control over how the evaluation can take place, ultimately empowering them to direct tests in their favour, which inhibits efforts at robust monitoring. Furthermore, there is usually no avenue for the necessary scrutiny of training data sets. Even withstanding these limitations, Davies, Strait and Birtwhistle conclude that “conducting evaluations and assessments is meaningless without the necessary enforcement powers to block the release of dangerous or high-risk models, or to remove unsafe products from the market.”\nThe reticence to implement firmer regulation might be attributed in some part to the perceived benefits to the state when their AI companies succeed. One often perceived benefit is that the percolating profits these companies accrue may benefit the economic buoyancy of the societies they function within. There is also cause for sovereign state competitiveness in “AI prowess” that stems from the potential for AI-based technology to underpin all aspects of society, prompting what has been described as an “AI arms race”. Here the UK may well regret allowing Google to acquire Deep Mind, whose output is responsible for bolstering the “UK’s share” of citations in the top 100 recent AI papers from 1.9% to 7.2%. However, a lack of robust regulation may prove as much a disservice to the companies releasing AI products as it is to society as a whole.\n“The medicine sector here [in the UK] is thriving, not in spite of regulation, but because of regulation,” says Strait. “People trust that the products you develop here are safe.” Katell, highlights the impact of pollution legislation on the automotive industry. “It jumped forward invention and discovery in automotive technology,” he tells The Data Shrink. “It seems prosaic in hindsight, but it wasn’t, it was a major innovation that was promoted by regulators, promoted by legislators.” The UK government’s chief scientific advisor Angela McLean seems to agree. “Good regulation is good for innovation,” she replied when asked about balancing regulation with favourable conditions for a flourishing AI sector at an Association of British Science Writers’ event in May. “We’re not there yet,” she added. The challenge is pinning down what good regulation looks like."
  },
  {
    "objectID": "ideas/posts/2024/06/11/ai-series-7.html#regulatory-ecosystems",
    "href": "ideas/posts/2024/06/11/ai-series-7.html#regulatory-ecosystems",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "Regulatory ecosystems",
    "text": "Regulatory ecosystems\nAs has been emphasised throughout the series, making a success of an AI project requires a unique skillset that combines expertise in AI with the domain expertise for the sector the project is contributing to, and there is often a dearth of people that straddle both camps. The same hunt for “unicorns” with useful expertise in the tech sector and policymakers can also be an obstacle for developing “good regulation”. One solution is to bring people from the different disciplines together to develop legislation collaboratively, as was arguably the case with the roll out of General Data Protection Regulations (GDPR) in 2018. “Policymakers and academics, they worked very closely together in the crafting of that law,” says Katell. “It was one of those rare moments in which we saw the boundaries really dissolve between policy and academia in a way that delivered something that I think we can agree was largely a positive outcome.”\nWhen it comes to AI, an obstacle to that kind of collaboration has been the lack of a common language. In “Defining AI in policy and practice” in 20201, Katell alongside Peaks Krafft at the University of Oxford and co-authors found that AI researchers favoured definitions of AI that “emphasise technical functionality”, whereas policy-makers tended towards definitions that “compare systems to human thinking and behavior”, which AI systems remain far from achieving. Strait also highlights a recurring theme among those without experience of actually making AI systems in overselling AI capabilities in suggestions that it will “help solve climate change” or “cure cancer”. “How are you measuring that?” he asks. “How are we making a clear sense of the efficacy, the proof behind those kinds of statements? Where are the case studies that actually work, and how are we determining that’s working?”\n\n\n\n\n\n\nFigure 2: Safety first. External governance is required to ensure the outcomes of AI deployment are safe and knock on effects have been considered. Credit: Shutterstock. Photo by 3rdtimeluckystudio.\n\n\n\nAs Krafft et al. point out in their 2020 paper, such exaggerated perceptions of AI capabilities can also hamper regulation. “As a result of this gap,” they write, “ethical and regulatory efforts may overemphasise concern about future technologies at the expense of pressing issues with existing deployed technologies.” Here a better understanding of what AI is can be helpful to focus attention on the problems that exist now – not just the potential workforce impact, but the carbon cost of training large language models, activities like nonconsensual gen-AI porn aggravating online gender inequality, and a widening digital divide disadvantaging pupils, workers and citizens who cannot afford all the latest AI tools, among others.\nFortunately, there has already been progress to breach the language divide between policy makers and the tech sector. “The current definitions [championed in policy circles] say things like technologies that can perform tasks that require intelligence when humans do them,” says Katell, which he describes as a far more sober and realistic definition than likening technologies to the way humans think and work. “This is really important,” he adds. “Because some of the problems that we see with AI now are symptomatic of the fact that they’re not humans and that they don’t have the same experience of the world.” As an example he describes someone driving a car with child in the car seat, calling on all their training and experience of road use to navigate roads and other traffic, while juggling their attention between driving and the child. “Things that AI is too brittle, to accomplish,” he adds, highlighting how a simple model may identify school buses in images quite impressively until it is presented with an image of a bus upside down. “The flexibility and adaptability, the softness of human reason, is actually its strength, its power.”\nGetting everybody on the same page can also help provide a more multimodal approach to governance. Empowering independent assessors of AI product safety prior to release is one thing but as Strait points out, “It could be more like the environmental sector, where we have a whole ecosystem of environmental impact assessments, organisations and consultancies that do this kind of work for different organisations and companies.” Internal teams within companies can play an important role too so long as they work sufficiently independently from the companies themselves. When set up with the right balance of expertise they can be better placed to understand and hence assess the technology and practical elements of its implementation. Although such teams can be expensive, getting the technical evaluation and consideration of ethical issues right can pose a competitive advantage for the companies themselves as well as providing a more thorough safeguard for society at large. Nonetheless there are also obvious advantages in having external regulatory bodies, which do not need to take into account the company’s profit margins or shareholders’ needs. An ideal set up might incorporate both approaches. In fact in their appraisal of the current UK AI Safety Institute arrangement, Davies, Strait and Birtwistle first highlight the need to integrate the AI Safety Institute “into a regulatory structure with complementary parts that can provide appropriate, context-specific assurance that AI systems are safe and effective for their intended use.”"
  },
  {
    "objectID": "ideas/posts/2024/06/11/ai-series-7.html#prosperity-for-all",
    "href": "ideas/posts/2024/06/11/ai-series-7.html#prosperity-for-all",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "Prosperity for all",
    "text": "Prosperity for all\nWith all the precedents in other sectors from environmental impact checks to pharmacology, an organised framework or ecosystem for robust, independent and meaningful evaluation of AI product safety seems an inevitable imperative, albeit potentially expensive. (Davies, Strait and Birtwistle cite £100 million a year as a typical cost for safety driven regulatory systems in the UK2, and the expertise demands of AI could further increase costs.) However, such regulatory reform will likely slow down the pace of technological development and the route to market. While the breathing space to adjust to the societal changes they bring with them may be welcomed by some, the delay can be quite unpopular in a tech sector where the ethos is famed for embracing a “move fast, break things” mentality. As Katell points out that ideal is based on the notion that the things being broken were unimportant – when it’s vulnerable people and societies that is “unacceptable breakage”.\nStrait also highlights the cultural mismatch between the companies developing AI products – where the research to market pipeline is extremely fast – and the sectors those tools are intended to serve, such as social care, education and health. Although Open AI eventually decided against full release of the Voice Engine, when it comes to the ethos of some AI technology companies , “The default is to put things out there and to not think through the ethical and societal implications,” says Strait who has direct experience of working for a company producing AI tools in the past. “I think it’s so critical for data scientists and ethicists to explore, and do that translation and interrogation of what are the ethics of the sector that we’re working in?”\nKatell voices concern shared by many that at present AI is under the control of a very small handful of very large, powerful technology companies, and as a result the AI releases making the most impact are targeting the agendas of the companies releasing them and their current and anticipated customer base, as opposed to the needs of society. The potential for such large tech agents to become too big to fail poses additional regulatory challenges. While many may lament the tension between a demand for open source data sets for testing AI models versus the need to respect data privacy, security and confidentiality, there have already been widely mooted instances where certain companies may not have met expectations for respecting copyright and terms of service. In fact the tech giants are not the only people developing AI models and the open source community have been known to pose valuable competition that may temper the tendency for AI to concentrate a lot of power into the hands of a small few3. However, open source developers can also pose a certain amount of regulatory complexity.\nThere is also an argument that these efforts should broaden their scope beyond baseline AI safety and aim to focus efforts in AI development towards tools that actively promote greater wellbeing and prosperity to the many. “We need to bring in other values like fairness, justice, and simple things like explainability, gender equity, racial equity,” says Katell, highlighting some of the other qualities that demand attention among others. Taking explainability as an example, there is increasing awareness of the need to understand how certain outputs are reached in order for people to feel comfortable with the technology, and the outputs requiring explanations differ from person to person. Although it can be hard to explain AI outputs, progress is being made in this direction. As Katell says, “We’re not helpless in managing these types of disruptions. It’s a matter of societies coming together and deciding that they can be managed.”\n\nExplore more data science ideas\n\n\n\n\n\nAbout the authors\n\nAnna Demming is a freelance science writer and editor based in Bristol, UK. She has a PhD from King’s College London in physics, specifically nanophotonics and how light interacts with the very small, and has been an editor for Nature Publishing Group (now Springer Nature), IOP Publishing and New Scientist. Other publications she contributes to include The Observer, New Scientist, Scientific American, Physics World and Chemistry World.\n\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. \n\n\n\nHow to cite\n\nDemming, Anna. 2024. “Ensuring new AI technologies help everyone thrive .” The Data Shrink, June 11, 2024. URL"
  },
  {
    "objectID": "ideas/posts/2024/06/11/ai-series-7.html#footnotes",
    "href": "ideas/posts/2024/06/11/ai-series-7.html#footnotes",
    "title": "AI series: Ensuring new AI technologies help everyone thrive",
    "section": "References",
    "text": "References\n\n\nKrafft, P. M., Young, M., Katell, M., Huang, K. & Bugingo, G. Defining AI in Policy versus Practice AIES ’20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society 72-78 (2020)↩︎\nSmakman, J, Davies, M. & Birtwhistle, M. Mission critical Ada Lovelace Policy Briefing (2023)↩︎\nGoogle “We Have No Moat, And Neither Does OpenAI semianalysis.com (2023) (semianalysis.com)↩︎"
  },
  {
    "objectID": "about-rwds.html",
    "href": "about-rwds.html",
    "title": "Welcome to The Data Shrink",
    "section": "",
    "text": "Welcome to the home of The Data Shrink, a new project from the Royal Statistical Society, in partnership with the American Statistical Association. This site and its content are being developed by data science practitioners and leaders with a single goal in mind: to help you deliver high quality, ethical, impactful data science in your workplace.",
    "crumbs": [
      "Welcome to The Data Shrink"
    ]
  },
  {
    "objectID": "about-rwds.html#what-are-our-aims",
    "href": "about-rwds.html#what-are-our-aims",
    "title": "Welcome to The Data Shrink",
    "section": "What are our aims?",
    "text": "What are our aims?\nThe Data Shrink aims to be a trusted, go-to source for high-quality, engaging and inspiring content which helps data science students, practitioners and leaders to:\n\ndiscover and learn more efficiently;\n\nacquire practical problem-solving skills;\n\nshare their knowledge and accomplishments publicly;\n\nwork smarter, ethically, and more effectively.",
    "crumbs": [
      "Welcome to The Data Shrink"
    ]
  },
  {
    "objectID": "about-rwds.html#what-we-provide",
    "href": "about-rwds.html#what-we-provide",
    "title": "Welcome to The Data Shrink",
    "section": "What we provide",
    "text": "What we provide\nResources are created to meet the needs of our target audiences. These include:\n\nCase studies – showing how data science is used to solve real-world problems in business, public policy and beyond.\nExplainers – interrogating the underlying assumptions and limitations of data science tools and methods, to help data scientists make smarter, more informed analytical choices.\nExercises – to challenge and develop the analytical mindset that all data scientists need to succeed.\n\nAdvice – interviews, Q&As, and FAQs on such topics as data science ethics, career paths, and communication, to support professional development.\n\nWe are also curating resources to help data scientists identify trustworthy, high-quality content. These include:\n\nTraining guides – step-by-step approaches and recommended sources for learning new skills and methods.\nDatasets – tagged and sorted to help educators and practitioners find data to meet their teaching and training needs.\nFeeds – who and what to follow to keep up with new ideas and developments.",
    "crumbs": [
      "Welcome to The Data Shrink"
    ]
  },
  {
    "objectID": "about-rwds.html#how-you-can-get-involved",
    "href": "about-rwds.html#how-you-can-get-involved",
    "title": "Welcome to The Data Shrink",
    "section": "How you can get involved",
    "text": "How you can get involved\nSee our open call for contributions.",
    "crumbs": [
      "Welcome to The Data Shrink"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders of The Data Shrink (RWDS) pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders of The Data Shrink (RWDS) pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Our standards",
    "text": "Our standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement responsibilities",
    "text": "Enforcement responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Scope",
    "text": "Scope\nThis Code of Conduct applies within all community spaces (encompassing this site, our GitHub repository, our social media channels, and any RWDS-organised online and offline events). It also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nNote that unless prior permission is agreed in writing with the editor of RWDS, only the editor and editorial board of RWDS may officially represent the community. Comment to the media must only be given by appointed representatives and must be approved by the RSS press office.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement",
    "text": "Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at rwds@rss.org.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement guidelines",
    "text": "Enforcement guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Attribution",
    "text": "Attribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2024/03/editors-note.html",
    "href": "viewpoints/editors-blog/posts/2024/03/editors-note.html",
    "title": "Editor’s note: Not saying goodbye, just saying…",
    "section": "",
    "text": "It’s not easy to leave a brilliant group of people you’ve worked with for almost a decade, but in a month’s time I’ll be moving on from the Royal Statistical Society (RSS).\nWhen I joined RSS in June 2014 I was looking for new challenges. I wanted to find out more about the ways statistics and data are used to understand and solve problems and inform decisions in science, business and industry, public policy, health… I could go on! Working for the RSS certainly delivered on that front: as editor of Significance for eight years and of The Data Shrink more recently, I have had many opportunities to learn.\nPretty much every day of my working life for the past nine years, eight months or so involved speaking with expert statisticians and data scientists or reading about their work. When there were things I didn’t understand, they were always happy to explain. When I shared my ideas for how to make their articles clearer or more readable, they took the time to listen. Together, we worked to create accessible, engaging stories about statistics and data. There have been hundreds of these collaborations over the years – too many to namecheck individually – but I have enjoyed them all, and I’ve learned something from each of them.\nBefore I head off to pursue a new set of challenges and learning opportunities, I want to say a big thank you to all the RSS staff and members, past and present, that I’ve been lucky to call my colleagues. Thank you also to the staff and members of the American Statistical Association who have been valued partners on Significance over the years and now RWDS too. It’s been a privilege to work with you all.\nThe chance to launch RWDS has been a particular highlight of my time at RSS, and I am grateful to have had the support and input of The Alan Turing Institute and many of its wonderful staff and researchers on this project. I’m excited to see the site continue to grow and develop into a valuable resource for the data science community, and I look forward to reading an upcoming series of articles that will explore the statistical and data science perspectives on AI – stay tuned for more on this soon.\nStatistics and data will continue to be a big part of my life, so this isn’t “goodbye.” Instead, I’ll just say, let’s keep in touch – and thank you for reading!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Pete Pedroza on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “Editor’s note: Not saying goodbye, just saying…” The Data Shrink, March 6, 2024. URL"
  },
  {
    "objectID": "viewpoints/editors-blog/posts/2024/01/22/gen-ai-framework.html",
    "href": "viewpoints/editors-blog/posts/2024/01/22/gen-ai-framework.html",
    "title": "UK government sets out 10 principles for use of generative AI",
    "section": "",
    "text": "The UK government has published a framework for the use of generative AI, setting out 10 principles for departments and staff to think about if using, or planning to use, this technology.\nIt covers the need to understand what generative AI is and its limitations, the lawful, ethical and secure use of the technology, and a requirement for “meaningful human control.”\nThe focus is on large language models (LLMs) as, according to the framework, these have “the greatest level of immediate application in government.”\nIt lists a number of promising use cases for LLMs, including the synthesise of complex data, software development, and summaries of text and audio. However, the document cautions against using generative AI for fully automated decision-making or in contexts where data is limited or explainability of decision-making is required. For example, it warns that:\n\n“although LLMs can give the appearance of reasoning, they are simply predicting the next most plausible word in their output, and may produce inaccurate or poorly-reasoned conclusions.”\n\nAnd on the issue of explainability, it says that:\n\n“generative AI is based on neural networks, which are so-called ‘black boxes’. This makes it difficult or impossible to explain the inner workings of the model which has potential implications if in the future you are challenged to justify decisioning or guidance based on the model.”\n\nThe framework goes on to discuss some of the practicalities of building generative AI solutions. It talks specifically about the value a multi-disciplinary team can bring to such projects, and emphasises the role of data scientists:\n\n“data scientists … understand the relevant data, how to use it effectively, and how to build/train and test models.”\n\nIt also speaks to the need to “understand how to monitor and mitigate generative AI drift, bias and hallucinations” and to have “a robust testing and monitoring process in place to catch these problems.”\nWhat do you make of the Generative AI Framework for His Majesty’s Government? What does it get right, and what needs more work?\n\n\n\n\n\n\nAnd in case you missed it…\n\n\n\nNew York State issued a policy on the Acceptable Use of Artificial Intelligence Technologies earlier this month. Similar to the UK government framework, it references the need for human oversight of AI models and rules out use of “automated final decision systems.” There is also discussion of fairness, equity and explainability, and AI risk assessment and management.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Massimiliano Morosinotto on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “UK government sets out 10 principles for use of generative AI.” The Data Shrink, January 22, 2024. URL"
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "",
    "text": "A little over a month ago, governments, technology firms, multilateral organisations, and academic and civil society groups came together at Bletchley Park – home of Britain’s World War II code breakers – to discuss the safety and risks of artificial intelligence.\nOne output from that event was a declaration, signed by countries in attendance, of their resolve to “work together in an inclusive manner to ensure human-centric, trustworthy and responsible AI that is safe, and supports the good of all.”\nWe also heard from UK prime minister Rishi Sunak of plans for an AI Safety Institute, to be based in the UK, which will “carefully test new types of frontier AI before and after they are released to address the potentially harmful capabilities of AI models, including exploring all the risks, from social harms like bias and misinformation, to the most unlikely but extreme risk, such as humanity losing control of AI completely.”\nBut at a panel debate at the Royal Statistical Society (RSS) the day before the Bletchley Park gathering, data scientists, statisticians, and machine learning experts questioned whether such an institute would be sufficient to meet the challenges posed by AI; whether data inputs – compared to AI model outputs – are getting the attention they deserve; and whether the summit was overly focused on AI doomerism and neglecting more immediate risks and harms. There were also calls for AI developers to be more driven to solve real-world problems, rather than just pursuing AI for AI’s sake.\nThe RSS event was chaired by Andrew Garrett, the Society’s president, and formed part of the national AI Fringe programme of activities. The panel featured:\nWhat follows are some edited highlights and key takeaways from the discussion."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#ai-safety-and-ai-risks",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#ai-safety-and-ai-risks",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "AI safety, and AI risks",
    "text": "AI safety, and AI risks\nAndrew Garrett: For those who were listening to the commentary last week, the PM [prime minister] made a very interesting speech. Rishi Sunak announced the creation of the world’s first AI Safety Institute in the UK, to examine, evaluate and test new types of AI. He also stated that he pushed hard to agree the first ever international statement about the risks of AI because, in his view, there wasn’t a shared understanding of the risks that we face. He used the example of the IPCC, the Intergovernmental Panel on Climate Change, to establish a truly global panel to publish a “state of AI science” report. And he also announced an investment in raw computing power, so around a billion pounds in a supercomputer, and £2.5 billion in quantum computers, making them available for researchers and businesses as well as government.\nThe RSS provided two responses this year to prominent [AI policy] reviews. The first was in June on the AI white paper, and the second was on the House of Lords Select Committee inquiry into large language models back in September. How do they relate to what the PM said? There’s some good news here, and maybe not quite so good news.\nFirst, the RSS had requested investments in AI evaluation and a risk-based approach. And you could argue, by stating that there will be a safety institute, that that certainly ticks one of the boxes. We also recommended investment in open source, in computing power, and in data access. In terms of computing power, that was certainly in the [PM’s] speech. We spoke about strengthening leadership, and in particular including practitioners in the [AI safety] debate. A lot of academics and maybe a lot of the big tech companies have been involved in the debate, but we want to get practitioners – those close to the coalface – involved in the debate. I’m not sure we’ve seen too much of that. We recommended that strategic direction was provided, because it’s such a fast-moving area, and the fact that the Bletchley Park Summit is happening tomorrow, I think, is good for that. And we also recommended that data science capability was built amongst the regulators. I don’t think there was any mention of that.\nThat’s the context [for the RSS event today]. What I’m going to do now is ask each of the panellists to give an introductory statement around the AI summit, focusing on the safety aspects. What do they see as the biggest risk? And how would they mitigate or manage this risk?\nDetlef Nauck: I work at BT and run the AI and data science research programme. We’ve been looking at the safety, reliability, and responsibility of AI for quite a number of years already. Five years ago, we put up a responsible AI framework in the company, and this is now very much tied into our data governance and risk management frameworks.\nLooking at the AI summit, they’re focusing on what they call “frontier models,” and they’re missing a trick here because I don’t think we need to worry about all-powerful AI; we need to worry about inadequate AI that is being used in the wrong context. For me, AI is programming with data, and that means I need to know what sort of data has been used to build the model, and I need AI vendors to be upfront about it and to tell me: What is the data that they have used to build it, how have they built it, or if they’ve tested for bias? And there are no protocols around this. So, therefore, I’m very much in favour of AI evaluation. But I don’t want to wait for an institute for AI evaluation. I want the academic research that needs to be done around this, which hasn’t been done. I want everybody who builds AI systems to take this responsibility and document properly what they’re doing.\n\n\n\n\n\n\n\n\n\n\n\nI hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.\n\n\n\nMihaela van der Schaar: I am an AI researcher building AI and machine learning technology. Before talking about the risks, I also would like to say that I see tremendous potential for good. Many of these machine learning AI models can transform for the better areas that I find extremely important – healthcare and education. That being said, there are substantial risks, and we need to be very careful about that. First, if not designed well, AI can be both unsafe as well as biased, and that could lead to tremendous impact, especially in medicine and education. I completely agree with all the points that the Royal Statistical Society has made not only about open source but also about data access. This AI technology cannot be built unless you have access to high quality data, and what I see a lot happening, especially in industry, is people have data sources that they’ll keep private, build second-rate or third-rate technology on them, and then turn that into commercialised products that are sold to us for a lot of money. If data is made widely available, the best as well as the safest AI can be produced, rather than monopolised.\nAnother area of risk that I’m especially worried about is human marginalisation. I hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned as an AI researcher about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.\nMartin Goodson: The AI Safety Summit is starting tomorrow. But, unfortunately, I think the government are focusing on the wrong risks. There are lots of risks to do with AI, and if you look at the scoping document for the summit, it says that what they’re interested in is misuse risk and the risk of loss of control. Misuse risk is that bad actors will gain access to information that they shouldn’t have and build chemical weapons and things like that. And the loss of control risk is that we will have this super intelligence which is going to take over and we should see, as is actually mentioned, the risk of the extinction of the human race, which I think is a bit overblown.\nBoth of these risks – the misuse risk and the loss of control risk – are potential risks. But we don’t really know how likely they are. We don’t even know whether they’re possible. But there are lots of risks that we do know are possible, like loss of jobs, and reductions in salary, particularly of white-collar jobs – that seems inevitable. There’s another risk, which is really important, which is the risk of monopolistic control by the small number of very powerful AI companies. These are the risks which are not just likely but are actually happening now – people are losing their jobs right now because of AI – and in terms of monopolistic control, OpenAI is the only company that has anything like a large language model as powerful as GPT-4. Even the mighty Google can’t really compete. This is a huge risk, I think, because we have no control over pricing: they could raise the prices if they wanted to; they could constrain access; they could only give access to certain people that they want to give access to. We don’t have any control over these systems.\nMark Levene: I work in NPL as a principal scientist in the data science department. I’m also emeritus professor in Birkbeck, University of London. I have a long-standing expertise in machine learning and focus in NPL on trustworthy AI and uncertainty quantification. I believe that measurement is a key component in locking-in AI safety. Trustworthy AI and safe AI both have similar goals but different emphases. We strive to demonstrate the trustworthiness of an AI system so that we can have confidence in the technology making what we perceive as responsible decisions. Safe AI puts the emphasis on the prevention of harmful consequences. The risk [of AI] is significant, and it could potentially be catastrophic if we think of nuclear power plants, or weapons, and so on. I think one of the problems here is, who is actually going to take responsibility? This is a big issue, and not necessarily an issue for the scientist to decide. Also, who is accountable? For instance, the developers of large language models: are they the ones that are accountable? Or is it the people who deploy the large language models and are fine-tuning them for their use cases?\nThe other thing I want to emphasise is the socio-technical characteristics [of the AI problem]. We need to get an interdisciplinary team of people to actually try and tackle these issues."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#do-we-need-an-ai-safety-institute",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#do-we-need-an-ai-safety-institute",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Do we need an AI Safety Institute?",
    "text": "Do we need an AI Safety Institute?\nAndrew Garrett: Do we need to have an AI Safety Institute, as Rishi Sunak has said? And if we don’t need one, why not?\nDetlef Nauck: I’m more in favour of encouraging academic research in the field and funding the kind of research projects that can look into how to build AI safely, [and] how to evaluate what it does. One of the key features of this technology is it has not come out of academic research; it has been built by large tech companies. And so, I think we have to do a bit of catch up in scientific research and in understanding how are we building these models, what can they do, and how do we control them?\nMihaela van der Schaar: This technology has a life of its own now, and we are using it for all sorts of things that maybe initially was not even intended. So, shall we create an AI [safety] institute? We can, but we need to realise first that testing AI and showing that it’s safe in all sorts of ways is complicated. I would dare say that doing that well is a big research challenge by itself. I don’t think just one institute will solve it. And I feel the industry needs to bear some of the responsibility. I was very impressed by Professor [Geoffrey] Hinton, who came to Cambridge and said, “I think that some of these companies should invest as much money in making safe AI as developing AI.” I resonated quite a lot with that.\nAlso, let’s not forget, many academic researchers have two hats nowadays: they are professors, and they are working for big tech [companies] for a lot of money. So, if we take this academic, we put them in this AI tech safety institute, we have potential for corruption. I’m not saying that this will happen. But one needs to be very aware, and there needs to be a very big separation between who develops [AI technology] and who tests it. And finally, we need to realise that we may require an enormous amount of computation to be able to validate and test correctly, and very few academic or governmental organisations may have [that].\n\n\n\n\nI think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?\n\n\n\n\n\n\n\n\n\n\nMartin Goodson: Can I disagree with this idea of an evaluation institute? I think it’s a really, really bad idea, for two reasons. The first is an argument about fairness. If you look at drug regulation, who pays for clinical trials? It’s not the government. It’s the pharmaceutical companies. They spend billions on clinical trials. So, why do we want to do this testing for free for the big tech companies? We’re just doing product development for them. It’s insane! They should be paying to show that their products are safe.\nThe other reason is, I think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. I think it’s pathetic. We were one of the main leaders of the Human Genome Project, and we really pushed it – the Wellcome Trust and scientists in the UK pushed the Human Genome Project because we didn’t want companies to have monopolistic control over the human genome. People were idealistic, there was a moral purpose. But now, we’re so reduced that all we can do is test some APIs that have been produced by Silicon Valley companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?\nMark Levene: Personally, I don’t see any problem in having an AI institute for safety or any other AI institutes. I think what’s important in terms of taxpayers’ money is that whatever institute or forum is invested in, it’s inclusive. One thing that the government should do is, we should have a panel of experts, and this panel should be interdisciplinary. And what this panel can do is it can advise government of the state of play in AI, and advise the regulators. And this panel doesn’t have to be static, it doesn’t have to be the same people all the time.\nAndrew Garrett: To evaluate something, whichever way you chose to do it, you need to have an inventory of those systems. So, with the current proposal, how would this AI Safety Institute have an inventory of what anyone was doing? How would it even work in practice?\nMartin Goodson: Unless we voluntarily go to them and say, “Can you test out our stuff?” then they wouldn’t. That’s the third reason why it’s a terrible idea. You’d need a licencing regime, like for drugs. You’d need to licence AI systems. But teenagers in their bedrooms are creating AI systems, so that’s impossible."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#lets-do-reality-centric-ai",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#lets-do-reality-centric-ai",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Let’s do reality-centric AI!",
    "text": "Let’s do reality-centric AI!\nAndrew Garrett: What are your thoughts about Rishi Sunak wanting the UK to be an AI powerhouse?\nMartin Goodson: It’s not going to be a powerhouse. This stuff about us being world leading in AI, it’s just a fiction. It’s a fairy tale. There are no real supercomputers in the UK. There are moves to build something, like you mentioned in your introduction, Andrew. But what are they going do with it? If they’re just going to build a supercomputer and carry on doing the same kinds of stuff that they’ve been doing for years, they’re not going to get anywhere. There needs to be a big project with an aim. You can build as many computers as you want. But if you haven’t got a plan for what to do with them, what’s the point?\nMihaela van der Schaar: I really would agree with that. What about solving some real problem: trying to solve cancer; trying to solve our crisis in healthcare, where we don’t have enough infrastructure and doctors to take care of us? What about solving the climate change problem, or even traffic control, or preventing the next financial crisis? I wrote a little bit about that, and I call it “let’s do reality-centric AI.” Let’s have some goal that’s human empowering, take a problem that we have – energy, climate, cancer, Alzheimer’s, better education for children, and more diverse education for children – and let us solve these big challenges, and in the process we will build AI that’s hopefully more human empowering, rather than just saying, “Oh, we are going to solve everything if we have general AI.” Right now, I hear too much about AI for the sake of AI. I’m not sure, despite all the technology we build, that we have advanced in solving some real-world problems that are important for humanity – and imminently important.\nMartin Goodson: So, healthcare– I tried to make an appointment with my GP last week, and they couldn’t get me an appointment for four weeks. In the US you have this United States Medical Licencing Examination, and in order to practice medicine you need to pass all three components, you need to pass them by about 60%. They are really hard tests. GPT-4 for gets over 80% in all three of those. So, it’s perfectly plausible, I think, that an AI could do at least some of the role of the GP. But, you’re right, there is no mission to do that, there is no ambition to do that.\nMihaela van der Schaar: Forget about replacing the doctors with ChatGPT, which I’m less sure is such a good idea. But, building AI to do the planning of healthcare, to say, “[Patient A], based on what we have found out about you, you’re not as high risk, maybe you can come in four weeks. But [patient B], you need to come tomorrow, because something is worrisome.”\nMartin Goodson: We can get into the details, but I think we are agreeing that a big mission to solve real problems would be a step forward, rather than worrying about these risks of superintelligences taking over everything, which is what the government is doing right now."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#managing-misinformation",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#managing-misinformation",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Managing misinformation",
    "text": "Managing misinformation\nAndrew Garrett: We have some important elections coming up in 2024 and 2025. We haven’t talked much about misinformation, and then disinformation. So, I’m interested to get your views here. How much is that a problem?\nDetlef Nauck: There’s a problem in figuring out when it happens, and that’s something we need to get our heads around. One thing that we’re looking at is, how do we make communication safe from bad actors? How do you know that you’re talking to the person you see on the camera and it’s not a deep fake? Detection mechanisms don’t really work, and they can be circumvented. So, it seems like what we need is new standards for communication systems, like watermarks and encryption built into devices. A camera should be able to say, “I’ve produced this picture, and I have watermarked it and it’s encrypted to a certain level,” and if you don’t see that, you can’t trust that what you see comes from a genuine camera, and it’s not artificially created. It’s more difficult around text and language – you can’t really watermark text.\nMark Levene: Misinformation is not just a derivative of AI. It’s a derivative of social networks and lots of other things.\nMihaela van der Schaar: I would agree that this is not only a problem with AI. We need to emphasise the role of education, and lifelong education. This is key to being able to comprehend, to judge for ourselves, to be trained to judge for ourselves. And maybe we need to teach different methods – from young kids to adults that are already working – to really exercise our own judgement. And that brings me to this AI for human empowerment. Can we build AI that is training us to become smarter, to become more able, more capable, more thoughtful, in addition to providing sources of information that are reliable and trustworthy?\nAndrew Garrett: So, empower people to be able to evaluate AI themselves?\nMihaela van der Schaar: Yes, but not only AI – all information that is given to us.\nMartin Goodson: On misinformation, I think this is really an important topic, because large language models are extremely persuasive. I asked ChatGPT a puzzle question, and it calculated all of this stuff and gave me paragraphs of explanations, and the answer was [wrong]. But it was so convincing I was almost convinced that it was right. The problem is, these things have been trained on the internet and the internet is full of marketing – it’s trillions of words of extremely persuasive writing. So, these things are really persuasive, and when you put that into a political debate or an election campaign, that’s when it becomes really, really dangerous. And that is extremely worrying and needs to be regulated.\n\n\n\n\n\n\n\n\n\n\n\nAt the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, ‘How did this information come about? Where did it come from?’\n\n\n\nMark Levene: You need ways to detect it. Even that is a big challenge. I don’t know if it’s impossible, because, if there’s regulation, for example, there should be traceability of data. So, at the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, “How did this information come about? Where did it come from?” But I agree that if you just look at an image or some text, and you don’t know where it came from, it’s easy to believe. Humans are easily fooled, because we’re just the product of what we know and what we’re used to, and if we see something that we recognise, we don’t question it."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#audience-qa",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#audience-qa",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Audience Q&A",
    "text": "Audience Q&A\n\nHow can we help organisations to deploy AI in a responsible way?\nDetlef Nauck: Help for the industry to deploy AI reliably and responsibly is something that’s missing, and for that, trust in AI is one of the things that needs to be built up. And you can only build up trust in AI if you know what these things are doing and they’re properly documented and tested. So that’s the kind of infrastructure, if you like, that’s missing. It’s not all big foundation models. It’s about, how do you actually use this stuff in practice? And 90% of that will be small, purpose-built AI models. That’s an area where the government can help. How do you empower smaller companies that don’t have the background of how AI works and how it can be used, how can they be supported in knowing what they can buy and what they can use and how they can use it?\nMark Levene: One example from healthcare which comes to mind: when you do a test, let’s say, a blood test, you don’t just get one number, you should get an interval, because there’s uncertainty. What current [AI] models do is they give you one answer, right? In fact, there’s a lot of uncertainty in the answer. One thing that can build trust is to make transparent the uncertainty that the AI outputs.\n\n\nHow can data scientists and statisticians help us understand how to use AI properly?\nMartin Goodson: One big thing, I think, is in culture. In machine learning – academic research and in industry – there isn’t a very scientific culture. There isn’t really an emphasis on observation and experimentation. We hire loads of people coming out of an MSc or a PhD in machine learning, and they don’t know anything, really, about doing an experiment or selection bias or how data can trip you up. All they think about is, you get a benchmark set of data and you measure the accuracy of your algorithm on that. And so there isn’t this culture of scientific experimentation and observation, which is what statistics is all about, really.\nMihaela van der Schaar: I agree with you, this is where we are now. But we are trying to change it. As a matter of fact, at the next big AI conference, NeurIPS, we plan to do a tutorial to teach people exactly this and bring some of these problems to the forefront, because trying really to understand errors in data, biases, confounders, misrepresentation – this is the biggest problem AI has today. We shouldn’t just build yet another, let’s say, classifier. We should spend time to improve the ability of these machine learning models to deal with all sorts of data.\n\n\nDo we honestly believe yet another institute, and yet more regulation, is the answer to what we’re grappling with here?\nDetlef Nauck: I think we all agree, another institute is not going to cut it. One of the main problems is regulators are not trained on AI, so it’s the wrong people looking into it. This is where some serious upskilling is required.\n\n\nAre we wrong to downplay the existential or catastrophic risks of AI?\nMartin Goodson: If I was an AI, a superintelligent AI, the easiest path for me to cause the extinction of the human race would be to spread misinformation about climate change, right? So, let’s focus on misinformation, because that’s an immediate danger to our way of life. Why are we focusing on science fiction? Let’s focus on reality.\n\n\nAI tech has advanced, but evaluation metrics haven’t moved forward. Why?\nMihaela van der Schaar: First, the AI community that I’m part of innovates at a very fast pace, and they don’t reward metrics. I am a big fan of metrics, and I can tell you, I can publish much faster a method in these top conferences then I can publish a metric. Number two, we often have in AI very stupid benchmarks, where we test everything on one dataset, and these datasets may be very wrong. On a more positive note, this is an enormous opportunity for machine learners and statisticians to work together and advance this very important field of metrics, of test sets, of data generating processes.\nMartin Goodson: The big problem with metrics right now is contamination, because most of the academic metrics and benchmark sets that we’re talking about, they’re published on the internet, and these systems are trained on the internet. I’ve already said that I don’t think this [evaluation] institute should exist. But if it did exist, there’s one thing that they could do, which is important, and that would be to create benchmark datasets that they do not publish. But obviously, you may decide, also, that the traditional idea of having a training set and a test set just doesn’t make any sense anymore. And there are loads of issues with data contamination, and data leakage between the training sets and the test sets."
  },
  {
    "objectID": "viewpoints/posts/2023/12/06/ai-fringe.html#closing-thoughts-what-would-you-say-to-the-ai-safety-summit",
    "href": "viewpoints/posts/2023/12/06/ai-fringe.html#closing-thoughts-what-would-you-say-to-the-ai-safety-summit",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Closing thoughts: What would you say to the AI Safety Summit?",
    "text": "Closing thoughts: What would you say to the AI Safety Summit?\nAndrew Garrett: If you were at the AI Safety Summit and you could make one point very succinctly, what would it be?\nMartin Goodson: You’re focusing on the wrong things.\nMark Levene: What’s important is to have an interdisciplinary team that will advise the government, rather than to build these institutes, and that this team should be independent and a team which will change over time, and it needs to be inclusive.\nMihaela van der Schaar: AI safety is complex, and we need to realise that people need to have the right expertise to be able to really understand the risks. And there is risk, as I mentioned before, of potential collusion, where people are both building the AI and saying it’s safe, and we need to separate these two worlds.\nDetlef Nauck: Focus on the data, not the models. That’s what’s important to build AI.\n\nDiscover more Viewpoints\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\nImages by Wes Cockx & Google DeepMind / Better Images of AI / AI large language models / Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” The Data Shrink, December 6, 2023. URL"
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html",
    "href": "contributor-docs/contributor-guidelines.html",
    "title": "Contributor guidelines",
    "section": "",
    "text": "Thank you for your interest in contributing to The Data Shrink. This page will walk you through the process of preparing and submitting your idea. If you haven’t done so already, please review our call for contributions before continuing.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "href": "contributor-docs/contributor-guidelines.html#site-functionality-and-ethos",
    "title": "Contributor guidelines",
    "section": "Site functionality and ethos",
    "text": "Site functionality and ethos\nThe Data Shrink is built on Quarto, the new open-source publishing system developed by Posit. The site has been designed from the ground up as a platform for data scientists, created by data scientists. Here’s what this means in practice:\n\nContributors can use data science software and tools to create content – e.g. Visual Studio Code, RStudio, Jupyter Lab; Python, R, Observable, and Shiny – allowing for the full integration of text, code, figures, equations, and other elements.\nReview and editing are transparent and collaborative, again making use of tools data scientists are familiar with – e.g. GitHub, Google Docs – for sharing and revising documents prior to publication.\nContent can be both engaging and interactive. Many data scientists learn by doing, so code can be made available as R Markdown or Jupyter Notebook files to be reused and experimented with offline. Or, the same documents can be used online through tools like Google Colab and Binder. Where appropriate, the use of interactive displays and Shiny apps is encouraged, allowing for data visualisations to be interrogated and regenerated on the fly.\nSite users are contributors too. Through annotation and commenting functionality, site users can interact and converse with authors and other members of the The Data Shrink community. And with all source files hosted on GitHub, users of our site can raise issues, or fork and propose improvements – leading to a true exchange of knowledge.\n\nFor more on how to work with The Data Shrink via GitHub, check out our repository README.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "href": "contributor-docs/contributor-guidelines.html#the-submission-process",
    "title": "Contributor guidelines",
    "section": "The submission process",
    "text": "The submission process\n\nContact The Data Shrink to discuss your proposed submission.\nWrite up a short content brief containing the following:\n\nTitle of submission\nAuthor name(s) and affiliation(s)\nTheme/topic area\nTarget audience\nSynopsis or sell line, summarising the story and its importance/value (250 words max.)\nKey audience takeaways\nFormats and features (e.g., text, audio, video; code blocks, interactive data visualisations, etc.)\nAccessibility considerations\nTarget length/word count\nFirst draft to be submitted by…\n\nThe RWDS_post_template repository on GitHub contains a Quarto document (content-brief.qmd) that can be used to produce a content brief in the style and format of a The Data Shrink article.\nOnce a content brief is finalised and approved, content is to be prepared in the agreed format and with reference to our style guide. For simple text-based articles, we recommend using Google Docs or Microsoft Word; for submissions that incorporate technical or multimedia content, such as code, equations or interactive graphics, we recommend the Quarto (.qmd) file format. Use the RWDS_post_template repository to create your draft article in Quarto using the correct style and formatting. A sample article in the repo (report.qmd) contains code examples for the Quarto features used by The Data Shrink. Documents can be submitted in Jupyter notebook (.ipynb) and R Markdown (.Rmd) formats but will require conversion before publishing.\nDraft submissions should be sent via email to The Data Shrink. Alternatively, contributors can commit their drafts to their own GitHub accounts using the RWDS_post_template repository and the repository link then shared with The Data Shrink.\n\n\nFor more information on how to work with The Data Shrink via GitHub, check out our GitHub repository README.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "href": "contributor-docs/contributor-guidelines.html#copyright-and-content-licencing",
    "title": "Contributor guidelines",
    "section": "Copyright and content licencing",
    "text": "Copyright and content licencing\nContributors retain copyright of their work, but agree to publish their work under a Creative Commons licence. Contributors are free to choose the licence that best suits their content. The chosen licence should be indicated on the draft submission.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#the-review-process",
    "href": "contributor-docs/contributor-guidelines.html#the-review-process",
    "title": "Contributor guidelines",
    "section": "The review process",
    "text": "The review process\nDraft submissions will be shared for review with members of the The Data Shrink Editorial Board. Comments and edits to documents will be made via Google Docs/MS Word/GitHub, allowing for (a) version control, (b) open dialogue between reviewers and contributors, and (c) a transparent and well-documented review process.\nOnce revisions are complete and content is accepted for publication, authors will be provided with HTML files to preview published content. Following sign-off by author and editor, HTML files will be made live.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/contributor-guidelines.html#post-publication",
    "href": "contributor-docs/contributor-guidelines.html#post-publication",
    "title": "Contributor guidelines",
    "section": "Post-publication",
    "text": "Post-publication\nContributors and editors will work together to promote content via social media platforms – Twitter/X, LinkedIn, blogs – and in other channels as appropriate – e.g., in response to related questions on Quora or Stack Overflow.\nContributors are encouraged to monitor their content regularly for user comments and discussions. Engaging in discussions with users – whether through the The Data Shrink platform or via social media and other channels – is an effective way of developing an audience: it builds profile for the contributor and their content, and encourages other users to find and interact with content.\nContributors also have the option of uploading the source code and files for their articles to Zenodo, an open science repository, and submitting this material to the The Data Shrink community. When a Zenodo record is created, a DOI is assigned, and this DOI can then be added to your published article on The Data Shrink. For more details on working with Zenodo, see help.zenodo.org.",
    "crumbs": [
      "Contributor guidelines"
    ]
  },
  {
    "objectID": "contributor-docs/training-guides.html",
    "href": "contributor-docs/training-guides.html",
    "title": "Training guides",
    "section": "",
    "text": "In data science, there’s no one-size-fits-all solution to every problem and challenge. So, part of the job of the data scientist is to rapidly learn about different sub-domains, tools and techniques, and put those learnings into practice.\nBut it can be time-consuming to figure out what you need to learn and in what order, and to identify the best resources for doing so. This is where our Training guides come in. Each will set out a learning pathway for data scientists to follow, with recommendations of textbooks, videos, practical exercises and other teaching material to use every step of the way.",
    "crumbs": [
      "Training guides"
    ]
  },
  {
    "objectID": "contributor-docs/training-guides.html#structure",
    "href": "contributor-docs/training-guides.html#structure",
    "title": "Training guides",
    "section": "Structure",
    "text": "Structure\nContributors should think about their training guides as being short online courses that are constructed from existing high-quality material. You do not need to create your own “course” content. Rather, you should focus on recommending texts and other material for users to follow in a logical ordered way, so that they may build up and secure their knowledge of a particular topic.\nGuides should feature a mix of content types – not only text, but audio and video – and they should provide ample opportunities for users to practice what they are learning.\nA brief and extremely simplified example of a guide is as follows:\n\nStep 1: Watch this introductory video on Topic X.\nStep 2: Now you are familiar with the basics of Topic X, you will want to read Chapter 2 of Textbook Y, which delves into more of the mathematical underpinnings.\nStep 3: Let’s try Topic X ourselves. This GitHub repository has code for you to run it in Python. Copy the code and give it a go.\nStep 4: You should now be ready to apply Topic X to a simple data challenge. Check out this Kaggle page and practice what you have learned so far.\nStep 5: We’re now moving from the “beginner” level to “intermediate”, and Training Course Z gives a thorough overview of what you need to know for the next stage of your learning journey.\n… etc.",
    "crumbs": [
      "Training guides"
    ]
  },
  {
    "objectID": "contributor-docs/training-guides.html#advice-and-recommendations",
    "href": "contributor-docs/training-guides.html#advice-and-recommendations",
    "title": "Training guides",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nBe mindful of different learning styles. Some people prefer to read, others prefer to watch or listen. So, wherever possible, for each stage of your training guide, try to provide a mix of resources that meet the same learning objectives.\nConsider barriers to entry. Data scientists in large organisations may have access to training budgets or mechanisms to apply for training funds. But that isn’t the case for all data scientists, meaning that paid-for materials and training courses might not be accessible to everyone. Recommend them sparingly, and if there are ways to access the material at reduced rates do let users know. However, you must not link to illicit copies of material – e.g., unauthorised PDF reproductions of textbooks.\nIf there are resource gaps, please tell us. While planning out your training guide, you may well struggle to find the perfect piece of content to recommend at a particular stage of your learning journey. If that is the case, do get in touch with us. One of the goals of The Data Shrink is to identify and plug these sorts of gaps, so that all in the data science community can benefit. We’ll sketch out a commission and take it out to our network of contacts. Or perhaps it’ll be something you want to create for the site!",
    "crumbs": [
      "Training guides"
    ]
  },
  {
    "objectID": "contributor-docs/explainers.html",
    "href": "contributor-docs/explainers.html",
    "title": "Explainers",
    "section": "",
    "text": "On The Data Shrink, Explainers are the stories behind the stories of data science in action. They are deep-dive explorations of the ideas, concepts, tools, and methods that make data science projects possible. In particular, we are keen to explore and explain the statistical underpinnings of modern data science techniques.\nA good Explainer will lead audiences through the what, when, how, and why of its chosen topic. The ultimate goal is to equip data scientists with the information and insight they need to make smarter, more informed analytical choices.\nThere are many different but effective ways of structuring an explainer and plentiful written examples in major media outlets like The Guardian and Vox, but these are generally written for a non-technical audience. Examples of technical explainers (with interactive elements) can be found on Amazon’s Machine Learning University.",
    "crumbs": [
      "Explainers"
    ]
  },
  {
    "objectID": "contributor-docs/explainers.html#structure",
    "href": "contributor-docs/explainers.html#structure",
    "title": "Explainers",
    "section": "Structure",
    "text": "Structure\nThe following outline is a basic guide to structuring an Explainer:\n\n\nHook\n\nIntroduce your topic, and explain why audiences should pay attention. For example, does your Explainer link to one of our published case studies? Does it focus on a tool or method that has been the subject of recent attention? Is it a foundational idea that is relevant to all sorts of data science applications?\n\n\n\nHigh-level summary\n\nA short, largely non-technical explanation of your topic. A good way to view this section is as an accessible condensed version of your complete Explainer. In thinking of it in this way, you can subtly signpost to audiences the areas you’ll be covering and the questions you’ll be answering throughout the remainder of your contribution.\n\n\n\nHistory and background\n\nIt can be useful from a practical perspective to explain how ideas, concepts, tools, and methods have developed over time. Applications may have become more complex in recent years, so exploring the origins of data science techniques might lead you to discover simpler use cases that can help support and illustrate your high-level summary.\n\n\n\nThe how, the when, the why\n\nThis section of your Explainer will likely be split into multiple subsections as you seek to build up your audience’s understanding of your chosen topic. It can be helpful to think about the sorts of questions an audience member might ask and to structure your contribution so that it directly addresses those questions (Q&A/FAQ formats are commonly used in explainer-type articles). If the focus of your Explainer is a data science method, for example, you’ll want to address the following:\n\n\n\nHow does it work and how is it applied (perhaps with an example or simulation)?\nWhat are the underlying assumptions?\nHow is performance checked and assessed?\nHow should outputs be interpreted?\nWhat are the pros and cons, the strengths and limitations of the approach?\nWhat are the optimal use cases, and when should the method be avoided altogether?\nAre there alternatives that people should know about?\n\n\nKey takeaways\n\nThis serves as your final summary: a chance to remind your audience of what they’ve learned from your Explainer and the main points they should keep in mind.\n\n\n\nTell me more\n\nIt’s sensible to assume that some of your audience will have further questions and will want to learn more about the topic. If you have additional sources of information to recommend, make sure to share them here.",
    "crumbs": [
      "Explainers"
    ]
  },
  {
    "objectID": "contributor-docs/explainers.html#advice-and-recommendations",
    "href": "contributor-docs/explainers.html#advice-and-recommendations",
    "title": "Explainers",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nFocus on what’s important. Your Explainer can’t hope to explain everything, so you need to be clear about what’s essential for your audience to know and what isn’t. Make good use of links and references to point audiences to other valuable sources of information that can enrich their understanding of your topic.\nBe clear about your target audience and their expected prior level of knowledge. In keeping with the point above, you need to be clear in your own mind about how much you expect your audience to know already about the general topic or subject matter. You can then structure your contribution accordingly. It might also be helpful to state explicitly, at the outset of your contribution, what assumptions you’ve made about your audience and highlight any background reading that might be beneficial.\nPlan out your route. To help you decide what to cover in your Explainer, we recommend first writing out your high-level summary of the topic and also your key takeaways. This provides you with a start point (A) and an end point (B) for your contribution. The challenge then is to figure out the main points or questions you will need to address to help your audience progress from point A to point B in a way that’s logical and intuitive for them to follow.",
    "crumbs": [
      "Explainers"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html",
    "href": "contributor-docs/style-guide.html",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to The Data Shrink are creating content for their colleagues and peers and should “speak” to them as such.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#tone",
    "href": "contributor-docs/style-guide.html#tone",
    "title": "Style guide",
    "section": "",
    "text": "Content must be presented in a conversational yet professional and respectful tone. Contributors should imagine themselves delivering a lively, engaging conference presentation, rather than preparing a dry, formal report or journal publication. Contributors to The Data Shrink are creating content for their colleagues and peers and should “speak” to them as such.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#structure",
    "href": "contributor-docs/style-guide.html#structure",
    "title": "Style guide",
    "section": "Structure",
    "text": "Structure\nEach contribution must, in effect, tell “a story”, and so contributors need to be clear (a) what their story is, (b) why people should be interested, and (c) what its main message or key takeaways are. To help figure this out, we recommend contributors apply the XY Story Formula.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "href": "contributor-docs/style-guide.html#technical-content-and-jargon",
    "title": "Style guide",
    "section": "Technical content and jargon",
    "text": "Technical content and jargon\nTechnical content is a necessary feature of a site like ours. Without it, an article or other piece of content may be of little practical use to a technical audience. But if there’s too much of it, even experts may struggle to stay engaged. Contributors are also faced with a dilemma when it comes to explaining technical content: explain nothing, and you risk alienating some of your audience; explain everything, and you’ll struggle to establish a clear, strong narrative thread. So, careful consideration is required:\n\nWho is my audience for this article?\nWhat is this audience likely to know already, and what needs to be explained?\nIf something needs to be explained, can I do so briefly and then link to other resources? Or is a full explanation required?\nIn telling my “story”, what are the absolute-need-to-knows, and what are the simply-nice-to-knows?\n\nThinking through these questions will help contributors to find the right mix of valuable, technical content paired with accessible, readable narrative.\nKeep in mind that the same general advice applies to the use of industry jargon. Jargon can be a valuable shorthand when communicating with people working in the same organisation or sector, but those working in different fields may struggle to make sense of it. So, contributors need to think carefully about how much jargon to use, and what needs to be explained.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#figuresgraphics",
    "href": "contributor-docs/style-guide.html#figuresgraphics",
    "title": "Style guide",
    "section": "Figures/graphics",
    "text": "Figures/graphics\nAll data visualisations and other graphical outputs directly related to the content of submissions must be presented neatly and cleanly (avoid chart junk). They should also be labelled correctly and legibly, with colours chosen carefully to ensure they can be easily distinguished by all readers. Accompanying captions must be written to support the reader’s understanding of the visual presentation (e.g., “Figure 1: a bar chart” is an insufficient description).\nIf contributors wish to use charts or graphs that are not their own work, they must ensure that such items are correctly sourced and referenced, and that permission to republish has been obtained. A letter or email confirming this permission is required.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#data-sources",
    "href": "contributor-docs/style-guide.html#data-sources",
    "title": "Style guide",
    "section": "Data sources",
    "text": "Data sources\nContributors must include within their submissions any links and/or references to the sources of data, code and/or software and software packages on which their analyses are based. We understand that some data sources may not be publicly available, whether for legal, ethical or commercial reasons. However, readers must still be told where the data come from, even if they are not able to access the data themselves.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#references",
    "href": "contributor-docs/style-guide.html#references",
    "title": "Style guide",
    "section": "References",
    "text": "References\nCitations are to be formatted in The Chicago Manual of Style author-date format.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/style-guide.html#use-of-images",
    "href": "contributor-docs/style-guide.html#use-of-images",
    "title": "Style guide",
    "section": "Use of images",
    "text": "Use of images\nImages for general illustration purposes will be sourced and – where necessary and within reason – paid for by The Data Shrink.\n\n\n\n\n\n\n\nNote\n\n\n\nFor all other style-related matters, we follow The Guardian and Observer Style Guide.",
    "crumbs": [
      "Style guide"
    ]
  },
  {
    "objectID": "contributor-docs/case-studies.html",
    "href": "contributor-docs/case-studies.html",
    "title": "Case studies",
    "section": "",
    "text": "Case studies are a core feature of the The Data Shrink platform. Our case studies are designed to show how data science is used to solve real-world problems in business, public policy and beyond.\nA good case study will be a source of information, insight and inspiration for each of our target audiences:",
    "crumbs": [
      "Case studies"
    ]
  },
  {
    "objectID": "contributor-docs/case-studies.html#structure",
    "href": "contributor-docs/case-studies.html#structure",
    "title": "Case studies",
    "section": "Structure",
    "text": "Structure\nCase studies should follow the structure below. It is not necessary to use the section headings we have provided – creativity and variety are encouraged. However, the areas outlined under each section heading should be covered in all submissions.\n\n\nThe problem/challenge\n\nSummarise the project and its relevance to your organisation’s needs, aims and ambitions.\n\n\n\nGoals\n\nSpecify what exactly you sought to achieve with this project.\n\n\n\nBackground\n\nAn opportunity to explain more about your organisation, your team’s work leading up to this project, and to introduce audiences more generally to the type of problem/challenge you faced, particularly if it is a problem/challenge that may be experienced by organisations working in different sectors and industries.\n\n\n\nApproach\n\nDescribe how you turned the organisational problem/challenge into a task that could be addressed by data science. Explain how you proposed to tackle the problem, including an introduction, explanation and (possibly) a demonstration of the method, model or algorithm used. (NB: If you have a particular interest and expertise in the method, model or algorithm employed, including the history and development of the approach, please consider writing an Explainer article for us.) Discuss the pros and cons, strengths and limitations of the approach.\n\n\n\nImplementation\n\nWalk audiences through the implementation process. Discuss any challenges you faced, the ethical questions you needed to ask and answer, and how you tested the approach to ensure that outcomes would be robust, unbiased, good quality, and aligned with the goals you set out to achieve.\n\n\n\nImpact\n\nHow successful was the project? Did you achieve your goals? How has the project benefited your organisation? How has the project benefited your team? Does it inform or pave the way for future projects?\n\n\n\nLearnings\n\nWhat are your key takeaways from the project? Are there lessons that you can apply to future projects, or are there learnings for other data scientists working on similar problems/challenges?",
    "crumbs": [
      "Case studies"
    ]
  },
  {
    "objectID": "contributor-docs/case-studies.html#advice-and-recommendations",
    "href": "contributor-docs/case-studies.html#advice-and-recommendations",
    "title": "Case studies",
    "section": "Advice and recommendations",
    "text": "Advice and recommendations\nYou do not need to divulge the detailed inner workings of your organisation. Audiences are mostly interested in understanding the general use case and the problem-solving process you went through, to see how they might apply the same approach within their own organisations.\nGoals can be defined quite broadly. There’s no expectation that you set out your organisation’s short- or long-term targets. Instead, audiences need to know enough about what you want to do so they can understand what motivates your choice of approach.\nUse toy examples and synthetic data to good effect. We understand that – whether for commercial, legal or ethical reasons – it can be difficult or impossible to share real data in your case studies, or to describe the actual outputs of your work. However, there are many ways to share learnings and insights without divulging sensitive information. This blog post from Lyft uses hypotheticals, mathematical notation and synthetic data to explain the company’s approach to causal forecasting without revealing actual KPIs or data.\nPeople like to experiment, so encourage them to do so. Our platform allows you to embed code and to link that code to interactive coding environments like Google Colab. So if, for example, you want to explain a technique like bootstrapping, why not provide a code block so that audiences can run a bootstrapping simulation themselves.\nLeverage links. You can’t be expected to explain or cover every detail in one case study, so feel free to point audiences to other sources of information that can enrich their understanding: blogs, videos, journal articles, conference papers, etc.",
    "crumbs": [
      "Case studies"
    ]
  },
  {
    "objectID": "feeds.html",
    "href": "feeds.html",
    "title": "RSS feeds",
    "section": "",
    "text": "Latest content\nrealworlddatascience.net/latest-content.xml\n\n\nCase studies\nrealworlddatascience.net/case-studies/index.xml\n\n\nIdeas\nrealworlddatascience.net/ideas/index.xml\n\n\nCareers\nrealworlddatascience.net/careers/index.xml\n\n\nViewpoints\nrealworlddatascience.net/viewpoints/index.xml"
  }
]